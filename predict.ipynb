{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "torch_device = 'cuda'\n",
    "\n",
    "QA_TOKENIZER = AutoTokenizer.from_pretrained(\"bert-large-cased-whole-word-masking-finetuned-squad\")\n",
    "QA_MODEL = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-cased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "QA_MODEL.to(torch_device)\n",
    "QA_MODEL.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 1024)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (12): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (13): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (14): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (15): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (16): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (17): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (18): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (19): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (20): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (21): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (22): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (23): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "#from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "#SUMMARY_TOKENIZER = PegasusTokenizer.from_pretrained('mayu0007/pegasus_large_covid')\n",
    "#SUMMARY_MODEL = PegasusForConditionalGeneration.from_pretrained('mayu0007/pegasus_large_covid')\n",
    "\n",
    "SUMMARY_MODEL.to(torch_device)\n",
    "SUMMARY_MODEL.eval()\n",
    "def Summarizer(string_all):    \n",
    "    if 1==1:\n",
    "        ## try generating an exacutive summary with bart abstractive summarizer\n",
    "        allAnswersTxt = string_all.replace('\\n','')\n",
    "\n",
    "        answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n",
    "        #answers_input_ids = SUMMARY_TOKENIZER.prepare_seq2seq_batch([allAnswersTxt], return_tensors='pt', truncation=True, padding='longest')['input_ids'].to(torch_device)\n",
    "        summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n",
    "                                               num_beams=4,\n",
    "                                               length_penalty=0.8,\n",
    "                                               repetition_penalty=2.0,\n",
    "                                               min_length=5,\n",
    "                                               no_repeat_ngram_size=0,\n",
    "                                                do_sample=False )\n",
    "\n",
    "        exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "        return exec_sum\n",
    "\n",
    "# Program to find most frequent  \n",
    "# element in a list \n",
    "  \n",
    "from collections import Counter \n",
    "  \n",
    "def most_frequent(List): \n",
    "    occurence_count = Counter(List) \n",
    "    return occurence_count.most_common(1)[0][0]\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,AutoModelForSequenceClassification\n",
    "#hg_model_hub_name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "hg_model_hub_name = \"ynie/xlnet-large-cased-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "\n",
    "tokenizer_nli = AutoTokenizer.from_pretrained(hg_model_hub_name, use_fast=False)\n",
    "model_nli = AutoModelForSequenceClassification.from_pretrained(hg_model_hub_name)\n",
    "model_nli.to(torch_device)\n",
    "model_nli.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import pysearch\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from anglicize import anglicize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import statistics\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"dev.jsonl\"\n",
    "# dataset_path = \"feverous_test_unlabeled.jsonl\"\n",
    "output_path = dataset_path.replace(\".jsonl\", \"_predictions.jsonl\")\n",
    "evaluation_path = dataset_path.replace(\".jsonl\", \"_evaluation.jsonl\")\n",
    "\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sqla\n",
    "\n",
    "db_fullpath = \"feverous_wikiv1.db\"\n",
    "db = sqla.create_engine(\"sqlite:///{}\".format(db_fullpath))\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "Session = sessionmaker(bind=db)\n",
    "sql_session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucene_dir = 'anserini/indexes/fever/lucene-index-fever-paragraph'\n",
    "searcher = pysearch.SimpleSearcher(lucene_dir)\n",
    "\n",
    "def get_relevant_pages(claim, FULL_CLAIM=True, SPACY_ENTITIES=True, CASE_ENTITIES=True, LINKED_PAGES=False, LINKING_PAGES=False, N_RESULTS=70):\n",
    "    \n",
    "    if not SPACY_ENTITIES and not CASE_ENTITIES and not LINKED_PAGES:\n",
    "        FULL_CLAIM = True\n",
    "    \n",
    "    start_timer = timer()\n",
    "\n",
    "    claim = nlp(claim.replace(\" \", \" \").replace(\"\", \" \")) # Replaces the obnoxious space character with normal space\n",
    "\n",
    "    keywords = set()\n",
    "    entities = set()\n",
    "\n",
    "    if SPACY_ENTITIES:\n",
    "        spacy_entities = [entity.text for entity in claim.ents if entity.label_ != \"DATE\"]\n",
    "        entities.update(spacy_entities)\n",
    "\n",
    "    if CASE_ENTITIES:\n",
    "        case_entities = set()\n",
    "        chunks = claim.noun_chunks\n",
    "        for chunk in chunks:\n",
    "            for token in tokenizer(chunk.text):\n",
    "                if token.text[0].isupper():\n",
    "                    case_entities.add(chunk.text)\n",
    "                    break\n",
    "\n",
    "        entities.update(case_entities)\n",
    "        #print(case_entities)\n",
    "        #print(entities)\n",
    "        #sys.exit(0)\n",
    "\n",
    "    keywords.update(entities)\n",
    "\n",
    "    if not FULL_CLAIM:\n",
    "        search_query = (\", \".join(keywords) + '\"' + '\", \"'.join(entities) + '\"')\n",
    "#            search_query = (\", \".join(keywords) + \", \".join(entities))\n",
    "    else:\n",
    "        search_query = (claim.text + ' \"' + '\", \"'.join(keywords) + '\" \"' + '\", \"'.join(entities) + '\"')\n",
    "#            search_query = (claim.text + \" \" + \", \".join(keywords) + \", \".join(entities))\n",
    "\n",
    "    if FULL_CLAIM or keywords:\n",
    "        try:\n",
    "            lucene_hits = searcher.search(search_query, k=N_RESULTS) #.encode(\"utf-8\")\n",
    "        except:\n",
    "            lucene_hits = None\n",
    "    else:\n",
    "        lucene_hits = None\n",
    "\n",
    "    hit_dictionary = {}\n",
    "\n",
    "    if lucene_hits:\n",
    "\n",
    "        for hit in lucene_hits:\n",
    "            hit_abs = hit.lucene_document.get(\"raw\").split(\" ~~ \")\n",
    "            step = 10\n",
    "            counter = 0\n",
    "            for i in range(0, len(hit_abs), step):\n",
    "                temp_abs = \" ~~ \".join(hit_abs[i:min([len(hit_abs), i+step])])\n",
    "                hit_dict = {\"abstract\": None, \"real_abstract\": None, \"title\": None}\n",
    "#                 hit_dict['abstract']=hit.lucene_document.get(\"raw\")\n",
    "#                 hit_dict['real_abstract']=hit.lucene_document.get(\"raw\")\n",
    "                hit_dict['title'] = str(hit.docid)\n",
    "                hit_dict['abstract'] = temp_abs\n",
    "                hit_dict['real_abstract'] = temp_abs\n",
    "                hit_dictionary[\"{}_{}\".format(hit.docid, counter)] = hit_dict\n",
    "                counter += 1\n",
    "    \n",
    "        linked_pages = set()\n",
    "        if LINKED_PAGES:\n",
    "            for hit in lucene_hits:\n",
    "                links = re.findall(r\"(?:\\[\\[)(.*?)(?:\\|)\", hit.raw)\n",
    "                for link in links:\n",
    "                    linked_pages.update([link.replace(\"_\", \" \").encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\")])\n",
    "                    \n",
    "        linking_pages = set()\n",
    "        if LINKING_PAGES and sql_session:\n",
    "            linking_results = sql_session.execute('select target, sources from inlinks where target IN (\"{}\")'.format('\", \"'.join([hit.docid.replace('\"', '\"\"') for hit in lucene_hits])))\n",
    "            for row in linking_results:\n",
    "                linking_pages.update(row[\"sources\"].split(\";\"))\n",
    "                \n",
    "            for linking_page in linking_pages:\n",
    "                if linking_page+\"_0\" not in hit_dictionary:\n",
    "                    page_raw = sql_session.execute('select * from wiki where id = \"{}\"'.format(linking_page.replace('\"', '\"\"').replace(\"'\", \"''\"))).fetchone()\n",
    "                    if not page_raw:\n",
    "                        continue\n",
    "                    page_raw = page_raw[1]\n",
    "                    hit_dict = {\"abstract\": page_raw, \"real_abstract\": page_raw, \"title\": linking_page}\n",
    "                    hit_dictionary[\"{}_{}\".format(linking_page, \"_0\")] = hit_dict\n",
    "                    \n",
    "                    \n",
    "        found_pages = [hit.docid for hit in lucene_hits]\n",
    "        for i in range(0, len(found_pages)):\n",
    "\n",
    "            try:\n",
    "                found_pages[i] = found_pages[i].encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "                #print(\"Done:\",found_pages[i])\n",
    "            except:\n",
    "                import regex\n",
    "                print(found_pages[i], anglicize(found_pages[i]), regex.sub(r'[^\\p{Latin}]', '', found_pages[i]).encode(\"latin-1\").decode(\"utf-8\"))\n",
    "                \n",
    "        found_pages = set(found_pages)\n",
    "\n",
    "        hit_scores = [hit.score for hit in lucene_hits]\n",
    "        hit_score_min = min(hit_scores)\n",
    "        hit_score_25 = np.percentile(hit_scores, 25)\n",
    "        hit_score_mean = np.mean(hit_scores)\n",
    "        hit_score_median = statistics.median(hit_scores)\n",
    "        hit_score_75 = np.percentile(hit_scores, 75)\n",
    "        hit_score_max = max(hit_scores)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        is_found = False\n",
    "        found_pages = set()\n",
    "        entities = set()\n",
    "        keywords = set()\n",
    "        linked_pages = set()\n",
    "        linking_pages = set()\n",
    "        lucene_hits = []\n",
    "        hit_scores = []\n",
    "        hit_score_min = None\n",
    "        hit_score_25 = None\n",
    "        hit_score_mean = None\n",
    "        hit_score_median = None\n",
    "        hit_score_75 = None\n",
    "        hit_score_max = None\n",
    "\n",
    "    end_timer = timer()\n",
    "    elapsed = int(end_timer - start_timer)\n",
    "    elapsed_formatted = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))\n",
    "        \n",
    "    result = [claim.text, elapsed_formatted, \n",
    "                found_pages, \n",
    "                keywords, \n",
    "                linked_pages, \n",
    "                linking_pages,\n",
    "                len(lucene_hits), \n",
    "                hit_score_min, hit_score_25, hit_score_mean, hit_score_median, \n",
    "                hit_score_75, hit_score_max, hit_scores]\n",
    "\n",
    "    #print(result)\n",
    "    \n",
    "    result = pd.DataFrame([result], columns=[\"CLAIM\", \"ELAPSED\", \n",
    "                                             \"PAGES_FOUND\",\n",
    "                                             \"KEYWORDS\", \n",
    "                                             \"LINKED_PAGES\",\n",
    "                                             \"LINKING_PAGES\",\n",
    "                                             \"N_LUCENE_HITS\", \n",
    "                                             \"HIT_SCORE_MIN\", \"HIT_SCORE_25\", \"HIT_SCORE_MEAN\", \"HIT_SCORE_MEDIAN\", \n",
    "                                             \"HIT_SCORE_75\", \"HIT_SCORE_MAX\", \"HIT_SCORES\"])\n",
    "    \n",
    "    return result, hit_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckEntailmentNeturalorContradict(ranked_aswers, string_all, pandasData):\n",
    "    if 1 == 1:\n",
    "        premise_list = []\n",
    "        hypothesis_list = []\n",
    "        entailment_prob = []\n",
    "        neutral_prob = []\n",
    "        contradict_prob = []\n",
    "        final_result_list = []\n",
    "        answers_to_check = 3\n",
    "        if len(ranked_aswers) > answers_to_check:\n",
    "            v_range = answers_to_check\n",
    "        else:\n",
    "            v_range = len(ranked_aswers)\n",
    "        for i in range(v_range):\n",
    "            if pandasData[i][2] >= 0.6:\n",
    "                max_length = 512\n",
    "#                 candidate_answer = Summarizer(ranked_aswers[i])\n",
    "                candidate_answer = ranked_aswers[i]\n",
    "                # premise = \"symptoms of covid-19 range from none (asymptomatic) to severe pneumonia and it can be fatal. fever and cough are the most common symptoms in patients with 2019-ncov infection. Several people have muscle soreness or fatigue as well as ards. diarrhea, hemoptysis, headache, sore throat, shock, and other symptoms only occur in a small number of patients\"\n",
    "                if len(candidate_answer) > len(ranked_aswers[i]):\n",
    "                    premise = ranked_aswers[i]\n",
    "                else:\n",
    "                    premise = candidate_answer\n",
    "                # premise = \"covid-19 symptom is highly various in each patient, with fever, fatigue, shortness of breath, and cough as the main presenting symptoms. patient with covid-19 may shows severe symptom with severe pneumonia and ards, mild symptom resembling simple upper respiration tract infection, or even completely asymptomatic. approximately 80 % of cases is mild \"\n",
    "                hypothesis = string_all\n",
    "                tokenized_input_seq_pair = tokenizer_nli.encode_plus(\n",
    "                    premise,\n",
    "                    hypothesis,\n",
    "                    max_length=max_length,\n",
    "                    return_token_type_ids=True,\n",
    "                    truncation=True,\n",
    "                )\n",
    "\n",
    "                input_ids = (\n",
    "                    torch.Tensor(tokenized_input_seq_pair[\"input_ids\"])\n",
    "                    .long()\n",
    "                    .unsqueeze(0)\n",
    "                    .to(torch_device)\n",
    "                )\n",
    "                # remember bart doesn't have 'token_type_ids', remove the line below if you are using bart.\n",
    "                token_type_ids = (\n",
    "                    torch.Tensor(tokenized_input_seq_pair[\"token_type_ids\"])\n",
    "                    .long()\n",
    "                    .unsqueeze(0)\n",
    "                    .to(torch_device)\n",
    "                )\n",
    "                attention_mask = (\n",
    "                    torch.Tensor(tokenized_input_seq_pair[\"attention_mask\"])\n",
    "                    .long()\n",
    "                    .unsqueeze(0)\n",
    "                    .to(torch_device)\n",
    "                )\n",
    "\n",
    "                outputs = model_nli(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=None,\n",
    "                )\n",
    "                # Note:\n",
    "                # \"id2label\": {\n",
    "                #     \"0\": \"entailment\",\n",
    "                #     \"1\": \"neutral\",\n",
    "                #     \"2\": \"contradiction\"\n",
    "                # },\n",
    "\n",
    "                predicted_probability = torch.softmax(outputs[0], dim=1)[\n",
    "                    0\n",
    "                ].tolist()  # batch_size only one\n",
    "\n",
    "                # print(\"Premise:\", premise)\n",
    "                premise_list.append(premise)\n",
    "                # print(\"Hypothesis:\", hypothesis)\n",
    "                hypothesis_list.append(hypothesis)\n",
    "                # print(\"input Ans:\",ranked_aswers[i])\n",
    "                # print(\"Entailment:\", predicted_probability[0])\n",
    "                entailment_prob.append(predicted_probability[0])\n",
    "                # print(\"Neutral:\", predicted_probability[1])\n",
    "                neutral_prob.append(predicted_probability[1])\n",
    "                # print(\"Contradiction:\", predicted_probability[2])\n",
    "                contradict_prob.append(predicted_probability[2])\n",
    "        if len(premise_list) > 0:\n",
    "            avg_entailment = np.average(entailment_prob)\n",
    "            avg_neutral = np.average(neutral_prob)\n",
    "            avg_contradiction = np.average(contradict_prob)\n",
    "            zipped_list = zip(entailment_prob, neutral_prob, contradict_prob)\n",
    "            for key, val in enumerate(hypothesis_list):\n",
    "                if neutral_prob[key] > 0.8:\n",
    "                    final_result = \"Neutral\"\n",
    "                elif entailment_prob[key] > contradict_prob[key]:\n",
    "                    final_result = \"True\"\n",
    "                    final_result_list.append(final_result)\n",
    "                else:\n",
    "                    final_result = \"False\"\n",
    "                    final_result_list.append(final_result)\n",
    "            if len(final_result_list) > 0:\n",
    "                verdict = most_frequent(final_result_list)\n",
    "            else:\n",
    "                verdict = \"Neutral\"\n",
    "        else:\n",
    "            #       zipped_list = zip(0,1,0)\n",
    "            zipped_list = zip([0], [1], [0])\n",
    "            verdict = \"Neutral\"\n",
    "    return zipped_list, verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canonical_title(title):\n",
    "    return title.split(\"_\")[0]\n",
    "\n",
    "def get_entities(text):\n",
    "    claim = nlp(text.replace(\" \", \" \").replace(\"\", \" \")) # Replaces the obnoxious space character with normal space\n",
    "\n",
    "    entities = set()\n",
    "    \n",
    "    spacy_entities = [entity.text for entity in claim.ents if entity.label_ != \"DATE\"]\n",
    "    entities.update(spacy_entities)\n",
    "    \n",
    "    claim_processed = nlp(text.replace(\" \", \" \").replace(\"\", \" \").replace(\"'\", \". \"))\n",
    "    \n",
    "    spacy_entities_processed = [entity.text for entity in claim_processed.ents if entity.label_ != \"DATE\"]\n",
    "    entities.update(spacy_entities)\n",
    "\n",
    "    case_entities = set()\n",
    "    chunks = claim.noun_chunks\n",
    "    for chunk in chunks:\n",
    "        for token in tokenizer(chunk.text):\n",
    "            if token.text[0].isupper():\n",
    "                case_entities.add(chunk.text)\n",
    "                break\n",
    "\n",
    "        entities.update(case_entities)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def get_people(text):\n",
    "    \n",
    "    claim = nlp(text.replace(\" \", \" \").replace(\"\", \" \")) # Replaces the obnoxious space character with normal space\n",
    "    \n",
    "    people = set()\n",
    "\n",
    "    for ent in claim.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            people.add(ent.text)\n",
    "            \n",
    "    claim_processed = nlp(text.replace(\" \", \" \").replace(\"\", \" \").replace(\"'\", \". \"))\n",
    "    \n",
    "    for ent in claim_processed.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            people.add(ent.text)\n",
    "            \n",
    "    case_entities = set()\n",
    "    chunks = claim_processed.noun_chunks\n",
    "    for chunk in chunks:\n",
    "        for token in tokenizer(chunk.text):\n",
    "            if token.text[0].isupper():\n",
    "                case_entities.add(chunk.text)\n",
    "                break\n",
    "                \n",
    "#     print(case_entities)\n",
    "                \n",
    "    for entity in case_entities:\n",
    "        candidate_entities = nlp(entity).ents\n",
    "        for candidate in candidate_entities:\n",
    "            if candidate.label_ == \"PERSON\":\n",
    "                people.add(candidate.text)\n",
    "            \n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_cells(page, search_for):\n",
    "    import json\n",
    "    import Levenshtein\n",
    "\n",
    "    result = sql_session.execute('''select * from wiki where id = \"{}\"'''.format(page.replace('\"', '\"\"').replace(\"'\", \"''\"))).fetchone()\n",
    "    if not result:\n",
    "#         print(\"Page not found:\",page)\n",
    "        return []\n",
    "    data = json.loads(result[1])\n",
    "    tables = [element for element in data[\"order\"] if \"table_\" in element]\n",
    "\n",
    "    matches = []\n",
    "    for table in tables:\n",
    "        rows = data[table][\"table\"]\n",
    "        for row in rows:\n",
    "            for cell in row:\n",
    "                for searching in search_for:\n",
    "                    similarities = [\n",
    "                        Levenshtein.ratio(searching, cell[\"value\"]), \n",
    "                        Levenshtein.ratio(searching, re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", cell[\"value\"], flags=re.MULTILINE))\n",
    "                    ]\n",
    "\n",
    "                    if max(similarities) >= 0.8:\n",
    "                        matches.append([max(similarities), cell[\"id\"]])\n",
    "                        break\n",
    "\n",
    "    matches.sort(key=lambda x: float(x[0]), reverse=True)\n",
    "\n",
    "    return [match[1] for match in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# print(os.environ.get(\"MKL_THREADING_LAYER\"))\n",
    "# os.environ[\"MKL_THREADING_LAYER\"] = \"None\"\n",
    "\n",
    "# print(os.environ.get(\"BUILD_DOCS\"))\n",
    "# os.environ[\"BUILD_DOCS\"] = \"1\"\n",
    "\n",
    "# !pip install --upgrade --force-reinstall torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.1+cu102.html\n",
    "    \n",
    "import torch_scatter\n",
    "\n",
    "# !pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTableQuestionAnswering, TapasConfig\n",
    "\n",
    "table_tokenizer = AutoTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "\n",
    "# config = TapasConfig(drop_rows_to_fit=True)\n",
    "\n",
    "table_model = AutoModelForTableQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "\n",
    "def get_table_evidence(page, claim):\n",
    "    from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    result = sql_session.execute(\"select * from wiki where id = '{}'\".format(page.replace('\"', '\"\"').replace(\"'\", \"''\"))).fetchone()\n",
    "    if not result:\n",
    "        return []\n",
    "    data = json.loads(result[1])\n",
    "    tables = [element for element in data[\"order\"] if \"table_\" in element]\n",
    "    if not tables:\n",
    "        return []\n",
    "    cell_ids = set()\n",
    "    queries = [claim]\n",
    "    \n",
    "    for table in tables:\n",
    "#         print(page, table)\n",
    "        table_data = json_table_to_dict(data[table][\"table\"], crop=True)\n",
    "        if not table_data:\n",
    "            continue\n",
    "        table_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in table_data.items() ])).replace(np.nan,' ')\n",
    "        inputs = table_tokenizer(table=table_df, queries=queries, padding='max_length', return_tensors=\"pt\")\n",
    "        if len(inputs[\"input_ids\"].view(-1)) > 1000:\n",
    "            continue\n",
    "        outputs = table_model(**inputs)\n",
    "        predicted_answer_coordinates, predicted_aggregation_indices = table_tokenizer.convert_logits_to_predictions(\n",
    "                inputs,\n",
    "                outputs.logits.detach(),\n",
    "                outputs.logits_aggregation.detach(),\n",
    "                cell_classification_threshold=0.3\n",
    "        )\n",
    "        # let's print out the results:\n",
    "        id2aggregation = {0: \"NONE\", 1: \"SUM\", 2: \"AVERAGE\", 3:\"COUNT\"}\n",
    "        aggregation_predictions_string = [id2aggregation[x] for x in predicted_aggregation_indices]\n",
    "        answers = []\n",
    "        for coordinates in predicted_answer_coordinates:\n",
    "            if len(coordinates) == 1:\n",
    "                # only a single cell:\n",
    "                answers.append(table_df.iat[coordinates[0]])\n",
    "            else:\n",
    "                # multiple cells\n",
    "                cell_values = []\n",
    "                for coordinate in coordinates:\n",
    "                    cell_values.append(table_df.iat[coordinate])\n",
    "                    \n",
    "                answers.extend(cell_values)\n",
    "                \n",
    "        for answer in answers:\n",
    "            for row in data[table][\"table\"]:\n",
    "                row_relevant = False\n",
    "                for cell in row:\n",
    "                    if answer == re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", cell[\"value\"], flags=re.MULTILINE) or answer == cell[\"value\"]:\n",
    "                        row_relevant = True\n",
    "                        cell_ids.add(cell[\"id\"])\n",
    "                        break\n",
    "                        \n",
    "                if row_relevant:\n",
    "                    for cell in row:\n",
    "#                         print(cell[\"id\"], cell[\"value\"])\n",
    "                        cell_ids.add(cell[\"id\"])\n",
    "                        \n",
    "    return list(cell_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def table_to_2d(table_tag):\n",
    "    rowspans = []  # track pending rowspans\n",
    "    rows = table_tag.find_all('tr')\n",
    "\n",
    "    # first scan, see how many columns we need\n",
    "    colcount = 0\n",
    "    for r, row in enumerate(rows):\n",
    "        cells = row.find_all(['td', 'th'], recursive=False)\n",
    "        # count columns (including spanned).\n",
    "        # add active rowspans from preceding rows\n",
    "        # we *ignore* the colspan value on the last cell, to prevent\n",
    "        # creating 'phantom' columns with no actual cells, only extended\n",
    "        # colspans. This is achieved by hardcoding the last cell width as 1. \n",
    "        # a colspan of 0 means fill until the end but can really only apply\n",
    "        # to the last cell; ignore it elsewhere. \n",
    "        colcount = max(\n",
    "            colcount,\n",
    "            sum(int(c.get('colspan', 1)) or 1 for c in cells[:-1]) + len(cells[-1:]) + len(rowspans))\n",
    "        # update rowspan bookkeeping; 0 is a span to the bottom. \n",
    "        rowspans += [int(c.get('rowspan', 1)) or len(rows) - r for c in cells]\n",
    "        rowspans = [s - 1 for s in rowspans if s > 1]\n",
    "\n",
    "    # it doesn't matter if there are still rowspan numbers 'active'; no extra\n",
    "    # rows to show in the table means the larger than 1 rowspan numbers in the\n",
    "    # last table row are ignored.\n",
    "\n",
    "    # build an empty matrix for all possible cells\n",
    "    table = [[None] * colcount for row in rows]\n",
    "\n",
    "    # fill matrix from row data\n",
    "    rowspans = {}  # track pending rowspans, column number mapping to count\n",
    "    for row, row_elem in enumerate(rows):\n",
    "        span_offset = 0  # how many columns are skipped due to row and colspans \n",
    "        for col, cell in enumerate(row_elem.find_all(['td', 'th'], recursive=False)):\n",
    "            # adjust for preceding row and colspans\n",
    "            col += span_offset\n",
    "            while rowspans.get(col, 0):\n",
    "                span_offset += 1\n",
    "                col += 1\n",
    "\n",
    "            # fill table data\n",
    "            rowspan = rowspans[col] = int(cell.get('rowspan', 1)) or len(rows) - row\n",
    "            colspan = int(cell.get('colspan', 1)) or colcount - col\n",
    "            # next column is offset by the colspan\n",
    "            span_offset += colspan - 1\n",
    "            value = cell.get_text()\n",
    "            for drow, dcol in product(range(rowspan), range(colspan)):\n",
    "                try:\n",
    "                    table[row + drow][col + dcol] = value\n",
    "                    rowspans[col + dcol] = rowspan\n",
    "                except IndexError:\n",
    "                    # rowspan or colspan outside the confines of the table\n",
    "                    pass\n",
    "\n",
    "        # update rowspan bookkeeping\n",
    "        rowspans = {c: s - 1 for c, s in rowspans.items() if s > 1}\n",
    "\n",
    "    return table\n",
    "\n",
    "def get_html_table(table, remove_links):\n",
    "    html_table = \"<table>\"\n",
    "    for row in table:\n",
    "        html_table += \"<tr>\"\n",
    "        for cell in row:\n",
    "            if remove_links:\n",
    "                html_table += '<td rowspan=\"{}\" colspan=\"{}\">{}</td>'.format(cell[\"row_span\"], cell[\"column_span\"], re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", cell[\"value\"], flags=re.MULTILINE)) \n",
    "            else:\n",
    "                html_table += '<td rowspan=\"{}\" colspan=\"{}\">{}</td>'.format(cell[\"row_span\"], cell[\"column_span\"], cell[\"value\"])\n",
    "        html_table += \"</tr>\"\n",
    "                \n",
    "    html_table += \"</table>\"\n",
    "    return html_table\n",
    "\n",
    "# json.loads(sql_session.execute(\"select * from wiki where id = 'John Laurie'\").fetchone()[1])[\"table_1\"][\"table\"]\n",
    "\n",
    "def json_table_to_dict(table, remove_links=True, crop=False):\n",
    "    from bs4 import BeautifulSoup\n",
    "    if \"table\" in table:\n",
    "        table = table[\"table\"]\n",
    "    list_table = table_to_2d(BeautifulSoup(get_html_table(table, remove_links=remove_links)))\n",
    "    if not list_table:\n",
    "        return {}\n",
    "    else:\n",
    "#         print(list_table)\n",
    "        pass\n",
    "    \n",
    "    header_index = None\n",
    "    for row_id, row in enumerate(table):\n",
    "        if row and row[0][\"is_header\"] == True:\n",
    "            header_index = row_id\n",
    "            break\n",
    "            \n",
    "    list_table = list_table[header_index:]\n",
    "    \n",
    "#     if table[0][0][\"is_header\"] != True:\n",
    "#         return {}\n",
    "\n",
    "#         print(\"Tablo nce:\",list_table)\n",
    "#         max_n_col = 0\n",
    "#         for row in table:\n",
    "#             if len(row) > max_n_col:\n",
    "#                 max_n_col = len(row)\n",
    "#         header = [list(range(0, max_n_col))]\n",
    "#         print(header)\n",
    "#         list_table = header.extend(list_table)\n",
    "#         print(\"Tablo sonra:\",list_table)\n",
    "        \n",
    "    if crop:\n",
    "        table_cropped = []\n",
    "#         print(list_table)\n",
    "        for row in list_table:\n",
    "#             print(\"Row:\",row)\n",
    "            if len(\" \".join(row)) > 700:\n",
    "                continue\n",
    "            else:\n",
    "                table_cropped.append(row)\n",
    "    else:\n",
    "        table_cropped = list_table\n",
    "        \n",
    "    dict_table = {}\n",
    "    if not table_cropped:\n",
    "        return {}\n",
    "    for col_id, column in enumerate(table_cropped[0]):\n",
    "        dict_table[column] = [row[col_id] for row in table_cropped[1:]]\n",
    "    return dict_table\n",
    "\n",
    "def get_numpy_table(table, remove_links=True):\n",
    "    if \"table\" in table:\n",
    "        table = table[\"table\"]\n",
    "    num_col = len(table[0])\n",
    "    num_rows = len(table)\n",
    "    data_x =np.empty(shape=(num_rows,num_col), dtype=object )\n",
    "\n",
    "    for elements in table:\n",
    "        for element in elements:\n",
    "            id = element[\"id\"].split(\"_\")\n",
    "            for j in range(int(element[\"column_span\"])):\n",
    "                for i in range(int(element[\"row_span\"])):\n",
    "                    k=0\n",
    "                    while (data_x[int(id[-2])+i][int(id[-1])+j+k] !=None):\n",
    "                        if ((int(id[-1])+j+k+1)<num_col):\n",
    "                            k = k+1\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    if remove_links:\n",
    "                        data_x[int(id[-2])+i][int(id[-1])+j+k] = re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", element[\"value\"], flags=re.MULTILINE)\n",
    "                    else:\n",
    "                        data_x[int(id[-2])+i][int(id[-1])+j+k] = element[\"value\"]                                                  \n",
    "                    # print(element[\"value\"])\n",
    "\n",
    "        # print(element[\"value\"])\n",
    "    return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3712,\n",
       " 3775,\n",
       " 3776,\n",
       " 3930,\n",
       " 4212,\n",
       " 4893,\n",
       " 5152,\n",
       " 5159,\n",
       " 5218,\n",
       " 5247,\n",
       " 5330,\n",
       " 5336,\n",
       " 5457,\n",
       " 5574,\n",
       " 5689,\n",
       " 5731,\n",
       " 5756,\n",
       " 5773,\n",
       " 5802]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems = [212+3500,\n",
    "275+3500,\n",
    "276+3500,\n",
    "430+3500,\n",
    "712+3500,\n",
    "1393+3500,\n",
    "1652+3500,\n",
    "1659+3500,\n",
    "1718+3500,\n",
    "1747+3500,\n",
    "1830+3500,\n",
    "1836+3500,\n",
    "1957+3500,\n",
    "2074+3500,\n",
    "2189+3500,\n",
    "2231+3500,\n",
    "2256+3500,\n",
    "2273+3500,\n",
    "2302+3500]\n",
    "\n",
    "problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def embed_useT(module):\n",
    "    with tf.Graph().as_default():\n",
    "        sentences = tf.compat.v1.placeholder(tf.string)\n",
    "        embed = hub.Module(module)\n",
    "        embeddings = embed(sentences)\n",
    "        session = tf.compat.v1.train.MonitoredSession()\n",
    "    return lambda x: session.run(embeddings, {sentences: x})\n",
    "embed_fn = embed_useT('/home/titanx/kaggle/working/sentence_wise_email/module/module_useT')\n",
    "\n",
    "def get_answers(query, hit_dictionary, improved_name_check=False, include_previous_sentence=True, display_table=False):\n",
    "    import re\n",
    "    for idx,v in hit_dictionary.items():\n",
    "        abs_dirty = v['abstract']\n",
    "        real_abs_dirty = v['real_abstract']\n",
    "        #abs_dirty = v['paragraph']\n",
    "        # looks like the abstract value can be an empty list\n",
    "        v['abstract_paragraphs'] = []\n",
    "        v['abstract_full'] = ''\n",
    "        v['real_abstract_full'] = ''\n",
    "        v['real_abstract_paragraphs']=[]\n",
    "\n",
    "        if abs_dirty:\n",
    "            # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
    "            # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
    "            # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
    "\n",
    "            if isinstance(abs_dirty, list):\n",
    "                for p in abs_dirty:\n",
    "                    v['abstract_paragraphs'].append(p['text'])\n",
    "                    v['abstract_full'] += p['text'] + ' \\n\\n'\n",
    "\n",
    "            # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
    "            if isinstance(abs_dirty, str):\n",
    "                v['abstract_paragraphs'].append(abs_dirty)\n",
    "                v['abstract_full'] += abs_dirty + ' \\n\\n'\n",
    "        if real_abs_dirty:\n",
    "            # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
    "            # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
    "            # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
    "\n",
    "\n",
    "            if isinstance(real_abs_dirty, list):\n",
    "                for p in real_abs_dirty:\n",
    "                    v['real_abstract_paragraphs'].append(p['text'])\n",
    "                    v['real_abstract_full'] += p['text'] + ' \\n\\n'\n",
    "\n",
    "            # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
    "            if isinstance(abs_dirty, str):\n",
    "                v['real_abstract_paragraphs'].append(real_abs_dirty)\n",
    "                v['real_abstract_full'] += real_abs_dirty + ' \\n\\n'\n",
    "        v[\"indexed_para\"]=v[\"abstract_full\"][len(v[\"real_abstract_full\"]):]\n",
    "        \n",
    "        \n",
    "        v[\"abstract_full_processed\"] = re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", v[\"abstract_full\"], flags=re.MULTILINE) # Link format removal\n",
    "        v[\"abstract_full_processed\"] = re.sub(r\"((sentence|table|cell|section|list|item)(_[0-9]+)+ )\", \"\", v[\"abstract_full_processed\"], flags=re.MULTILINE)\n",
    "        v[\"abstract_full_processed\"] = v[\"abstract_full_processed\"].replace(\"~~\", \". ~~ .\")\n",
    "#     v[\"abstract_full_processed\"] = v[\"abstract_full_processed\"].replace(\"~~\", \" ~~ \")\n",
    "        \n",
    "\n",
    "#     def embed_useT(module):\n",
    "#         with tf.Graph().as_default():\n",
    "#             sentences = tf.compat.v1.placeholder(tf.string)\n",
    "#             embed = hub.Module(module)\n",
    "#             embeddings = embed(sentences)\n",
    "#             session = tf.compat.v1.train.MonitoredSession()\n",
    "#         return lambda x: session.run(embeddings, {sentences: x})\n",
    "#     embed_fn = embed_useT('/home/titanx/kaggle/working/sentence_wise_email/module/module_useT')\n",
    "\n",
    "    import numpy as np\n",
    "    #Reconstruct text for generating the answer text from the BERT result, do the necessary text preprocessing and construct the answer.\n",
    "    def reconstructText(tokens, start=0, stop=-1):\n",
    "        tokens = tokens[start: stop]\n",
    "        if '[SEP]' in tokens:\n",
    "            sepind = tokens.index('[SEP]')\n",
    "            tokens = tokens[sepind+1:]\n",
    "        txt = ' '.join(tokens)\n",
    "        txt = txt.replace(' ##', '')\n",
    "        txt = txt.replace('##', '')\n",
    "        txt = txt.strip()\n",
    "        txt = \" \".join(txt.split())\n",
    "        txt = txt.replace(' .', '.')\n",
    "        txt = txt.replace('( ', '(')\n",
    "        txt = txt.replace(' )', ')')\n",
    "        txt = txt.replace(' - ', '-')\n",
    "        txt_list = txt.split(' , ')\n",
    "        txt = ''\n",
    "        nTxtL = len(txt_list)\n",
    "        if nTxtL == 1:\n",
    "            return txt_list[0]\n",
    "        newList =[]\n",
    "        for i,t in enumerate(txt_list):\n",
    "            if i < nTxtL -1:\n",
    "                if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n",
    "                    newList += [t,',']\n",
    "                else:\n",
    "                    newList += [t, ', ']\n",
    "            else:\n",
    "                newList += [t]\n",
    "        return ''.join(newList)\n",
    "\n",
    "\n",
    "    def makeBERTSQuADPrediction(title, document, question):\n",
    "        title = get_canonical_title(title)\n",
    "        ## we need to rewrite this function so that it chuncks the document into 250-300 word segments with\n",
    "        ## 50 word overlaps on either end so that it can understand and check longer abstracts\n",
    "        ## Get chuck of documents in order to overcome the BERT's max 512 token problem. First count the number of tokens. For example if it is\n",
    "        ##   larger than 2048 token determine the split size and overlapping token count. Split the document into pieces to fed into BERT encoding. Overlapping\n",
    "        ##      is necessary to maintain the coherence between the splits. If the token count is larger than some number 2560 token approximately this will fail.\n",
    "        nWords = len(document.split())\n",
    "        input_ids_all = QA_TOKENIZER.encode(question, document)\n",
    "        tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n",
    "        overlapFac = 1.1\n",
    "        if len(input_ids_all)*overlapFac > 2560:\n",
    "            nSearchWords = int(np.ceil(nWords/6))\n",
    "            fifth = int(np.ceil(nWords/5))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[fifth-int(nSearchWords*overlapFac/2):fifth+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[fifth*2-int(nSearchWords*overlapFac/2):fifth*2+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[fifth*3-int(nSearchWords*overlapFac/2):fifth*3+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[fifth*4-int(nSearchWords*overlapFac/2):fifth*4+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "\n",
    "        elif len(input_ids_all)*overlapFac > 2048:\n",
    "            nSearchWords = int(np.ceil(nWords/5))\n",
    "            quarter = int(np.ceil(nWords/4))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
    "            \n",
    "        elif len(input_ids_all)*overlapFac > 1536:\n",
    "            nSearchWords = int(np.ceil(nWords/4))\n",
    "            third = int(np.ceil(nWords/3))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
    "            \n",
    "        elif len(input_ids_all)*overlapFac > 1024:\n",
    "            nSearchWords = int(np.ceil(nWords/3))\n",
    "            middle = int(np.ceil(nWords/2))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "        elif len(input_ids_all)*overlapFac > 512:\n",
    "            nSearchWords = int(np.ceil(nWords/2))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "        else:\n",
    "            input_ids = [input_ids_all]\n",
    "        absTooLong = False    \n",
    "        \n",
    "        answers = []\n",
    "        cons = []\n",
    "        #print(input_ids)\n",
    "        for iptIds in input_ids:\n",
    "            tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n",
    "            #print(tokens)\n",
    "            sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)\n",
    "            num_seg_a = sep_index + 1\n",
    "            num_seg_b = len(iptIds) - num_seg_a\n",
    "            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "            assert len(segment_ids) == len(iptIds)\n",
    "            n_ids = len(segment_ids)\n",
    "            #print(n_ids)\n",
    "            if n_ids < 512:\n",
    "                outputs = QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
    "                                        token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n",
    "                #print(QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
    "                #                        token_type_ids=torch.tensor([segment_ids]).to(torch_device)))\n",
    "            else:\n",
    "                #this cuts off the text if its more than 512 words so it fits in model space\n",
    "                #need run multiple inferences for longer text. add to the todo. I will try to expand this to longer sequences if it is necessary. \n",
    "#                 print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n",
    "                absTooLong = True\n",
    "                outputs= QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device), \n",
    "                                        token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))\n",
    "            start_scores=outputs.start_logits\n",
    "            end_scores=outputs.end_logits\n",
    "            start_scores = start_scores[:,1:-1]\n",
    "            end_scores = end_scores[:,1:-1]\n",
    "            answer_start = torch.argmax(start_scores)\n",
    "            answer_end = torch.argmax(end_scores)\n",
    "            #print(answer_start, answer_end)\n",
    "            answer = reconstructText(tokens, answer_start, answer_end+2)\n",
    "        \n",
    "            if answer.startswith('. ') or answer.startswith(', '):\n",
    "                answer = answer[2:]\n",
    "                \n",
    "            c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n",
    "            answers.append(answer)\n",
    "            cons.append(c)\n",
    "        \n",
    "        maxC = max(cons)\n",
    "        iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n",
    "        \n",
    "        entities = get_entities(question)\n",
    "        people = get_people(question)\n",
    "        \n",
    "        title = get_canonical_title(title)\n",
    "        \n",
    "#         entity_similarities = [Levenshtein.ratio(entity, title) for entity in entities]\n",
    "#         person_similarities = [Levenshtein.ratio(person, title) for person in people]\n",
    "        \n",
    "        person_found = False\n",
    "        \n",
    "        for person in people:\n",
    "#             print(person, title)\n",
    "            if person in title or title in person:\n",
    "#                 print(\"buldu\")\n",
    "                person_found = True\n",
    "                break\n",
    "        \n",
    "#         if person_similarities and max(person_similarities) >= 0.8:\n",
    "#         if title in people:\n",
    "        if person_found:\n",
    "#             confidence = max([0.95, cons[iMaxC]])\n",
    "            confidence = 2*cons[iMaxC]\n",
    "            \n",
    "#             print(confidence)\n",
    "#         elif entity_similarities and max(entity_similarities) >= 0.8:\n",
    "#             confidence = max([0.9, cons[iMaxC]])\n",
    "        else:\n",
    "            confidence = cons[iMaxC]\n",
    "            \n",
    "        answer = answers[iMaxC]\n",
    "        \n",
    "        sep_index = tokens_all.index('[SEP]')\n",
    "        full_txt_tokens = tokens_all[sep_index+1:]\n",
    "        \n",
    "        abs_returned = reconstructText(full_txt_tokens)\n",
    "\n",
    "        ans={}\n",
    "        ans['answer'] = answer\n",
    "        #print(answer)\n",
    "        if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n",
    "            ans['confidence'] = -1000000\n",
    "        else:\n",
    "            #confidence = torch.max(start_scores) + torch.max(end_scores)\n",
    "            #confidence = np.log(confidence.item())\n",
    "            ans['confidence'] = confidence\n",
    "        #ans['start'] = answer_start.item()\n",
    "        #ans['end'] = answer_end.item()\n",
    "        ans['abstract_bert'] = abs_returned\n",
    "        ans['abs_too_long'] = absTooLong\n",
    "        return ans\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    def searchAbstracts(hit_dictionary, question):\n",
    "        abstractResults = {}\n",
    "#         for k,v in tqdm(hit_dictionary.items()):\n",
    "        for k,v in hit_dictionary.items():\n",
    "#             abstract = v['abstract_full']\n",
    "            abstract = v['abstract_full_processed']\n",
    "#             print(abstract)\n",
    "            indexed_para=v['indexed_para']\n",
    "            title = v[\"title\"]\n",
    "            if abstract:\n",
    "                ans = makeBERTSQuADPrediction(title, abstract, question)\n",
    "                if ans['answer']:\n",
    "                    confidence = ans['confidence']\n",
    "#                     print(title,\"-\",confidence)\n",
    "                    abstractResults[confidence]={}\n",
    "                    abstractResults[confidence]['answer'] = ans['answer']\n",
    "                    #abstractResults[confidence]['start'] = ans['start']\n",
    "                    #abstractResults[confidence]['end'] = ans['end']\n",
    "                    abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n",
    "                    abstractResults[confidence]['idx'] = k\n",
    "                    abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n",
    "\n",
    "                    \n",
    "        cList = list(abstractResults.keys())\n",
    "        if cList:\n",
    "            maxScore = max(cList)\n",
    "            total = 0.0\n",
    "            exp_scores = []\n",
    "            for c in cList:\n",
    "                s = np.exp(c-maxScore)\n",
    "                exp_scores.append(s)\n",
    "            total = sum(exp_scores)\n",
    "            for i,c in enumerate(cList):\n",
    "                abstractResults[exp_scores[i]/total] = abstractResults.pop(c)\n",
    "                \n",
    "        return abstractResults\n",
    "\n",
    "    answers = searchAbstracts(hit_dictionary, query)\n",
    "\n",
    "    workingPath = '/root/kaggle/working'\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "    #from summarizer import Summarizer\n",
    "    #summarizerModel = Summarizer()\n",
    "    def displayResults(hit_dictionary, answers, question, improved_name_check=False, include_previous_sentence=True, display_table=False):\n",
    "        \n",
    "        question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n",
    "        #all_HTML_txt = question_HTML\n",
    "        confidence = list(answers.keys())\n",
    "        confidence.sort(reverse=True)\n",
    "        \n",
    "        page_answers = dict()\n",
    "\n",
    "        for c in confidence:\n",
    "#             if c>0 and c <= 1 and len(answers[c]['answer']) != 0:\n",
    "            if c>0 and len(answers[c]['answer']) != 0:\n",
    "                if 'idx' not in  answers[c]:\n",
    "                    continue\n",
    "                rowData = []\n",
    "                idx = answers[c]['idx']\n",
    "                title = hit_dictionary[idx]['title']\n",
    "                \n",
    "                full_abs = answers[c]['abstract_bert']\n",
    "                bert_ans = answers[c]['answer']\n",
    "                \n",
    "                \n",
    "                #split_abs = full_abs.split(bert_ans)\n",
    "                #x=(split_abs[0][:split_abs[0].rfind('. ')+1])\n",
    "                #sentance_beginning = x[x.rfind('. ')+1:] + split_abs[0][split_abs[0].rfind('. ')+1:]\n",
    "                #sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n",
    "                x=''\n",
    "                y=''\n",
    "                z=''\n",
    "                t=''\n",
    "\n",
    "                # print(bert_ans)\n",
    "                split_abs = full_abs.split(bert_ans)\n",
    "                \n",
    "                if split_abs[0][-5:] == \"~ ~. \":\n",
    "                    answer_sentence_start = True\n",
    "                else:\n",
    "                    answer_sentence_start = False\n",
    "                \n",
    "#                 if get_canonical_title(title) == \"Abraham Annan\":\n",
    "#                     print(\"Buraya bak:\",split_abs[0])\n",
    "#                     print(split_abs[0][-5:])\n",
    "                    \n",
    "                if include_previous_sentence and len(split_abs[0].split(\"~ ~\")) > 1:\n",
    "                    sentance_beginning = \" ~ ~ \".join(split_abs[0].split(\"~ ~\")[-2:]) # Also includes the previous sentence\n",
    "                else:\n",
    "                    sentance_beginning = split_abs[0].split(\"~ ~\")[-1] # Only the sentence that includes the answer\n",
    "                if len(split_abs) > 1:\n",
    "                    sentance_end = split_abs[1].split(\"~ ~\")[0]\n",
    "                else:\n",
    "                    sentance_end = \"\"\n",
    "                \n",
    "                if answer_sentence_start:\n",
    "                    sentance_full = sentance_beginning + \"~ ~\" + bert_ans + sentance_end\n",
    "                else:\n",
    "                    sentance_full = sentance_beginning + bert_ans + sentance_end\n",
    "                    \n",
    "#                 print(sentance_full)\n",
    "                \n",
    "                sentences = sentance_full.split(\"~ ~\")\n",
    "                \n",
    "                abstract_full = hit_dictionary[idx][\"abstract_full\"]\n",
    "                \n",
    "                all_answer_ids = set()\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                \n",
    "                    for element in abstract_full.split(\" ~~ \"):\n",
    "                        element = re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", element, flags=re.MULTILINE) # Link format removal\n",
    "                        answer_ids = re.findall(r\"((sentence|table|cell|section|list|item)(_[0-9]+)+ )\", element)\n",
    "\n",
    "                        if not answer_ids:\n",
    "                            continue\n",
    "\n",
    "                        answer_ids_formatted = set()\n",
    "\n",
    "                        for ids in answer_ids:\n",
    "                            if isinstance(ids, str):\n",
    "                                candidate = ids.replace(\" \", \"\")\n",
    "                                if \"_\" in candidate and candidate[0] != \"_\" and candidate not in [\"sentence\", \"table\", \"cell\", \"section\", \"list\", \"item\"]:  # very hacky\n",
    "                                    answer_ids_formatted.add(candidate)\n",
    "                            else:\n",
    "                                for id in ids:\n",
    "                                    candidate = id.replace(\" \", \"\")\n",
    "                                    if \"_\" in candidate and candidate[0] != \"_\" and candidate not in [\"sentence\", \"table\", \"cell\", \"section\", \"list\", \"item\"]:  # very hacky\n",
    "                                        answer_ids_formatted.add(candidate)\n",
    "                                        \n",
    "                                        \n",
    "#                         if get_canonical_title(title) == \"Abraham Annan\":\n",
    "#                             print(\"Abraham Annan\",answer_ids, answer_ids_formatted)\n",
    "                                        \n",
    "#                         all_answer_ids.extend(answer_ids_formatted)\n",
    "#                         print(answer_ids_formatted, title)\n",
    "                        for answer_id_formatted in answer_ids_formatted:\n",
    "#                             if get_canonical_title(title) == \"Abraham Annan\":\n",
    "#                                 print(element)\n",
    "#                                 print(answer_id_formatted)\n",
    "#                                 print(element.replace(answer_id_formatted, \"\").strip())\n",
    "#                                 print(sentence)\n",
    "#                                 print(\"=======\")\n",
    "                            if Levenshtein.ratio(element.replace(answer_id_formatted, \"\").strip(), sentence) >= 0.9:\n",
    "                                all_answer_ids.add(answer_id_formatted)\n",
    "                \n",
    "                \n",
    "#                 sentence_pre = abstract_full.split(sentance_full)[0].split(\"~~\")[-1]\n",
    "                \n",
    "                \n",
    "#                 print(sentance_full, full_abs)\n",
    "\n",
    "#                 answer_ids = re.findall(r\"((sentence|table|cell|section|list|item)( _ [0-9]+)+ )\", sentance_full)\n",
    "#                 answer_ids = re.findall(r\"((sentence|table|cell|section|list|item)(_[0-9]+)+ )\", sentence_pre)\n",
    "#                 answer_ids_formatted = []\n",
    "\n",
    "#                 for ids in answer_ids:\n",
    "#                     if isinstance(ids, str):\n",
    "#                         candidate = ids.replace(\" \", \"\")\n",
    "#                         if \"_\" in candidate and candidate[0] != \"_\" and candidate not in [\"sentence\", \"table\", \"cell\", \"section\", \"list\", \"item\"]:  # very hacky\n",
    "#                             answer_ids_formatted.append(candidate)\n",
    "#                     else:\n",
    "#                         for id in ids:\n",
    "#                             candidate = id.replace(\" \", \"\")\n",
    "#                             if \"_\" in candidate and candidate[0] != \"_\" and candidate not in [\"sentence\", \"table\", \"cell\", \"section\", \"list\", \"item\"]:  # very hacky\n",
    "#                                 answer_ids_formatted.append(candidate)\n",
    "                                \n",
    "#                 print(answer_ids_formatted)\n",
    "                \n",
    "#                 page_answers[title] = answer_ids_formatted\n",
    "                page_answers[get_canonical_title(title)] = list(all_answer_ids)\n",
    "#                 page_answers[title] = element_id\n",
    "\n",
    "                answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n",
    "                answers[c]['partial_answer'] = bert_ans+sentance_end\n",
    "                answers[c]['sentence_beginning'] = sentance_beginning\n",
    "                answers[c]['sentence_end'] = sentance_end\n",
    "                answers[c]['title'] = get_canonical_title(title)\n",
    "            else:\n",
    "                answers.pop(c)\n",
    "        \n",
    "        \n",
    "        ## now rerank based on semantic similarity of the answers to the question\n",
    "        ## Universal sentence encoder\n",
    "        cList = list(answers.keys())\n",
    "        allAnswers = [answers[c]['full_answer'] for c in cList]\n",
    "        \n",
    "        messages = [question]+allAnswers\n",
    "        \n",
    "        encoding_matrix = embed_fn(messages)\n",
    "        similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n",
    "        rankings = similarity_matrix[1:,0]\n",
    "        \n",
    "        for i,c in enumerate(cList):\n",
    "            answers[rankings[i]] = answers.pop(c)\n",
    "\n",
    "        ## now form pandas dv\n",
    "        confidence = list(answers.keys())\n",
    "        confidence.sort(reverse=True)\n",
    "        pandasData = []\n",
    "        ranked_aswers = []\n",
    "        full_abs_list=[]\n",
    "        for c in confidence:\n",
    "            rowData=[]\n",
    "            title = answers[c]['title']\n",
    "            idx = answers[c]['idx']\n",
    "            rowData += [idx]            \n",
    "            sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='red'>\"+answers[c]['answer']+\"</font> \"+answers[c]['sentence_end']+'</div>'\n",
    "            answer_key = answers[c]['answer'].split(\"\\t\")[-1].strip() if \"\\t\" in answers[c]['answer'] else answers[c]['answer'].strip()\n",
    "\n",
    "            # rowData += [sentance_html, answer_key, c,title]\n",
    "            rowData += [sentance_html, c,title]\n",
    "            pandasData.append(rowData)\n",
    "            ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n",
    "            full_abs_list.append(' '.join([answers[c]['abstract_bert']]))\n",
    "        \n",
    "        people = get_people(question)\n",
    "        \n",
    "        for row_id, row in enumerate(pandasData):\n",
    "            title = row[3]\n",
    "            for person in people:\n",
    "                if improved_name_check and (title in person or person in title or Levenshtein.ratio(title.split(\" (\")[0], person) >= 0.8):\n",
    "                    pandasData[row_id][2] = pandasData[row_id][2] * 2\n",
    "                    break\n",
    "                elif title in person or person in title:\n",
    "                    pandasData[row_id][2] = pandasData[row_id][2] * 2\n",
    "                    break\n",
    "                    \n",
    "        pandasData.sort(key=lambda x: float(x[2]), reverse=True)\n",
    "        \n",
    "        pdata2 = pandasData\n",
    "            \n",
    "        if display_table:\n",
    "            display(HTML(question_HTML))\n",
    "\n",
    "        #    display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>Body of Evidence:</b></div>'))\n",
    "\n",
    "            # df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Answer Key', 'Confidence', 'Title/Link'])\n",
    "            df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])\n",
    "\n",
    "            display(HTML(df.to_html(render_links=True, escape=False)))\n",
    "        return page_answers,full_abs_list,ranked_aswers,pandasData\n",
    "\n",
    "    return displayResults(hit_dictionary, answers, query, improved_name_check, include_previous_sentence, display_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# depth or breadth\n",
    "# EVIDENCE_RETRIEVAL = \"breadth\"\n",
    "EVIDENCE_RETRIEVAL = \"depth\"\n",
    "\n",
    "EVAL = True\n",
    "\n",
    "# with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#     f.write('{\"id\": \"\", \"predicted_label\": \"\", \"predicted_evidence\": \"\"}\\n')\n",
    "\n",
    "# if os.path.isfile(output_path):\n",
    "#     os.remove(output_path)\n",
    "    \n",
    "# if EVAL and os.path.isfile(evaluation_path):\n",
    "#     os.remove(evaluation_path)\n",
    "    \n",
    "from tqdm import tqdm\n",
    "for data in tqdm(dataset[101:]):\n",
    "    \n",
    "    claim = data[\"claim\"]\n",
    "    \n",
    "    if not claim:\n",
    "        continue\n",
    "        \n",
    "#     print(claim)\n",
    "    \n",
    "    hit_stats, hit_dictionary = get_relevant_pages(claim, N_RESULTS=10)\n",
    "    \n",
    "#     hit_dictionary\n",
    "#     break\n",
    "    \n",
    "    page_answers, full_abs_list, ranked_aswers, pandasData = get_answers(claim, hit_dictionary, improved_name_check=True, include_previous_sentence=True, display_table=False)\n",
    "#     print(page_answers)\n",
    "\n",
    "#     print(pandasData)\n",
    "#     continue\n",
    "\n",
    "    entities = get_entities(claim)\n",
    "    people = get_people(claim)\n",
    "\n",
    "    max_cell_evidence = 25\n",
    "    \n",
    "    cell_evidences = []\n",
    "    for row in pandasData:\n",
    "        if row[2] < 0.5:\n",
    "            continue\n",
    "            \n",
    "        page = row[3]\n",
    "        if len(cell_evidences) >= max_cell_evidence:\n",
    "            break\n",
    "            \n",
    "        cells = get_relevant_cells(page, [entity for entity in entities if entity not in people])\n",
    "        \n",
    "        try:\n",
    "            model_cells = get_table_evidence(page, claim)\n",
    "        except:\n",
    "            model_cells = []\n",
    "        \n",
    "        \n",
    "#         if not cells and not model_cells:\n",
    "#             continue\n",
    "            \n",
    "#         if not cells:\n",
    "#             continue\n",
    "        \n",
    "        for cell in model_cells:\n",
    "            if cell not in cells:\n",
    "                cells.append(cell)\n",
    "        \n",
    "        cells = cells[0:min([len(cells),(max_cell_evidence-len(cell_evidences))])]\n",
    "        \n",
    "        \n",
    "        for cell in cells:\n",
    "            cell_evidences.append([page, cell.split(\"_\", 1)[0], cell.split(\"_\", 1)[1]])\n",
    "        \n",
    "#         cell_evidences.extend(cells[0:min([len(cells),(max_cell_evidence-len(cell_evidences))])])\n",
    "        \n",
    "    cell_evidences = cell_evidences[0:max_cell_evidence] # Just to be sure\n",
    "#     print(cell_evidences)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(cell_evidences)\n",
    "\n",
    "    evidence_probs, verdict = CheckEntailmentNeturalorContradict(ranked_aswers,claim,pandasData)\n",
    "\n",
    "    if verdict == \"False\":\n",
    "        predicted_label = \"REFUTES\"\n",
    "    elif verdict == \"True\":\n",
    "        predicted_label = \"SUPPORTS\"\n",
    "    else:\n",
    "        predicted_label = \"NOT ENOUGH INFO\"\n",
    "        \n",
    "#     entities = get_entities(claim)\n",
    "    people = get_people(claim)\n",
    "    \n",
    "    for row_id, row in enumerate(pandasData):\n",
    "        title = row[3]\n",
    "        for person in people:\n",
    "            if title in person or person in title:\n",
    "                pandasData[row_id][2] = pandasData[row_id][2] * 2\n",
    "                break\n",
    "    \n",
    "    pages_by_confidence = [row[0] for row in pandasData]\n",
    "    \n",
    "    \n",
    "#     print(pandasData[0][2])\n",
    "#     print(pages_by_confidence)\n",
    "#     print(page_answers[\"Abraham Annan\"])\n",
    "    \n",
    "    evidences = []\n",
    "    \n",
    "    if EVIDENCE_RETRIEVAL == \"depth\":\n",
    "        for page in pages_by_confidence:\n",
    "            page = get_canonical_title(page)\n",
    "            idx = page_answers[page]\n",
    "            for element in idx:\n",
    "                if len(evidences) < 5:\n",
    "                    element_type, element_id = element.split(\"_\", 1)\n",
    "                    evidence = [page, element_type, element_id]\n",
    "#                     if len(evidences) == 0:\n",
    "#                         print(evidence)\n",
    "                    if evidence not in evidences:\n",
    "                        evidences.append(evidence)\n",
    "                else:\n",
    "                    break\n",
    "    else:\n",
    "        while len(evidences) < 5:\n",
    "            for page in pages_by_confidence:\n",
    "                page = get_canonical_title(page)\n",
    "                idx = page_answers[page]\n",
    "                for element in idx:\n",
    "                    if len(evidences) < 5:\n",
    "                        element_type, element_id = element.split(\"_\", 1)\n",
    "                        evidence = [page, element_type, element_id]\n",
    "                        if evidence not in evidences:\n",
    "                            evidences.append(evidence)\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        break\n",
    "        \n",
    "    \n",
    "    evidences.extend(cell_evidences) # Cell evidences are included\n",
    "    \n",
    "    output = {\"predicted_label\": predicted_label, \"predicted_evidence\": evidences}\n",
    "    \n",
    "    with open(output_path, 'a', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=None)\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "    \n",
    "    if EVAL and data[\"label\"] and data[\"evidence\"]:\n",
    "\n",
    "        evaluation = {\"claim\": data[\"claim\"], \"label\": data[\"label\"], \"predicted_label\": predicted_label, \n",
    "                      \"evidence\": data[\"evidence\"], \"predicted_evidence\": []}\n",
    "        \n",
    "        for evidence in evidences:\n",
    "            evidence_str = evidence[0] + \"_\" + evidence[1] + \"_\" + evidence[2]\n",
    "            evaluation[\"predicted_evidence\"].append(evidence_str)\n",
    "\n",
    "        with open(evaluation_path, 'a', encoding='utf-8') as f:\n",
    "            json.dump(evaluation, f, ensure_ascii=False, indent=None)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "#     print(claim)\n",
    "#     print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line in the original file: 5802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:04<00:00,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Emblem of Andalusia', 'sentence', '0'], ['Genalguacil', 'sentence', '0'], ['Genalguacil', 'cell', '0_1_1'], ['Genalguacil', 'header', 'cell_0_3_0'], ['Genalguacil', 'header', 'cell_0_1_0']]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "skipped_row = 3500+2302\n",
    "for data in tqdm(list(dataset[skipped_row-1:skipped_row])):\n",
    "    print(\"Line in the original file:\",skipped_row)\n",
    "    \n",
    "    claim = data[\"claim\"]\n",
    "    \n",
    "    if not claim:\n",
    "        continue\n",
    "        \n",
    "#     print(claim)\n",
    "    \n",
    "    hit_stats, hit_dictionary = get_relevant_pages(claim, N_RESULTS=10)\n",
    "    \n",
    "#     hit_dictionary\n",
    "#     break\n",
    "    \n",
    "    page_answers, full_abs_list, ranked_aswers, pandasData = get_answers(claim, hit_dictionary, display_table=False)\n",
    "#     print(page_answers)\n",
    "\n",
    "#     print(pandasData)\n",
    "#     continue\n",
    "\n",
    "    entities = get_entities(claim)\n",
    "    people = get_people(claim)\n",
    "\n",
    "    max_cell_evidence = 25\n",
    "    \n",
    "    cell_evidences = []\n",
    "    for row in pandasData:\n",
    "        if row[2] < 0.5:\n",
    "            continue\n",
    "            \n",
    "        page = row[3]\n",
    "        if len(cell_evidences) >= max_cell_evidence:\n",
    "            break\n",
    "            \n",
    "        cells = get_relevant_cells(page, [entity for entity in entities if entity not in people])\n",
    "        \n",
    "        model_cells = get_table_evidence(page, claim)\n",
    "        \n",
    "#         if not cells and not model_cells:\n",
    "#             continue\n",
    "            \n",
    "#         if not cells:\n",
    "#             continue\n",
    "        \n",
    "        for cell in model_cells:\n",
    "            if cell not in cells:\n",
    "                cells.append(cell)\n",
    "        \n",
    "        cells = cells[0:min([len(cells),(max_cell_evidence-len(cell_evidences))])]\n",
    "        \n",
    "        \n",
    "        for cell in cells:\n",
    "            cell_evidences.append([page, cell.split(\"_\", 1)[0], cell.split(\"_\", 1)[1]])\n",
    "        \n",
    "#         cell_evidences.extend(cells[0:min([len(cells),(max_cell_evidence-len(cell_evidences))])])\n",
    "        \n",
    "    cell_evidences = cell_evidences[0:max_cell_evidence] # Just to be sure\n",
    "#     print(cell_evidences)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(cell_evidences)\n",
    "\n",
    "    evidence_probs, verdict = CheckEntailmentNeturalorContradict(ranked_aswers,claim,pandasData)\n",
    "\n",
    "    if verdict == \"False\":\n",
    "        predicted_label = \"REFUTES\"\n",
    "    elif verdict == \"True\":\n",
    "        predicted_label = \"SUPPORTS\"\n",
    "    else:\n",
    "        predicted_label = \"NOT ENOUGH INFO\"\n",
    "        \n",
    "#     entities = get_entities(claim)\n",
    "    people = get_people(claim)\n",
    "    \n",
    "    for row_id, row in enumerate(pandasData):\n",
    "        title = row[3]\n",
    "        for person in people:\n",
    "            if title in person or person in title:\n",
    "                pandasData[row_id][2] = pandasData[row_id][2] * 2\n",
    "                break\n",
    "    \n",
    "    pages_by_confidence = [row[0] for row in pandasData]\n",
    "    \n",
    "    \n",
    "#     print(pandasData[0][2])\n",
    "#     print(pages_by_confidence)\n",
    "#     print(page_answers[\"Abraham Annan\"])\n",
    "    \n",
    "    evidences = []\n",
    "    \n",
    "    if EVIDENCE_RETRIEVAL == \"depth\":\n",
    "        for page in pages_by_confidence:\n",
    "            page = get_canonical_title(page)\n",
    "            idx = page_answers[page]\n",
    "            for element in idx:\n",
    "                if len(evidences) < 5:\n",
    "                    element_type, element_id = element.split(\"_\", 1)\n",
    "                    evidence = [page, element_type, element_id]\n",
    "#                     if len(evidences) == 0:\n",
    "#                         print(evidence)\n",
    "                    if evidence not in evidences:\n",
    "                        evidences.append(evidence)\n",
    "                else:\n",
    "                    break\n",
    "    else:\n",
    "        while len(evidences) < 5:\n",
    "            for page in pages_by_confidence:\n",
    "                page = get_canonical_title(page)\n",
    "                idx = page_answers[page]\n",
    "                for element in idx:\n",
    "                    if len(evidences) < 5:\n",
    "                        element_type, element_id = element.split(\"_\", 1)\n",
    "                        evidence = [page, element_type, element_id]\n",
    "                        if evidence not in evidences:\n",
    "                            evidences.append(evidence)\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        break\n",
    "        \n",
    "    \n",
    "    evidences.extend(cell_evidences) # Cell evidences are included\n",
    "    \n",
    "    output = {\"predicted_label\": predicted_label, \"predicted_evidence\": evidences}\n",
    "    print(output)\n",
    "    \n",
    "    with open(\"error_manual.jsonl\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=None)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "#     print(claim)\n",
    "#     print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Buryat language'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher.search(\"The number three is \\\"Gurba(n)\\\" in Classical Mongolian, \\\"Gurav\\\" in Khalka, and \\\"Gurb\\\" in Buryat.\", k=10)[0].docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_3500_reverse.jsonl\", \"r\") as read_file:\n",
    "    lines = read_file.readlines()\n",
    "    \n",
    "with open(\"test_3500_reordered.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for line in reversed(lines):\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_session.execute(\"select * from wiki where id = 'Vtor Oliveira'\").fetchone()\n",
    "\n",
    "# json.loads(sql_session.execute(\"select * from wiki where id = 'John Laurie'\").fetchone()[1])[\"table_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_table_evidence(\"John Laurie\", \"In John Laurie's partial television credits, he was part of the Bees on the Boat-Deck in 1939 and Jackanory in 1971.\")\n",
    "                    \n",
    "#         display(table)\n",
    "#         print(\"\")\n",
    "#         for query, answer, predicted_agg in zip(queries, answers, aggregation_predictions_string):\n",
    "#            print(query)\n",
    "#            if predicted_agg == \"NONE\":\n",
    "#              print(\"Predicted answer: \" + answer)\n",
    "#            else:\n",
    "#              print(\"Predicted answer: \" + predicted_agg + \" > \" + answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_table = table_to_2d(BeautifulSoup(get_html_table(json.loads(sql_session.execute(\"select * from wiki where id = 'John Laurie'\").fetchone()[1])[\"table_1\"][\"table\"])))\n",
    "\n",
    "# dict_table = {}\n",
    "\n",
    "# for row_id, row in enumerate(list_table):\n",
    "#     if row_id == 0:\n",
    "#         for col_id, column in enumerate(row):\n",
    "#             dict_table[column] = [row[col_id] for row in list_table[1:]]\n",
    "#     break\n",
    "    \n",
    "# dict_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
