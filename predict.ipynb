{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "torch_device = 'cuda'\n",
    "\n",
    "QA_TOKENIZER = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1-squad\")\n",
    "QA_MODEL = AutoModelForQuestionAnswering.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1-squad\")\n",
    "\n",
    "QA_MODEL.to(torch_device)\n",
    "QA_MODEL.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c75613c0f942b09ec0fd55ab2c659f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=898822.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d136da4049bb4a138e5e7f531f00026f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=456318.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a304b92e0a24b58b602f1e82be6f28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87ae03875e34a838508513e425ef784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1802.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ef6eca91e04b59abfe66ffc994437d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1222317369.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f952078fb92a4a1e8bfc308796eb1506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=24.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256307483b164bc2a0e255d1618eb7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=948.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e42f3c23774cb48ec65b3a2a8525f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=798011.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee381f1e6f94401a85a6a4bfcd561214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=202.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc011ddab824a89b4225c77301416e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1445439643.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 1024)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (12): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (13): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (14): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (15): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (16): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (17): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (18): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (19): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (20): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (21): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (22): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (23): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "#from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "#SUMMARY_TOKENIZER = PegasusTokenizer.from_pretrained('mayu0007/pegasus_large_covid')\n",
    "#SUMMARY_MODEL = PegasusForConditionalGeneration.from_pretrained('mayu0007/pegasus_large_covid')\n",
    "\n",
    "SUMMARY_MODEL.to(torch_device)\n",
    "SUMMARY_MODEL.eval()\n",
    "def Summarizer(string_all):    \n",
    "    if 1==1:\n",
    "        ## try generating an exacutive summary with bart abstractive summarizer\n",
    "        allAnswersTxt = string_all.replace('\\n','')\n",
    "\n",
    "        answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n",
    "        #answers_input_ids = SUMMARY_TOKENIZER.prepare_seq2seq_batch([allAnswersTxt], return_tensors='pt', truncation=True, padding='longest')['input_ids'].to(torch_device)\n",
    "        summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n",
    "                                               num_beams=4,\n",
    "                                               length_penalty=0.8,\n",
    "                                               repetition_penalty=2.0,\n",
    "                                               min_length=5,\n",
    "                                               no_repeat_ngram_size=0,\n",
    "                                                do_sample=False )\n",
    "\n",
    "        exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "        return exec_sum\n",
    "\n",
    "# Program to find most frequent  \n",
    "# element in a list \n",
    "  \n",
    "from collections import Counter \n",
    "  \n",
    "def most_frequent(List): \n",
    "    occurence_count = Counter(List) \n",
    "    return occurence_count.most_common(1)[0][0]\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,AutoModelForSequenceClassification\n",
    "#hg_model_hub_name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "hg_model_hub_name = \"ynie/xlnet-large-cased-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "\n",
    "tokenizer_nli = AutoTokenizer.from_pretrained(hg_model_hub_name, use_fast=False)\n",
    "model_nli = AutoModelForSequenceClassification.from_pretrained(hg_model_hub_name)\n",
    "model_nli.to(torch_device)\n",
    "model_nli.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import pysearch\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from anglicize import anglicize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import statistics\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"dev.jsonl\"\n",
    "output_path = dataset_path.replace(\".jsonl\", \"_predictions.jsonl\")\n",
    "\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sqla\n",
    "\n",
    "db_fullpath = \"feverous_wikiv1.db\"\n",
    "db = sqla.create_engine(\"sqlite:///{}\".format(db_fullpath))\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "Session = sessionmaker(bind=db)\n",
    "sql_session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucene_dir = 'anserini/indexes/fever/lucene-index-fever-paragraph'\n",
    "searcher = pysearch.SimpleSearcher(lucene_dir)\n",
    "\n",
    "def get_relevant_pages(claim, FULL_CLAIM=True, SPACY_ENTITIES=True, CASE_ENTITIES=True, LINKED_PAGES=False, LINKING_PAGES=False, N_RESULTS=70):\n",
    "    \n",
    "    if not SPACY_ENTITIES and not CASE_ENTITIES and not LINKED_PAGES:\n",
    "        FULL_CLAIM = True\n",
    "    \n",
    "    start_timer = timer()\n",
    "\n",
    "    claim = nlp(claim.replace(\" \", \" \").replace(\"­\", \" \")) # Replaces the obnoxious space character with normal space\n",
    "\n",
    "    keywords = set()\n",
    "    entities = set()\n",
    "\n",
    "    if SPACY_ENTITIES:\n",
    "        spacy_entities = [entity.text for entity in claim.ents]\n",
    "        entities.update(spacy_entities)\n",
    "\n",
    "    if CASE_ENTITIES:\n",
    "        case_entities = set()\n",
    "        chunks = claim.noun_chunks\n",
    "        for chunk in chunks:\n",
    "            for token in tokenizer(chunk.text):\n",
    "                if token.text[0].isupper():\n",
    "                    case_entities.add(chunk.text)\n",
    "                    break\n",
    "\n",
    "        entities.update(case_entities)\n",
    "        #print(case_entities)\n",
    "        #print(entities)\n",
    "        #sys.exit(0)\n",
    "\n",
    "    keywords.update(entities)\n",
    "\n",
    "    if not FULL_CLAIM:\n",
    "        search_query = (\", \".join(keywords) + '\"' + '\", \"'.join(entities) + '\"')\n",
    "#            search_query = (\", \".join(keywords) + \", \".join(entities))\n",
    "    else:\n",
    "        search_query = (claim.text + ' \"' + '\", \"'.join(keywords) + '\" \"' + '\", \"'.join(entities) + '\"')\n",
    "#            search_query = (claim.text + \" \" + \", \".join(keywords) + \", \".join(entities))\n",
    "\n",
    "    if FULL_CLAIM or keywords:\n",
    "        try:\n",
    "            lucene_hits = searcher.search(search_query.encode(\"utf-8\"), k=N_RESULTS)\n",
    "        except:\n",
    "            lucene_hits = None\n",
    "    else:\n",
    "        lucene_hits = None\n",
    "\n",
    "    hit_dictionary = {}\n",
    "\n",
    "    if lucene_hits:\n",
    "\n",
    "        for hit in lucene_hits:\n",
    "            hit_dict = {\"abstract\": None, \"real_abstract\": None, \"title\": None}\n",
    "            hit_dict['abstract']=hit.lucene_document.get(\"raw\")\n",
    "            hit_dict['real_abstract']=hit.lucene_document.get(\"raw\")\n",
    "            hit_dict['title'] = str(hit.docid)\n",
    "            hit_dictionary[str(hit.docid)] = hit_dict\n",
    "    \n",
    "        linked_pages = set()\n",
    "        if LINKED_PAGES:\n",
    "            for hit in lucene_hits:\n",
    "                links = re.findall(r\"(?:\\[\\[)(.*?)(?:\\|)\", hit.raw)\n",
    "                for link in links:\n",
    "                    linked_pages.update([link.replace(\"_\", \" \").encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\")])\n",
    "                    \n",
    "        linking_pages = set()\n",
    "        if LINKING_PAGES and sql_session:\n",
    "            linking_results = sql_session.execute('select target, sources from inlinks where target IN (\"{}\")'.format('\", \"'.join([hit.docid.replace('\"', '\"\"') for hit in lucene_hits])))\n",
    "            for row in linking_results:\n",
    "                linking_pages.update(row[\"sources\"].split(\";\"))\n",
    "                    \n",
    "        found_pages = [hit.docid for hit in lucene_hits]\n",
    "        for i in range(0, len(found_pages)):\n",
    "\n",
    "            try:\n",
    "                found_pages[i] = found_pages[i].encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "                #print(\"Done:\",found_pages[i])\n",
    "            except:\n",
    "                import regex\n",
    "                print(found_pages[i], anglicize(found_pages[i]), regex.sub(r'[^\\p{Latin}]', '', found_pages[i]).encode(\"latin-1\").decode(\"utf-8\"))\n",
    "                \n",
    "        found_pages = set(found_pages)\n",
    "\n",
    "        hit_scores = [hit.score for hit in lucene_hits]\n",
    "        hit_score_min = min(hit_scores)\n",
    "        hit_score_25 = np.percentile(hit_scores, 25)\n",
    "        hit_score_mean = np.mean(hit_scores)\n",
    "        hit_score_median = statistics.median(hit_scores)\n",
    "        hit_score_75 = np.percentile(hit_scores, 75)\n",
    "        hit_score_max = max(hit_scores)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        is_found = False\n",
    "        found_pages = set()\n",
    "        entities = set()\n",
    "        keywords = set()\n",
    "        linked_pages = set()\n",
    "        linking_pages = set()\n",
    "        lucene_hits = []\n",
    "        hit_scores = []\n",
    "        hit_score_min = None\n",
    "        hit_score_25 = None\n",
    "        hit_score_mean = None\n",
    "        hit_score_median = None\n",
    "        hit_score_75 = None\n",
    "        hit_score_max = None\n",
    "\n",
    "    end_timer = timer()\n",
    "    elapsed = int(end_timer - start_timer)\n",
    "    elapsed_formatted = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))\n",
    "        \n",
    "    result = [claim.text, elapsed_formatted, \n",
    "                found_pages, \n",
    "                keywords, \n",
    "                linked_pages, \n",
    "                linking_pages,\n",
    "                len(lucene_hits), \n",
    "                hit_score_min, hit_score_25, hit_score_mean, hit_score_median, \n",
    "                hit_score_75, hit_score_max, hit_scores]\n",
    "\n",
    "    #print(result)\n",
    "    \n",
    "    result = pd.DataFrame([result], columns=[\"CLAIM\", \"ELAPSED\", \n",
    "                                             \"PAGES_FOUND\",\n",
    "                                             \"KEYWORDS\", \n",
    "                                             \"LINKED_PAGES\",\n",
    "                                             \"LINKING_PAGES\",\n",
    "                                             \"N_LUCENE_HITS\", \n",
    "                                             \"HIT_SCORE_MIN\", \"HIT_SCORE_25\", \"HIT_SCORE_MEAN\", \"HIT_SCORE_MEDIAN\", \n",
    "                                             \"HIT_SCORE_75\", \"HIT_SCORE_MAX\", \"HIT_SCORES\"])\n",
    "    \n",
    "    return result, hit_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckEntailmentNeturalorContradict(ranked_aswers, string_all, pandasData):\n",
    "    if 1 == 1:\n",
    "        premise_list = []\n",
    "        hypothesis_list = []\n",
    "        entailment_prob = []\n",
    "        neutral_prob = []\n",
    "        contradict_prob = []\n",
    "        final_result_list = []\n",
    "        if len(ranked_aswers) > 8:\n",
    "            v_range = 8\n",
    "        else:\n",
    "            v_range = len(ranked_aswers)\n",
    "        for i in range(v_range):\n",
    "            if pandasData[i][2] >= 0.4:\n",
    "                max_length = 512\n",
    "                candidate_answer = Summarizer(ranked_aswers[i])\n",
    "                # premise = \"symptoms of covid-19 range from none (asymptomatic) to severe pneumonia and it can be fatal. fever and cough are the most common symptoms in patients with 2019-ncov infection. Several people have muscle soreness or fatigue as well as ards. diarrhea, hemoptysis, headache, sore throat, shock, and other symptoms only occur in a small number of patients\"\n",
    "                if len(candidate_answer) > len(ranked_aswers[i]):\n",
    "                    premise = ranked_aswers[i]\n",
    "                else:\n",
    "                    premise = candidate_answer\n",
    "                # premise = \"covid-19 symptom is highly various in each patient, with fever, fatigue, shortness of breath, and cough as the main presenting symptoms. patient with covid-19 may shows severe symptom with severe pneumonia and ards, mild symptom resembling simple upper respiration tract infection, or even completely asymptomatic. approximately 80 % of cases is mild \"\n",
    "                hypothesis = string_all\n",
    "                tokenized_input_seq_pair = tokenizer_nli.encode_plus(\n",
    "                    premise,\n",
    "                    hypothesis,\n",
    "                    max_length=max_length,\n",
    "                    return_token_type_ids=True,\n",
    "                    truncation=True,\n",
    "                )\n",
    "\n",
    "                input_ids = (\n",
    "                    torch.Tensor(tokenized_input_seq_pair[\"input_ids\"])\n",
    "                    .long()\n",
    "                    .unsqueeze(0)\n",
    "                    .to(torch_device)\n",
    "                )\n",
    "                # remember bart doesn't have 'token_type_ids', remove the line below if you are using bart.\n",
    "                token_type_ids = (\n",
    "                    torch.Tensor(tokenized_input_seq_pair[\"token_type_ids\"])\n",
    "                    .long()\n",
    "                    .unsqueeze(0)\n",
    "                    .to(torch_device)\n",
    "                )\n",
    "                attention_mask = (\n",
    "                    torch.Tensor(tokenized_input_seq_pair[\"attention_mask\"])\n",
    "                    .long()\n",
    "                    .unsqueeze(0)\n",
    "                    .to(torch_device)\n",
    "                )\n",
    "\n",
    "                outputs = model_nli(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=None,\n",
    "                )\n",
    "                # Note:\n",
    "                # \"id2label\": {\n",
    "                #     \"0\": \"entailment\",\n",
    "                #     \"1\": \"neutral\",\n",
    "                #     \"2\": \"contradiction\"\n",
    "                # },\n",
    "\n",
    "                predicted_probability = torch.softmax(outputs[0], dim=1)[\n",
    "                    0\n",
    "                ].tolist()  # batch_size only one\n",
    "\n",
    "                # print(\"Premise:\", premise)\n",
    "                premise_list.append(premise)\n",
    "                # print(\"Hypothesis:\", hypothesis)\n",
    "                hypothesis_list.append(hypothesis)\n",
    "                # print(\"input Ans:\",ranked_aswers[i])\n",
    "                # print(\"Entailment:\", predicted_probability[0])\n",
    "                entailment_prob.append(predicted_probability[0])\n",
    "                # print(\"Neutral:\", predicted_probability[1])\n",
    "                neutral_prob.append(predicted_probability[1])\n",
    "                # print(\"Contradiction:\", predicted_probability[2])\n",
    "                contradict_prob.append(predicted_probability[2])\n",
    "        if len(premise_list) > 0:\n",
    "            avg_entailment = np.average(entailment_prob)\n",
    "            avg_neutral = np.average(neutral_prob)\n",
    "            avg_contradiction = np.average(contradict_prob)\n",
    "            zipped_list = zip(entailment_prob, neutral_prob, contradict_prob)\n",
    "            for key, val in enumerate(hypothesis_list):\n",
    "                if neutral_prob[key] > 0.8:\n",
    "                    final_result = \"Neutral\"\n",
    "                elif entailment_prob[key] > contradict_prob[key]:\n",
    "                    final_result = \"True\"\n",
    "                    final_result_list.append(final_result)\n",
    "                else:\n",
    "                    final_result = \"False\"\n",
    "                    final_result_list.append(final_result)\n",
    "            if len(final_result_list) > 0:\n",
    "                verdict = most_frequent(final_result_list)\n",
    "            else:\n",
    "                verdict = \"Neutral\"\n",
    "        else:\n",
    "            #       zipped_list = zip(0,1,0)\n",
    "            zipped_list = zip([0], [1], [0])\n",
    "            verdict = \"Neutral\"\n",
    "    return zipped_list, verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "def embed_useT(module):\n",
    "    with tf.Graph().as_default():\n",
    "        sentences = tf.compat.v1.placeholder(tf.string)\n",
    "        embed = hub.Module(module)\n",
    "        embeddings = embed(sentences)\n",
    "        session = tf.compat.v1.train.MonitoredSession()\n",
    "    return lambda x: session.run(embeddings, {sentences: x})\n",
    "embed_fn = embed_useT('/home/titanx/kaggle/working/sentence_wise_email/module/module_useT')\n",
    "\n",
    "def get_answers(query, hit_dictionary, display_table=False):\n",
    "    for idx,v in hit_dictionary.items():\n",
    "        abs_dirty = v['abstract']\n",
    "        real_abs_dirty = v['real_abstract']\n",
    "        #abs_dirty = v['paragraph']\n",
    "        # looks like the abstract value can be an empty list\n",
    "        v['abstract_paragraphs'] = []\n",
    "        v['abstract_full'] = ''\n",
    "        v['real_abstract_full'] = ''\n",
    "        v['real_abstract_paragraphs']=[]\n",
    "\n",
    "        if abs_dirty:\n",
    "            # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
    "            # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
    "            # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
    "\n",
    "            if isinstance(abs_dirty, list):\n",
    "                for p in abs_dirty:\n",
    "                    v['abstract_paragraphs'].append(p['text'])\n",
    "                    v['abstract_full'] += p['text'] + ' \\n\\n'\n",
    "\n",
    "            # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
    "            if isinstance(abs_dirty, str):\n",
    "                v['abstract_paragraphs'].append(abs_dirty)\n",
    "                v['abstract_full'] += abs_dirty + ' \\n\\n'\n",
    "        if real_abs_dirty:\n",
    "            # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
    "            # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
    "            # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
    "\n",
    "\n",
    "            if isinstance(real_abs_dirty, list):\n",
    "                for p in real_abs_dirty:\n",
    "                    v['real_abstract_paragraphs'].append(p['text'])\n",
    "                    v['real_abstract_full'] += p['text'] + ' \\n\\n'\n",
    "\n",
    "            # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
    "            if isinstance(abs_dirty, str):\n",
    "                v['real_abstract_paragraphs'].append(real_abs_dirty)\n",
    "                v['real_abstract_full'] += real_abs_dirty + ' \\n\\n'\n",
    "        v[\"indexed_para\"]=v[\"abstract_full\"][len(v[\"real_abstract_full\"]):]\n",
    "\n",
    "#     def embed_useT(module):\n",
    "#         with tf.Graph().as_default():\n",
    "#             sentences = tf.compat.v1.placeholder(tf.string)\n",
    "#             embed = hub.Module(module)\n",
    "#             embeddings = embed(sentences)\n",
    "#             session = tf.compat.v1.train.MonitoredSession()\n",
    "#         return lambda x: session.run(embeddings, {sentences: x})\n",
    "#     embed_fn = embed_useT('/home/titanx/kaggle/working/sentence_wise_email/module/module_useT')\n",
    "\n",
    "    import numpy as np\n",
    "    #Reconstruct text for generating the answer text from the BERT result, do the necessary text preprocessing and construct the answer.\n",
    "    def reconstructText(tokens, start=0, stop=-1):\n",
    "        tokens = tokens[start: stop]\n",
    "        if '[SEP]' in tokens:\n",
    "            sepind = tokens.index('[SEP]')\n",
    "            tokens = tokens[sepind+1:]\n",
    "        txt = ' '.join(tokens)\n",
    "        txt = txt.replace(' ##', '')\n",
    "        txt = txt.replace('##', '')\n",
    "        txt = txt.strip()\n",
    "        txt = \" \".join(txt.split())\n",
    "        txt = txt.replace(' .', '.')\n",
    "        txt = txt.replace('( ', '(')\n",
    "        txt = txt.replace(' )', ')')\n",
    "        txt = txt.replace(' - ', '-')\n",
    "        txt_list = txt.split(' , ')\n",
    "        txt = ''\n",
    "        nTxtL = len(txt_list)\n",
    "        if nTxtL == 1:\n",
    "            return txt_list[0]\n",
    "        newList =[]\n",
    "        for i,t in enumerate(txt_list):\n",
    "            if i < nTxtL -1:\n",
    "                if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n",
    "                    newList += [t,',']\n",
    "                else:\n",
    "                    newList += [t, ', ']\n",
    "            else:\n",
    "                newList += [t]\n",
    "        return ''.join(newList)\n",
    "\n",
    "\n",
    "    def makeBERTSQuADPrediction(document, question):\n",
    "        ## we need to rewrite this function so that it chuncks the document into 250-300 word segments with\n",
    "        ## 50 word overlaps on either end so that it can understand and check longer abstracts\n",
    "        ## Get chuck of documents in order to overcome the BERT's max 512 token problem. First count the number of tokens. For example if it is\n",
    "        ##   larger than 2048 token determine the split size and overlapping token count. Split the document into pieces to fed into BERT encoding. Overlapping\n",
    "        ##      is necessary to maintain the coherence between the splits. If the token count is larger than some number 2560 token approximately this will fail.\n",
    "        nWords = len(document.split())\n",
    "        input_ids_all = QA_TOKENIZER.encode(question, document)\n",
    "        tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n",
    "        overlapFac = 1.1\n",
    "        if len(input_ids_all)*overlapFac > 2560:\n",
    "            nSearchWords = int(np.ceil(nWords/6))\n",
    "            fifth = int(np.ceil(nWords/5))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[fifth-int(nSearchWords*overlapFac/2):fifth+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[fifth*2-int(nSearchWords*overlapFac/2):fifth*2+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[fifth*3-int(nSearchWords*overlapFac/2):fifth*3+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[fifth*4-int(nSearchWords*overlapFac/2):fifth*4+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "\n",
    "        elif len(input_ids_all)*overlapFac > 2048:\n",
    "            nSearchWords = int(np.ceil(nWords/5))\n",
    "            quarter = int(np.ceil(nWords/4))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
    "            \n",
    "        elif len(input_ids_all)*overlapFac > 1536:\n",
    "            nSearchWords = int(np.ceil(nWords/4))\n",
    "            third = int(np.ceil(nWords/3))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
    "            \n",
    "        elif len(input_ids_all)*overlapFac > 1024:\n",
    "            nSearchWords = int(np.ceil(nWords/3))\n",
    "            middle = int(np.ceil(nWords/2))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "        elif len(input_ids_all)*overlapFac > 512:\n",
    "            nSearchWords = int(np.ceil(nWords/2))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "        else:\n",
    "            input_ids = [input_ids_all]\n",
    "        absTooLong = False    \n",
    "        \n",
    "        answers = []\n",
    "        cons = []\n",
    "        #print(input_ids)\n",
    "        for iptIds in input_ids:\n",
    "            tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n",
    "            #print(tokens)\n",
    "            sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)\n",
    "            num_seg_a = sep_index + 1\n",
    "            num_seg_b = len(iptIds) - num_seg_a\n",
    "            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "            assert len(segment_ids) == len(iptIds)\n",
    "            n_ids = len(segment_ids)\n",
    "            #print(n_ids)\n",
    "            if n_ids < 512:\n",
    "                outputs = QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
    "                                        token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n",
    "                #print(QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
    "                #                        token_type_ids=torch.tensor([segment_ids]).to(torch_device)))\n",
    "            else:\n",
    "                #this cuts off the text if its more than 512 words so it fits in model space\n",
    "                #need run multiple inferences for longer text. add to the todo. I will try to expand this to longer sequences if it is necessary. \n",
    "#                 print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n",
    "                absTooLong = True\n",
    "                outputs= QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device), \n",
    "                                        token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))\n",
    "            start_scores=outputs.start_logits\n",
    "            end_scores=outputs.end_logits\n",
    "            start_scores = start_scores[:,1:-1]\n",
    "            end_scores = end_scores[:,1:-1]\n",
    "            answer_start = torch.argmax(start_scores)\n",
    "            answer_end = torch.argmax(end_scores)\n",
    "            #print(answer_start, answer_end)\n",
    "            answer = reconstructText(tokens, answer_start, answer_end+2)\n",
    "        \n",
    "            if answer.startswith('. ') or answer.startswith(', '):\n",
    "                answer = answer[2:]\n",
    "                \n",
    "            c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n",
    "            answers.append(answer)\n",
    "            cons.append(c)\n",
    "        \n",
    "        maxC = max(cons)\n",
    "        iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n",
    "        confidence = cons[iMaxC]\n",
    "        answer = answers[iMaxC]\n",
    "        \n",
    "        sep_index = tokens_all.index('[SEP]')\n",
    "        full_txt_tokens = tokens_all[sep_index+1:]\n",
    "        \n",
    "        abs_returned = reconstructText(full_txt_tokens)\n",
    "\n",
    "        ans={}\n",
    "        ans['answer'] = answer\n",
    "        #print(answer)\n",
    "        if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n",
    "            ans['confidence'] = -1000000\n",
    "        else:\n",
    "            #confidence = torch.max(start_scores) + torch.max(end_scores)\n",
    "            #confidence = np.log(confidence.item())\n",
    "            ans['confidence'] = confidence\n",
    "        #ans['start'] = answer_start.item()\n",
    "        #ans['end'] = answer_end.item()\n",
    "        ans['abstract_bert'] = abs_returned\n",
    "        ans['abs_too_long'] = absTooLong\n",
    "        return ans\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    def searchAbstracts(hit_dictionary, question):\n",
    "        abstractResults = {}\n",
    "#         for k,v in tqdm(hit_dictionary.items()):\n",
    "        for k,v in hit_dictionary.items():\n",
    "            abstract = v['abstract_full']\n",
    "            indexed_para=v['indexed_para']\n",
    "            if abstract:\n",
    "                ans = makeBERTSQuADPrediction(abstract, question)\n",
    "                if ans['answer']:\n",
    "                    confidence = ans['confidence']\n",
    "                    abstractResults[confidence]={}\n",
    "                    abstractResults[confidence]['answer'] = ans['answer']\n",
    "                    #abstractResults[confidence]['start'] = ans['start']\n",
    "                    #abstractResults[confidence]['end'] = ans['end']\n",
    "                    abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n",
    "                    abstractResults[confidence]['idx'] = k\n",
    "                    abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n",
    "\n",
    "                    \n",
    "        cList = list(abstractResults.keys())\n",
    "\n",
    "        if cList:\n",
    "            maxScore = max(cList)\n",
    "            total = 0.0\n",
    "            exp_scores = []\n",
    "            for c in cList:\n",
    "                s = np.exp(c-maxScore)\n",
    "                exp_scores.append(s)\n",
    "            total = sum(exp_scores)\n",
    "            for i,c in enumerate(cList):\n",
    "                abstractResults[exp_scores[i]/total] = abstractResults.pop(c)\n",
    "        return abstractResults\n",
    "\n",
    "    answers = searchAbstracts(hit_dictionary, query)\n",
    "\n",
    "    workingPath = '/root/kaggle/working'\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "    #from summarizer import Summarizer\n",
    "    #summarizerModel = Summarizer()\n",
    "    def displayResults(hit_dictionary, answers, question, display_table=False):\n",
    "        \n",
    "        question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n",
    "        #all_HTML_txt = question_HTML\n",
    "        confidence = list(answers.keys())\n",
    "        confidence.sort(reverse=True)\n",
    "        \n",
    "        confidence = list(answers.keys())\n",
    "        confidence.sort(reverse=True)\n",
    "        \n",
    "        page_answers = dict()\n",
    "\n",
    "        for c in confidence:\n",
    "            if c>0 and c <= 1 and len(answers[c]['answer']) != 0:\n",
    "                if 'idx' not in  answers[c]:\n",
    "                    continue\n",
    "                rowData = []\n",
    "                idx = answers[c]['idx']\n",
    "                title = hit_dictionary[idx]['title']\n",
    "\n",
    "                \n",
    "                full_abs = answers[c]['abstract_bert']\n",
    "                bert_ans = answers[c]['answer']\n",
    "                \n",
    "                \n",
    "                #split_abs = full_abs.split(bert_ans)\n",
    "                #x=(split_abs[0][:split_abs[0].rfind('. ')+1])\n",
    "                #sentance_beginning = x[x.rfind('. ')+1:] + split_abs[0][split_abs[0].rfind('. ')+1:]\n",
    "                #sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n",
    "                x=''\n",
    "                y=''\n",
    "                z=''\n",
    "                t=''\n",
    "\n",
    "                # print(bert_ans)\n",
    "                split_abs = full_abs.split(bert_ans)\n",
    "                sentance_beginning = split_abs[0].split(\" ~ ~ \")[-1]\n",
    "                if len(split_abs) > 1:\n",
    "                    sentance_end = split_abs[1].split(\" ~ ~ \")[0]\n",
    "                else:\n",
    "                    sentance_end = \"\"\n",
    "                    \n",
    "                sentance_full = sentance_beginning + bert_ans+ sentance_end\n",
    "#                 print(sentance_full)\n",
    "\n",
    "                answer_ids = re.findall(r\"((sentence|table|cell|section|list|item)( _ [0-9]+)+ )\", sentance_full)\n",
    "                answer_ids_formatted = []\n",
    "\n",
    "                for ids in answer_ids:\n",
    "                    if isinstance(ids, str):\n",
    "                        candidate = ids.replace(\" \", \"\")\n",
    "                        if \"_\" in candidate and candidate[0] != \"_\" and candidate not in [\"sentence\", \"table\", \"cell\", \"section\", \"list\", \"item\"]:  # very hacky\n",
    "                            answer_ids_formatted.append(candidate)\n",
    "                    else:\n",
    "                        for id in ids:\n",
    "                            candidate = id.replace(\" \", \"\")\n",
    "                            if \"_\" in candidate and candidate[0] != \"_\" and candidate not in [\"sentence\", \"table\", \"cell\", \"section\", \"list\", \"item\"]:  # very hacky\n",
    "                                answer_ids_formatted.append(candidate)\n",
    "#                 print(answer_ids_formatted)\n",
    "                \n",
    "                page_answers[title] = answer_ids_formatted\n",
    "\n",
    "                answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n",
    "                answers[c]['partial_answer'] = bert_ans+sentance_end\n",
    "                answers[c]['sentence_beginning'] = sentance_beginning\n",
    "                answers[c]['sentence_end'] = sentance_end\n",
    "                answers[c]['title'] = title\n",
    "            else:\n",
    "                answers.pop(c)\n",
    "        \n",
    "        \n",
    "        ## now rerank based on semantic similarity of the answers to the question\n",
    "        ## Universal sentence encoder\n",
    "        cList = list(answers.keys())\n",
    "        allAnswers = [answers[c]['full_answer'] for c in cList]\n",
    "        \n",
    "        messages = [question]+allAnswers\n",
    "        \n",
    "        encoding_matrix = embed_fn(messages)\n",
    "        similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n",
    "        rankings = similarity_matrix[1:,0]\n",
    "        \n",
    "        for i,c in enumerate(cList):\n",
    "            answers[rankings[i]] = answers.pop(c)\n",
    "\n",
    "        ## now form pandas dv\n",
    "        confidence = list(answers.keys())\n",
    "        confidence.sort(reverse=True)\n",
    "        pandasData = []\n",
    "        ranked_aswers = []\n",
    "        full_abs_list=[]\n",
    "        for c in confidence:\n",
    "            rowData=[]\n",
    "            title = answers[c]['title']\n",
    "            idx = answers[c]['idx']\n",
    "            rowData += [idx]            \n",
    "            sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='red'>\"+answers[c]['answer']+\"</font> \"+answers[c]['sentence_end']+'</div>'\n",
    "            answer_key = answers[c]['answer'].split(\"\\t\")[-1].strip() if \"\\t\" in answers[c]['answer'] else answers[c]['answer'].strip()\n",
    "\n",
    "            # rowData += [sentance_html, answer_key, c,title]\n",
    "            rowData += [sentance_html, c,title]\n",
    "            pandasData.append(rowData)\n",
    "            ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n",
    "            full_abs_list.append(' '.join([answers[c]['abstract_bert']]))\n",
    "        else:\n",
    "            pdata2 = pandasData\n",
    "            \n",
    "        if display_table:\n",
    "            display(HTML(question_HTML))\n",
    "\n",
    "        #    display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>Body of Evidence:</b></div>'))\n",
    "\n",
    "            # df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Answer Key', 'Confidence', 'Title/Link'])\n",
    "            df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])\n",
    "\n",
    "            display(HTML(df.to_html(render_links=True, escape=False)))\n",
    "        return page_answers,full_abs_list,ranked_aswers,pandasData\n",
    "\n",
    "    return displayResults(hit_dictionary, answers, query, display_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algebraic logic has five Logical system and Lindenbaum–Tarski algebra which includes Physics algebra and Nodal algebra (provide models of propositional modal logics).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:11<09:13,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 7389, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['List of Boolean algebra topics', 'sentence', '0'], ['Commutative diagram', 'sentence', '12'], ['Commutative diagram', 'sentence', '13'], ['Commutative diagram', 'sentence', '14'], ['Commutative diagram', 'sentence', '15']]}\n",
      "Aramais Yepiskoposyan played for FC Ararat Yerevan, an Armenian football club based in Yerevan during 1986 to 1991.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:26<13:58,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 13969, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Ivan Paunović', 'sentence', '3'], ['Mario Frick (footballer)', 'sentence', '0'], ['Frank Egharevba', 'sentence', '5'], ['Frank Egharevba', 'sentence', '6'], ['Frank Egharevba', 'sentence', '7']]}\n",
      "Scot's book is titled The Discoverie of Witchcraft, and was never published at the end the printer gives his name, there are also four dedications one of which is to Sir Roger Manwood.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:41<16:46, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9770, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Aleister Crowley', 'sentence', '358'], ['Burlescombe', 'sentence', '32'], ['Burlescombe', 'sentence', '33'], ['Burlescombe', 'sentence', '34'], ['Burlescombe', 'sentence', '35']]}\n",
      "Family Guy is an American animated sitcom that features five main voice actors, and numerous regular cast and also includes recurring casts such as H. Jon Benjamin who voices Carl and has appeared in 22 (out of 349) episodes, Johnny Brennan as Mort Goldman, and Horace the bartender that has appeared in 90 episodes, and Ralph Garman who has been working with the team since 2001 and has appeared in 213 episodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [01:02<21:38, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 3044, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Family Guy (season 16)', 'sentence', '8'], ['Julie Hagerty', 'sentence', '34'], ['The CBS Late Movie', 'sentence', '74'], ['Rawhide (TV series)', 'sentence', '2'], ['Jerry Lewis', 'sentence', '3']]}\n",
      "In John Laurie's partial television credits, he was part of the Bees on the Boat-Deck in 1939 and Jackanory in 1971.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [01:21<23:53, 15.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 4053, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Junior Coghlan', 'sentence', '46'], ['Stanley Rubin', 'sentence', '4'], ['Stanley Rubin', 'sentence', '5'], ['Joseph Cotten', 'sentence', '122'], ['Joseph Cotten', 'sentence', '123']]}\n",
      "List of Once Upon a Time (debuted October 23, 2011) episodes consists of 10 specials, one of the narrators Alan Dale.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [01:41<25:54, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10121, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Ronnie Scribner', 'sentence', '56'], ['Ronnie Scribner', 'sentence', '57'], ['Fringe (TV series)', 'sentence', '3'], ['Beavis and Butt-Head', 'sentence', '7'], ['James Spader', 'sentence', '38']]}\n",
      "Per Axel Rydberg, born on July 6, 1860 in Odh, Västergötland, situated outside Sweden, was a graduate of University of Nebraska–Lincoln in the field of Botany (the science of plant life and a branch of biology).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [01:54<23:56, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1350, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Derrick Norman Lehmer', 'sentence', '4'], ['Greta Arwidsson', 'sentence', '14'], ['Frank Bruce', 'sentence', '1'], ['Paul L. Douglas', 'sentence', '0'], ['Tsutomu Tomioka', 'sentence', '0']]}\n",
      "A village in China, in 2011 Kolhan had 693 people with around half (390) being workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [02:14<25:17, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 2534, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Kofud Mozhdeh', 'sentence', '1'], ['Rofuh Chah', 'sentence', '1'], ['Springwell Village', 'sentence', '1'], ['Nakashipara (community development block)', 'sentence', '20'], ['Chapra, Nadia (community development block)', 'sentence', '79']]}\n",
      "Xiao Sha competed in the World Championships and the World Cup.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [02:29<24:17, 16.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 44769, 'predicted_label': 'SUPPORTS', 'predicted_evidence': [['Irine Kharisma Sukandar', 'sentence', '10'], ['Pedro Cordero', 'sentence', '8'], ['FIBT World Championships 2011', 'sentence', '15'], ['Serhiy Semenov', 'sentence', '25'], ['Ed Adams', 'sentence', '5']]}\n",
      "The Glenn–Thompson Plantation, a historic plantation house near Pittsview in Russell County, Alabama, has undergone extensive renovations including the addition of four rooms and a patio in 1840, the original wood fireplace mantels have been sealed with marble hearths.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [02:45<24:06, 16.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 3385, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Abbey Mausoleum (Arlington County, Virginia)', 'sentence', '94'], ['Alexandria, Louisiana', 'sentence', '264']]}\n",
      "Ken Banks developed the FrontlineSMS (language: Java) in 2005 to help conservationists keep in touch with communities in Kruger National Park in South Africa.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [03:04<25:07, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10806, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Ishaqbini Hirola Conservancy', 'sentence', '10'], ['Ishaqbini Hirola Conservancy', 'sentence', '11'], ['Kuala Lumpur', 'sentence', '417'], ['Economy of Lesotho', 'sentence', '63'], ['Bank Woori Saudara', 'sentence', '1']]}\n",
      "Walter Tyrrell was credited with three aerial victories in Lamotte, a commune in the Somme department of northern France on the 7th day of April 1918 and two more just days after in April 11 and 12.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [03:26<26:40, 18.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1645, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Joachim Brendel', 'sentence', '33'], ['Fred Everest Banbury', 'sentence', '0'], ['Vittorio Tur', 'sentence', '24'], ['Phillip Scott Burge', 'sentence', '13'], ['Zeppelin LZ85', 'sentence', '2']]}\n",
      "Harman Tarrant was a member of the first incarnation for a longer time than Samuel Charles and John Stewart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [03:43<26:00, 18.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1474, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Samuel Homfray', 'sentence', '0'], ['Daniel Gurney', 'sentence', '16'], ['David Thompson (New Hampshire settler)', 'sentence', '25'], ['James E. Stewart', 'sentence', '6'], ['James E. Stewart', 'sentence', '7']]}\n",
      "2014 Sky Blue FC season number 18 Lindsi Cutshall (born October 18, 1990) played the FW position.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [04:01<25:42, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 7339, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Christine Sinclair', 'sentence', '44'], ['Kévin Malcuit', 'sentence', '11'], ['Thomas Verheydt', 'sentence', '18'], ['Ian Harkes', 'sentence', '7'], ['Ryan Thomas (footballer)', 'sentence', '20']]}\n",
      "1976 United States Senate election in North Dakota (held November 2, 1976) had two candidates and the winner was Quentin Burdick.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [04:18<24:41, 17.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 34350, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['1976 United States presidential election in Missouri', 'sentence', '1'], ['1976 United States presidential election in Kentucky', 'sentence', '1'], ['1976 United States presidential election in Rhode Island', 'sentence', '1'], ['1976 United States presidential election in Nevada', 'sentence', '1'], ['2004 United States presidential election in South Carolina', 'sentence', '22']]}\n",
      "Harrier Shing of the Liberal party has always been a member of the Eastern Victoria Region from 2006 until 2018.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [04:39<25:48, 18.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 7966, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Joe Dickson', 'sentence', '1'], ['Joe Dickson', 'sentence', '2'], ['2007 Williamstown state by-election', 'sentence', '5'], ['Archdale Parkhill', 'sentence', '0'], ['Liberal Party of Australia', 'sentence', '110']]}\n",
      "Ohio State scored fewer points than Purdue at the 1947 NCAA Swimming and Diving Championships.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [04:52<23:06, 16.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 21446, 'predicted_label': 'REFUTES', 'predicted_evidence': [['1922 Big Ten Conference football season', 'sentence', '4'], ['2007 Illinois Fighting Illini football team', 'sentence', '54'], ['Skyline High School (Washington)', 'sentence', '29'], [\"2000–01 Illinois Fighting Illini men's basketball team\", 'sentence', '11'], [\"2000–01 Illinois Fighting Illini men's basketball team\", 'sentence', '12']]}\n",
      "Greenbrier West High School is a public school under the purview of Amy Robertson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [05:10<23:21, 17.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 986, 'predicted_label': 'REFUTES', 'predicted_evidence': [['West Ashley High School', 'sentence', '0'], ['Aberdeen, Washington', 'sentence', '80'], ['Wayzata High School', 'sentence', '0'], ['Centreville, Virginia', 'sentence', '56'], ['Benwood, West Virginia', 'sentence', '117']]}\n",
      "Wellington Nascimento Carvalho is a Brazilian football player who played for Portimonense in the Primeira Liga in 2017-18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [05:22<20:59, 15.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 3716, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Émerson Carvalho da Silva', 'sentence', '0'], ['Nilton Fernandes', 'sentence', '0'], ['Juninho (footballer, born May 1989)', 'sentence', '0'], ['Bruno César', 'sentence', '0'], ['Luis Alberto Márquez', 'sentence', '0']]}\n",
      "Al Stokes had more home runs than than runs batted in.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [05:40<21:30, 16.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9727, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Kansas City Royals', 'sentence', '142'], ['Les Moss', 'sentence', '9'], ['1961 Detroit Tigers season', 'sentence', '64'], ['1990 Chicago Cubs season', 'sentence', '7'], ['1909 Detroit Tigers season', 'sentence', '102']]}\n",
      "Penguin Books-published book series Opposing Viewpoints with print and online versions tries to encourage critical thinking and issue awareness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [05:57<21:49, 16.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 48965, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Alexander of Aphrodisias', 'sentence', '31'], ['Alexander of Aphrodisias', 'sentence', '32'], ['Découvertes Gallimard', 'sentence', '191'], ['Caroline Webb', 'sentence', '0'], ['Caroline Webb', 'sentence', '1']]}\n",
      "French Patrick Tambay failed to finish first in the 1982 German Grand Prix, making fellow citizen Keke Rosberg the race winner.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [06:15<21:40, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 12250, 'predicted_label': 'SUPPORTS', 'predicted_evidence': [['Henrik Gustafsson', 'sentence', '20'], ['2008 Malaysian Grand Prix', 'sentence', '23'], ['1979 FIM Motocross World Championship', 'sentence', '5'], ['1979 FIM Motocross World Championship', 'sentence', '6'], ['The Wonder (horse)', 'sentence', '32']]}\n",
      "GT cars are divided into 2 or 3 subgroups (GT3, GT4, and GT Open), on the other hand, TC, which are heavily modified road-going cars used in motorsport road racing competitions, are divided by calculated engine capacity (TC1 - TC4).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [06:24<18:25, 14.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9583, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['1995 IMSA GT Championship', 'sentence', '1']]}\n",
      "The two episodes of Das unsichtbare Visier, which are 89 minutes long, are Das Nest im Urwald and Depot im Skagerrak.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [06:34<16:30, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 3050, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Rúben Oliveira (footballer, born 1994)', 'sentence', '10']]}\n",
      "The Sap production discography produced a total of eight songs in seven different albums from 2011 to 2015.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [06:53<18:21, 14.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 15953, 'predicted_label': 'REFUTES', 'predicted_evidence': [['4 (Beyoncé album)', 'sentence', '6'], ['Apocalyptica', 'sentence', '67'], ['Ryan Leslie discography', 'sentence', '0'], ['David Pevsner', 'sentence', '34'], ['David Pevsner', 'sentence', '35']]}\n",
      "Although there is rough grazing on the slopes of the hill, no part of it falls within the definitions of open country as set out under the Countryside and Rights of Way Act 2000 (known informally as the CRoW Act or \"Right to Roam\" Act), nevertheless, access is enjoyed along the north-south crest of the ridge by means of a minor public road.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [07:12<19:55, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 5658, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Monk Soham', 'sentence', '69'], ['Navenby', 'sentence', '222'], ['Abergavenny', 'sentence', '127']]}\n",
      "During the 1949 season, under coach John Gill, the Western Michigan Broncos defeated Ohio, but lost to Central Michigan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [07:27<18:59, 15.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 12890, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['1956 Big Ten Conference football season', 'sentence', '4'], ['1942 Big Ten Conference football season', 'sentence', '6'], ['1896 Western Conference football season', 'sentence', '6'], ['Knute Rockne', 'sentence', '88'], ['1939 Big Ten Conference football season', 'sentence', '0']]}\n",
      "Phukkhom of BNK48 participated in the 2019 music video \"Siang Khorong Khong Khon Siachai\" by Tor Saksit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [07:40<17:41, 14.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 3663, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Mariah Paris Balenciaga', 'sentence', '28'], ['Songs from Northern Britain', 'sentence', '52'], ['PICK-UP (band)', 'sentence', '56'], ['Mystery of Venus', 'sentence', '3'], ['Mystery of Venus', 'sentence', '4']]}\n",
      "The Golden Calf Occupation Award is one of 16 awards given to film occupations such as cameraman and music composer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [07:57<18:08, 15.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9865, 'predicted_label': 'SUPPORTS', 'predicted_evidence': [['List of awards and nominations received by Andrew Garfield', 'sentence', '6'], ['List of accolades received by Hidden Figures', 'sentence', '11'], ['Jonathan S. Raymond', 'sentence', '19'], ['Mannix', 'sentence', '102'], ['Listapad', 'sentence', '55']]}\n",
      "The Puerto Rican radio station WMEG first aired in 1966 and they used to play only music in Spanish but now most of the the music played is American Top 40 music.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [08:14<18:27, 16.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 32009, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['WULT', 'sentence', '13'], ['WXKL', 'sentence', '8'], ['WIPR-TV', 'sentence', '31'], ['Vicente Carattini', 'sentence', '34'], ['WOYL', 'sentence', '30']]}\n",
      "The Side A comprises of \"Anniversary Waltz\" by Dave Franklin, Al Dubin, the longest of all tracks which plays for two minutes and 27 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [08:34<19:26, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 11314, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Jubilation (song)', 'sentence', '27'], [\"The Ladies' Bras\", 'sentence', '6'], ['Old School Songs', 'sentence', '3'], ['Another Day (U2 song)', 'sentence', '5'], ['÷ (album)', 'sentence', '27']]}\n",
      "Shalom Levin(27 March 1916-14 April 1995) was an Israeli teacher and politician from Rakaw, Germany, who served as a member of the Knesset for the Alignment between 1969 and 1977 and as general secretary of the Israel Teachers Union between 1955 and 1980,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [08:52<19:31, 17.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 7265, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Per Gahrton', 'sentence', '0'], ['Sarah Bavly', 'sentence', '33'], ['Sarah Bavly', 'sentence', '34'], ['Sarah Bavly', 'sentence', '35'], ['Sarah Bavly', 'sentence', '36']]}\n",
      "In the 2009 Swiss Olympic curling trials, only the two teams Basel and St. Moritz entered the competition.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [09:04<17:26, 15.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 25378, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['John Morris (curler)', 'sentence', '65'], ['1985–86 FC Basel season', 'sentence', '34'], ['Bids for the 1928 Winter Olympics', 'sentence', '0'], ['1987–88 FC Basel season', 'sentence', '41'], ['1987–88 FC Basel season', 'sentence', '42']]}\n",
      "Bill Persky (a TV director) was born to a Jewish family in 1931 and got married three times, where Joanna Patton was his third wife.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [09:23<18:10, 16.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 268, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Boulting brothers', 'sentence', '2'], ['Edward Hearn (actor)', 'sentence', '33'], ['Aage Bohr', 'sentence', '3'], ['Tadeusz Olsza', 'sentence', '0'], ['Paul Wellstone', 'sentence', '27']]}\n",
      "The 565th Strategic Missile Squadron participated in nine campaigns from June 16, 1943, until May 1945.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [09:44<19:11, 17.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 11720, 'predicted_label': 'SUPPORTS', 'predicted_evidence': [['Naval regions and districts of the Kriegsmarine', 'sentence', '128'], ['USS Maury (DD-401)', 'sentence', '34'], ['USS Sampson (DD-394)', 'sentence', '59'], ['John Balmer', 'sentence', '89'], ['Dunkeswell Aerodrome', 'sentence', '9']]}\n",
      "The 2016 Serbia OQT basketball team included players such as Ognjen Jaramaz, Marko Kešelj, and Ognjen Kuzmić, a Serbian professional basketball player for Crvena zvezda, but were all removed on May 20, 2016.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [09:58<17:50, 16.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 17798, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Dragan Nikolić (coach)', 'sentence', '1'], ['Marko Popović (basketball, born 1985)', 'sentence', '0'], ['Memiš Limani', 'sentence', '2'], ['T. J. DiLeo', 'sentence', '5'], ['T. J. DiLeo', 'sentence', '6']]}\n",
      "The plot of Six Charlies in Search of an Author is that of a book being written by Adolphus \"Jim\" Spriggs where Grytpype-Thynne blackmails Neddie with some X-ray photographs which results in Neddie pawning himself at a pawnshop to pay for the photograph, but ultimately ends with a happy ending as Neddie marries his fiancée Gladys Minkwater.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [10:20<18:54, 18.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1183, 'predicted_label': 'REFUTES', 'predicted_evidence': [[\"Red Orc's Rage\", 'sentence', '1'], ['Final Exam (1981 film)', 'sentence', '61'], ['Jungle Woman', 'sentence', '76'], ['Jungle Woman', 'sentence', '77'], ['Hard Sun', 'sentence', '7']]}\n",
      "Sclerodermataceae's fruit-bodies are mostly epigenous and the peridium is mostly simple rarely 2-layered, firm, rarely thin, membranous, breaking open irregularly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [10:33<17:07, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 40535, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Eremaea (plant)', 'sentence', '32'], ['1106 Cydonia', 'sentence', '14']]}\n",
      "Abraham Annan Adjei has never played professional football.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [10:47<15:53, 15.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 21641, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Tom Lang', 'sentence', '0'], ['Tom Lang', 'sentence', '1'], ['Ludovic Quistin', 'sentence', '16'], ['Jesse Holley', 'sentence', '46'], ['Abraham Joshua Heschel', 'sentence', '3']]}\n",
      "Rex Jory remained a copy boy until his retirement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [10:57<13:52, 14.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 6293, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Fernando Da Silva', 'sentence', '5'], ['Rex Heslop', 'sentence', '0'], ['Bullamakanka', 'sentence', '2'], ['Zoran Modli', 'sentence', '6'], ['Zoran Modli', 'sentence', '7']]}\n",
      "Italian film actor Peppe Lanzetta has appeared in films since his first, Blues metropolitano, in 1985 to his most recent, Due soldati, in 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [11:14<14:34, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9220, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Nico Mirallegro', 'sentence', '19'], ['Toto in Color', 'sentence', '0'], ['Toto in Color', 'sentence', '1'], ['Arnold Schwarzenegger', 'sentence', '4'], ['Coen brothers', 'sentence', '25']]}\n",
      "Two ships were named after U.S. Navy enlistee, Charles Ausburne, after he stood to duty when the Antilles,a ship that was chartered by the Army for troop transport, was torpedoed amd sunk by a German U-Boat, under him.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [11:35<15:48, 16.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 2021, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Daniel Turner (naval officer)', 'sentence', '29'], ['USS Coontz', 'sentence', '0'], ['USS Coontz', 'sentence', '1'], ['Pennsylvania-class cruiser', 'sentence', '53'], ['German submarine U-301', 'sentence', '0']]}\n",
      "UA Valettoise was founded in 1948, is in the CFA 2 Group E league, and is based in the Provence-Alpes-Côte d'Azur region in southeastern France.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [11:50<15:03, 16.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 2117, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Rugby league in France', 'sentence', '50'], ['Provence football team', 'sentence', '0'], ['Arrondissement of Castellane', 'sentence', '0'], ['Arrondissement of Forcalquier', 'sentence', '0'], ['Arrondissement of Aix-en-Provence', 'sentence', '0']]}\n",
      "Russia Unics Kazan won the 2019-2020 Eurocup Basketball recording a single loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [12:06<14:52, 16.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1036, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['BC Neptūnas', 'sentence', '84'], ['BC Samara', 'sentence', '10'], ['BC Samara', 'sentence', '11'], ['BC Samara', 'sentence', '12'], ['BC Samara', 'sentence', '13']]}\n",
      "Juventus, colloquially known as Juve (pronounced [ˈjuːve), is a professional football club based in Turin, Piedmont, Italy, that competes in the Serie A, the top flight of Italian football, won the most numbers of gold in the Coppa Italia finals.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [12:22<14:25, 16.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 7831, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Juventus Atlético Clube', 'sentence', '1'], ['Giovanni Vecchina', 'sentence', '1'], ['Emiliano Testini', 'sentence', '0'], ['Benedikt Höwedes', 'sentence', '49'], ['Jesse Fioranelli', 'sentence', '5']]}\n",
      "National Security Guard, formed on September 22, 1986 in Kolkata, has 14,000 personnel and an annual budget of ₹11.9 billion (US$166.8 million) in 2020-21.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [12:43<15:28, 17.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 21827, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Altaba', 'sentence', '57'], ['Electricity sector in Chile', 'sentence', '140'], ['Azerbaijani Armed Forces', 'sentence', '4'], ['Economy of Armenia', 'sentence', '389'], ['Economy of Liechtenstein', 'sentence', '31']]}\n",
      "The kingdom of Plantae are multicellular eukaryotic organisms that form the biological kingdom for which Apistogramma regani is a member of one of its over 1.5 million species.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [13:01<15:25, 17.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 3675, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Animal (disambiguation)', 'sentence', '0'], ['Kunzea micrantha', 'sentence', '19'], ['Kunzea pauciflora', 'sentence', '12'], ['Life', 'sentence', '169'], ['Eremophila brevifolia', 'sentence', '19']]}\n",
      "The 2004 North Carolina lieutenant governor election saw Republican opposition Jim Snyder, who only had 1,453,705, which was only 42.78% of the total lose to incumbent Democratic candidate Bev Perdue who had 1,888,397 votes, which was 55.57% of the total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [13:20<15:20, 18.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 33404, 'predicted_label': 'SUPPORTS', 'predicted_evidence': [['2008 United States House of Representatives elections in California', 'sentence', '148'], ['1992 Indiana gubernatorial election', 'sentence', '2'], ['1992 Indiana gubernatorial election', 'sentence', '3'], ['Carl E. Moses', 'sentence', '15'], ['Joe Garcia', 'sentence', '87']]}\n",
      "Fereydoon Fazli made a record of 14 goal assists from 2005 to 2010 with two clubs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [13:36<14:40, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10216, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Ray Macias', 'sentence', '14'], ['Christian Barreiro', 'sentence', '3'], ['Lahti Pelicans', 'sentence', '105'], ['Hans Henrik Andreasen', 'sentence', '6'], ['Ariel Pereyra', 'sentence', '4']]}\n",
      "There are no school board members and over 5000 students in 2016, Stroudsburg Area School District is just one of the 500 public school districts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [13:56<15:02, 18.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 19966, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Wyckoff, New Jersey', 'sentence', '108'], ['Wyckoff, New Jersey', 'sentence', '109'], ['Lake Stevens, Washington', 'sentence', '155'], ['Sultan, Washington', 'sentence', '122'], ['Glen Rock, New Jersey', 'sentence', '103']]}\n",
      "Jason Naismith played for ten clubs but never signed with Peterborough United, also known as \"The Posh\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [14:12<14:02, 17.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 43418, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Roy Killin', 'sentence', '10'], ['Fred Hill (footballer, born 1940)', 'sentence', '3'], ['Wayne Bullimore', 'sentence', '3'], [\"Charles N'Zogbia\", 'sentence', '38'], ['Billy Manuel', 'sentence', '10']]}\n",
      "Hanaukyō Meido Tai was written by Morishige, published by\tAkita Shoten and had 10 episodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [14:22<11:56, 15.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 5710, 'predicted_label': 'REFUTES', 'predicted_evidence': [['List of Full Metal Panic! chapters', 'sentence', '13'], ['Tai Lue people', 'sentence', '13'], ['Tokyo ESP', 'sentence', '0'], ['Fairy Navigator Runa', 'sentence', '0'], ['Marriage Royale', 'sentence', '0']]}\n",
      "The Petronas Philharmonic Hall, which was constructed with exposed movable ceiling panels, has seven movable panels in the upper ceiling which can be adjusted to alter the volume in the hall and simulate a wide range of acoustic environments.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [14:37<11:40, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10588, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Crystal Ballroom (Portland, Oregon)', 'sentence', '33'], ['Hong Kong Sinfonietta', 'sentence', '21']]}\n",
      "Cape Fear and Yadkin Valley Railway operated in North and South Carolina right after Reconstruction, created in 1879 from a consolidation with two other railroads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [14:57<12:23, 16.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 493, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Brewster Subdivision', 'sentence', '21'], ['Boyce, Virginia', 'sentence', '30'], ['Norlina Subdivision', 'sentence', '8'], ['Paul Kruger', 'sentence', '420'], ['Macon and Brunswick Railroad', 'sentence', '0']]}\n",
      "Bergamo Lions won the Eurobowl (European American football contest) in 2010, 2011 and 2012, while losing to the Chrysler Vikings in the finals of 2014 and 2015 but In Germany they have been unbeaten since 1998.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [15:16<12:47, 17.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 2054, 'predicted_label': 'REFUTES', 'predicted_evidence': [['A.S. Roma', 'sentence', '36'], ['Donar (basketball club)', 'sentence', '5'], ['Donar (basketball club)', 'sentence', '6'], ['Arsenal F.C.', 'sentence', '249'], ['Nashville Venom', 'sentence', '13']]}\n",
      "The Reed College Radio Club from Reed College, which is a private liberal arts college, built a station that used a system that transmitted the signal through area power lines while carrying programming atypical of radio in the area from its earliest days.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [15:35<12:43, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 45786, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['New York Institute of Technology', 'sentence', '296'], ['KHSU', 'sentence', '0'], ['WOYL', 'sentence', '30'], ['Neillsville, Wisconsin', 'sentence', '51']]}\n",
      "Jacques Freitag represented South Africa in World Youth Championships and All-Africa Games in 1999.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [15:52<12:15, 17.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 36334, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Luchia Yishak', 'sentence', '11'], ['2007 Rugby World Cup Final', 'sentence', '2'], ['Gete Wami', 'sentence', '4'], ['South Africa at the 2012 Summer Olympics', 'sentence', '25'], ['South Africa at the 2012 Summer Olympics', 'sentence', '26']]}\n",
      "Pirate Party member Dirk Poot once ran for the office of Prime Minister of the Netherlands.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [16:09<12:00, 17.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 29432, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Ylva Johansson', 'sentence', '5'], ['Australian Labor Party', 'sentence', '107'], ['Australian Labor Party', 'sentence', '108'], ['1963 Canadian federal election', 'sentence', '3'], ['Edo Ronchi', 'sentence', '11']]}\n",
      "George Brown began his Liberal Party leadership in Canada on the first of July in 1867.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [16:28<12:00, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 4329, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Archdale Parkhill', 'sentence', '9'], ['John Sparrow David Thompson', 'sentence', '48'], ['Edward Millen', 'sentence', '4'], ['John Diefenbaker', 'sentence', '61'], ['Tom Osborne (Canadian politician)', 'sentence', '7']]}\n",
      "In the years 1999 to 2015, Kazunori Iio first played club team Verdy Kawasakim went on to play on Tokyo Verfy (which he played on three additional times over these years, before ending on Yokohama FC.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [16:43<11:07, 17.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 17044, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Stefan Lainer', 'sentence', '7'], ['Juninho (footballer, born May 1989)', 'sentence', '16']]}\n",
      "In personal finance, a grace period, which can range from minutes to a number of days, refers to the allocated time which no interest is charged on a credit card or when fees are due but can be paid without penalty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [17:02<11:12, 17.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 12792, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['XVA', 'sentence', '13'], ['Telewest', 'sentence', '55'], ['Bella Italia', 'sentence', '28'], ['Apollo 8', 'sentence', '337'], ['Leap second', 'sentence', '154']]}\n",
      "The Oregon Garden opened in 2011 but never netted a profit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [17:16<10:13, 16.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10004, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Wentworth Castle', 'sentence', '8'], ['Cayuga Nature Center', 'sentence', '11'], ['Cayuga Nature Center', 'sentence', '12'], ['Terra Nova Adventure Park', 'sentence', '31'], ['Bogor Botanical Gardens', 'sentence', '63']]}\n",
      "The 70th Academy Awards saw Titanic which had an initial worldwide gross of over $1.84 billion win Best Director for James Cameron and Best Picture along with 9 other awards.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [17:36<10:27, 17.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 6272, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['List of accolades received by Hidden Figures', 'sentence', '11'], ['Julia Roberts', 'sentence', '131'], ['Notes on Blindness', 'sentence', '4'], ['The Box (2009 film)', 'sentence', '51'], ['Albert Brooks', 'sentence', '62']]}\n",
      "Lars Hjorth briefly played for Bærum SK, in 1996and in 1999 he was on the roster of sixth-tier club Lommedalens IL, and upon returning to Lyn, the team languished in the third tier and Hjorth only played twelve games.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [17:51<09:43, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 15834, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Eliteserien play-offs', 'sentence', '20'], ['FK Donn', 'sentence', '4'], ['Martin Knudsen (footballer)', 'sentence', '4'], ['Martin Knudsen (footballer)', 'sentence', '5'], ['Martin Knudsen (footballer)', 'sentence', '6']]}\n",
      "To get to Lake Tikub, one has to take the Pan-Philippine Highway South heading to Tiaong, Quezon, in the Philippines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [18:07<09:19, 16.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 24089, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Mount Sembrano', 'sentence', '9'], ['Mount Sembrano', 'sentence', '10'], ['Mount Sembrano', 'sentence', '11'], ['Mount Sembrano', 'sentence', '12'], ['Apo Reef', 'sentence', '20']]}\n",
      "In the 1989 NFL Season (the same season that NFL commissioner Pete Rozelle announced his retirement), the New York Giants drafted Brian Williams in the first round, while drafting both Bob Kratch and Greg Jackson in the third round, and David Meggett in the fifth round.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [18:25<09:21, 17.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 4547, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Michael Crabtree', 'sentence', '97'], ['Atlanta Falcons', 'sentence', '2'], ['1983 NFL Draft', 'sentence', '11'], ['1983 NFL Draft', 'sentence', '12'], ['2020 NFL season', 'sentence', '16']]}\n",
      "Pro Wrestling Guerrilla has a large roster, including Aramis, Dragon Lee, and Jeff Cobb, but none of its wrestlers have ever won the PWG World Championship.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [18:41<08:58, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 14667, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['The U.S. Express', 'sentence', '0'], ['Sports in Norfolk, Virginia', 'sentence', '22'], ['Davey Richards', 'sentence', '155'], ['The Bad Crew', 'sentence', '22'], ['Shuji Kondo', 'sentence', '55']]}\n",
      "1991 Waterford City Council election was planed to take place on 27 June 1991 with six parties all of which have at least two seats.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [18:59<08:52, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 21871, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Bergbahn', 'sentence', '12'], ['Gauteng Provincial Legislature', 'sentence', '9'], ['Politics of Lithuania', 'sentence', '67'], ['George Proctor Kane', 'sentence', '120'], ['Port Republic, New Jersey', 'sentence', '101']]}\n",
      "The Semi-finals of the 1903 Norwegian Football Cup was played between 6 teams.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [19:15<08:20, 16.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10791, 'predicted_label': 'REFUTES', 'predicted_evidence': [['1935 Palestine Cup', 'sentence', '3'], ['2010 Norwegian Football Cup Final', 'sentence', '0'], ['2010 Norwegian Football Cup Final', 'sentence', '1'], ['AFL pre-season competition', 'sentence', '16'], ['SC Germania Hamburg', 'sentence', '24']]}\n",
      "Takashi Taniguchi is a Japanese voice actor from Hokkaidō, the second largest island of Japan formerly known as Ezo, who lived to be 65 years old.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [19:30<07:52, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 7181, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Yuichi Tsuchiya', 'sentence', '0'], ['Takashi Fukaya', 'sentence', '0'], ['Takashi Nagayasu', 'sentence', '0'], ['Takashi Tanihata', 'sentence', '0'], ['Hiroko Nakano', 'sentence', '0']]}\n",
      "Car production in Italy by Lancia saw a slowdown in the past decade, from 300,087 cars in 1990 to only 58,759 in 2019.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [19:49<07:55, 16.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 3828, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['AvtoVAZ', 'sentence', '143'], ['Koenigsegg', 'sentence', '98'], ['Lamborghini', 'sentence', '115'], ['Economy of Kuwait', 'sentence', '104'], ['Brazil', 'sentence', '624']]}\n",
      "Many of the works of Dallas John Baker are gay-themed, though not all of them, and some of his short stories are set in Australia.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [20:05<07:34, 16.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 8805, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Duplex (building)', 'sentence', '33'], ['W-class Melbourne tram', 'sentence', '106'], ['World Youth Day 2008', 'sentence', '1'], ['Julian Lennon', 'sentence', '47']]}\n",
      "While Azerbaijan's economic structure was moving slowly in 1920-1939, the country produced more goods in 1948 and increased production by 39% as 1950 stepped in.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [20:25<07:37, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1897, 'predicted_label': 'REFUTES', 'predicted_evidence': [['History of Latvia', 'sentence', '542'], ['Economic history of Pakistan', 'sentence', '145'], ['Economic history of Australia', 'sentence', '311'], ['Demographics of Kazakhstan', 'sentence', '44'], ['Economy of Kenya', 'sentence', '275']]}\n",
      "The book series Opposing Viewpoints with print and online versions tries to encourage critical thinking and issue awareness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [20:45<07:41, 18.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 3006, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Mediation (Marxist theory and media studies)', 'sentence', '9'], ['37 INK', 'sentence', '0'], ['Alfred Russel Wallace', 'sentence', '414'], ['Utilitarianism (book)', 'sentence', '31'], ['J. K. Rowling', 'sentence', '353']]}\n",
      "Brazilian former professional football forward José Antônio Martins Galvão, played with his club in three countries, including in Switzerland with not a single goal made.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [21:00<06:53, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 2249, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Wendell Lira', 'sentence', '1'], ['Bruno César', 'sentence', '0'], ['Matheus Sávio', 'sentence', '0'], ['José Welison', 'sentence', '0'], ['Vágner Love', 'sentence', '12']]}\n",
      "Ephraim Stern (born 1934) returned to the Hebrew University as a professor in 1971 but did not take up any administrative position.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [21:18<06:41, 17.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9791, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Theodore Motzkin', 'sentence', '10'], ['Abraham Joshua Heschel', 'sentence', '3'], ['Abraham Joshua Heschel', 'sentence', '4'], ['History of the University of Tehran', 'sentence', '36'], ['Ralph Griswold', 'sentence', '6']]}\n",
      "From 2008-2010 Cartoon Network aired Ben 10: Alien Force.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [21:37<06:38, 18.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 30194, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Aqua Teen Hunger Force (season 1)', 'sentence', '43'], ['List of The Amazing World of Gumball episodes', 'sentence', '10'], ['List of The Amazing World of Gumball episodes', 'sentence', '11'], ['Fringe (TV series)', 'sentence', '3'], ['My Knight and Me', 'sentence', '1']]}\n",
      "Da Yooper's has released albums such as Yoopanese on July 4, 1986, Culture Shock on November 1, 1987, Camp Fever in August 1, 1988, and Yoop It Up in August 1, 1989.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [21:52<06:01, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 4844, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Billy Currington', 'sentence', '1'], ['Lowtide', 'sentence', '29'], ['Jeff Lynne', 'sentence', '132'], ['Jeff Lynne', 'sentence', '133'], ['Jeff Lynne', 'sentence', '134']]}\n",
      "Rosa Torre González served as a promoter during the 1916 Primer Congreso Feminista (First Feminist Congress).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [22:06<05:22, 16.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 42748, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Aurelia Castillo de González', 'sentence', '0'], ['Lesbian feminism', 'sentence', '176'], ['Lesbian feminism', 'sentence', '177'], ['Lesbian pulp fiction', 'sentence', '36'], ['Union, Progress and Democracy', 'sentence', '4']]}\n",
      "Democrat Eni Faleomavaega was the most senior member of the United States House of Representatives in the 105th Congress.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [22:25<05:23, 17.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 22196, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Jimmy Carter', 'sentence', '6'], ['Jack Martins', 'sentence', '2'], ['Walt Rogers', 'sentence', '1'], ['2008 United States House of Representatives elections in California', 'sentence', '147'], ['Harvey Santana', 'sentence', '0']]}\n",
      "The liquid gas CCl2F2 is used in aerosol cans.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [22:44<05:18, 17.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 12327, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Liquid nitrogen wash', 'sentence', '11'], ['Electrokinetic phenomena', 'sentence', '2'], ['Phosphorus trifluorodichloride', 'sentence', '3'], ['Argon', 'sentence', '114'], ['Argon', 'sentence', '115']]}\n",
      "Seven notable animated television series, including Super Why!, a children's educational show created by Angela C. Santomero and Samantha Freeman Alpert, Phineas and Ferb and WordGirl,  were released in September 2007.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [23:03<05:05, 17.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 15109, 'predicted_label': 'REFUTES', 'predicted_evidence': [['The Famous Adventures of Mr. Magoo', 'sentence', '0'], ['Rawhide (TV series)', 'sentence', '100'], ['Anime', 'sentence', '185'], ['Toonbox', 'sentence', '14'], [\"Regulations on children's television programming in the United States\", 'sentence', '2']]}\n",
      "In the 1966 NCAA College Division football season, the UMass Redmen football team (representing the University of Massachusetts in the NCAA Division I Football Bowl Subdivision (FBS) - the fourth oldest program in FBS) was beat Maine 10-7, lost to Darthmouth 7-17, bested Connecticut, and Rhode Island with scores 12-6 and 14-9 respectively as their first 4 games of the season.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [23:17<04:26, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 14106, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['2001 Arkansas State Indians football team', 'sentence', '0'], ['2017 Southern Utah Thunderbirds football team', 'sentence', '4']]}\n",
      "The Alexander Faribault House is a historic house museum in 12 First Avenue, Faribault, MN built by Alexander Faribault, a local tradesman and philanthropist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [23:33<04:10, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 19479, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Netherley House', 'sentence', '0'], ['Hotel Europe (Vancouver)', 'sentence', '0'], ['Clifton, Cincinnati', 'sentence', '12'], ['Albrecht Altdorfer', 'sentence', '80'], ['Queens Campus, Rutgers University', 'sentence', '67']]}\n",
      "Psathyrotes ramosissima is a species of flowering plant that is a part of the plantae family and belongs to the asteraceae family known by the common name turtleback.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [23:45<03:30, 15.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10826, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Berlandiera', 'sentence', '0'], ['Lessingia', 'sentence', '0'], ['Taylorilygus apicalis', 'sentence', '0'], ['Hottonia', 'sentence', '1']]}\n",
      "The New York state election for the 86th New York State Legislature, consisting of the New York State Senate and the New York State Assembly, was held on November 6, 1984, but no statewide elective offices were up for election.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [24:06<03:40, 16.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 5900, 'predicted_label': 'REFUTES', 'predicted_evidence': [['January 2015 Speaker of the United States House of Representatives election', 'sentence', '15'], ['Electoral reform in New York', 'sentence', '14'], ['Edgewater, New Jersey', 'sentence', '200'], ['1880 Democratic National Convention', 'sentence', '0'], ['Cresskill, New Jersey', 'sentence', '77']]}\n",
      "Radio Televisyen Malaysia has fourteen states with over 20 stations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [24:21<03:16, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 12543, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Australian Broadcasting Corporation', 'sentence', '9'], ['WBGQ', 'sentence', '0'], ['WBGQ', 'sentence', '1'], ['WFNX (101.7 FM)', 'sentence', '40'], ['Samsung Galaxy A3 (2017)', 'sentence', '10']]}\n",
      "Classical guitarist Mats Scheidegger specializes in classical music and serves as artistic director of the \"Tage für Neue Musik Zürich\" festival since 1998.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [24:36<02:54, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 45803, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Bernd Wiesemann', 'sentence', '4'], ['Bernd Wiesemann', 'sentence', '5'], ['Bernd Wiesemann', 'sentence', '6'], ['Hong Kong Sinfonietta', 'sentence', '10'], ['Neue Philharmonie Frankfurt', 'sentence', '4']]}\n",
      "John McCain won 16 delegates during the 2008 Washington Republican presidential primary, which is eight more than Mike Huckabee won.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [24:58<02:57, 17.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1150, 'predicted_label': 'SUPPORTS', 'predicted_evidence': [['South Hill, Virginia', 'sentence', '29'], ['1976 United States presidential election in Nevada', 'sentence', '9'], ['Jimmy Carter', 'sentence', '230'], ['Politics of Virginia', 'sentence', '31'], ['Don Sundquist', 'sentence', '24']]}\n",
      "In the 2014 Nuneaton and Bedworth Borough Council election in Wem Brook ward Labour Party's Tracy Sheppard won with 54% of the votes and conservative Andy Sargeant won 16%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [25:14<02:36, 17.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 5338, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Milton Keynes Council elections', 'sentence', '9'], ['2005 United Kingdom general election in England', 'sentence', '10'], ['New Milford, New Jersey', 'sentence', '101'], ['2007 Harlow District Council election', 'sentence', '6'], ['2007 Harlow District Council election', 'sentence', '7']]}\n",
      "Cheng Yuan played for various clubs since 2011 and has recorded a total of 70 appearances and less than 20 goals in his career so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [25:32<02:20, 17.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 2425, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Bruno César', 'sentence', '40'], ['Dean Henderson', 'sentence', '10'], ['Mario Frick (footballer)', 'sentence', '1'], ['Michael Brown (footballer, born 1977)', 'sentence', '93'], ['Julian de Guzman', 'sentence', '56']]}\n",
      "Out of nine schools that join the Frisco League, two of which are from Pulaski county.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [25:48<01:59, 17.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 3120, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['East Rutherford, New Jersey', 'sentence', '111'], ['Pulaski, Virginia', 'sentence', '28'], ['North Arlington, New Jersey', 'sentence', '22'], ['Pointer, Kentucky', 'sentence', '0'], ['G. David Gearhart', 'sentence', '36']]}\n",
      "Cars pre chassis number 120 (approx), about April 1958, had similar bodies to the early Berkeley “Sports” cars,  among which could be the \"Sports\" type SA322,  on the other hand, cars manufactured from about April 1958 to the end of production had vertical front door edges and internal door hinges.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [26:05<01:41, 16.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 7008, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Porsche Carrera Cup Australia Championship', 'sentence', '22']]}\n",
      "Charles Grey was a politician in 1907 and 1909.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [26:17<01:17, 15.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 4280, 'predicted_label': 'REFUTES', 'predicted_evidence': [['George William Barnard', 'sentence', '1'], ['Henry Maley', 'sentence', '0'], ['Sholto Douglas, 19th Earl of Morton', 'sentence', '0'], ['Thomas Wilding', 'sentence', '6'], ['George Manville Fenn', 'sentence', '2']]}\n",
      "The systematic name of alcohol oxidase class is alcohol:oxygen oxidoreductase and it belongs to the family oxidoreductases which is classified as EC 1 and broken down to 21 subclasses.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [26:29<00:57, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 6355, 'predicted_label': 'NOT ENOUGH INFO', 'predicted_evidence': [['Lipid', 'sentence', '167']]}\n",
      "Marit Tingelstad (born 18 June 1938 in Lier) is a Norwegian politician for the Centre Party who was elected to the Norwegian Parliament from Oppland in 1993 and had previously served in the position of deputy representative during the term 1989–1993.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [26:43<00:43, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 2116, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Jan Bøhler', 'sentence', '0'], ['Frank Jenssen', 'sentence', '1'], ['Nils Hilsen', 'sentence', '0'], ['Ole Brandt', 'sentence', '0'], ['Karoline Bjørnson', 'sentence', '0']]}\n",
      "Horst Großmann (born on 19 November 1891) was active in the army from 1911 to 1920 and ,was a recipient of the Knight's Cross of the Iron Cross with Oak Leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [26:59<00:29, 14.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 4136, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Klaus Quaet-Faslem', 'sentence', '0'], ['Anton Mader', 'sentence', '0'], ['Chari Maigumeri', 'sentence', '9'], ['Chari Maigumeri', 'sentence', '10'], ['Chari Maigumeri', 'sentence', '11']]}\n",
      "Actor and theatre director Lewis Casson was married to  Elsie Fogerty, with whom he had three children.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [27:16<00:15, 15.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 24516, 'predicted_label': 'REFUTES', 'predicted_evidence': [['Gian Sammarco', 'sentence', '9'], ['Gian Sammarco', 'sentence', '10'], ['Jack MacGowran', 'sentence', '23'], ['Joseph Cotten', 'sentence', '109'], ['Jerome K. Jerome', 'sentence', '16']]}\n",
      "In the 1984 Italian Grand Prix, Michele Alboreto of Italy scored the highest with nine points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [27:31<00:00, 16.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 8724, 'predicted_label': 'REFUTES', 'predicted_evidence': [['2017 Chinese Grand Prix', 'sentence', '78'], ['List of Formula One driver records', 'sentence', '36'], ['Lotus E22', 'sentence', '6'], ['2011 Vacansoleil–DCM season', 'sentence', '19'], ['1979 FIM Motocross World Championship', 'sentence', '5']]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# depth or breadth\n",
    "# EVIDENCE_RETRIEVAL = \"breadth\"\n",
    "EVIDENCE_RETRIEVAL = \"depth\"\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('{\"id\": \"\", \"predicted_label\": \"\", \"predicted_evidence\": \"\"}\\n')\n",
    "\n",
    "from tqdm import tqdm\n",
    "for data in tqdm(dataset[:100]):\n",
    "    \n",
    "    claim = data[\"claim\"]\n",
    "    \n",
    "    if not claim:\n",
    "        continue\n",
    "    \n",
    "    hit_stats, hit_dictionary = get_relevant_pages(claim)\n",
    "    \n",
    "    page_answers, full_abs_list, ranked_aswers, pandasData = get_answers(claim, hit_dictionary, display_table=False)\n",
    "#     print(page_answers)\n",
    "#     print(pandasData)\n",
    "#     continue\n",
    "\n",
    "    evidence_probs, verdict = CheckEntailmentNeturalorContradict(ranked_aswers,claim,pandasData)\n",
    "\n",
    "    if verdict == \"False\":\n",
    "        predicted_label = \"REFUTES\"\n",
    "    elif verdict == \"True\":\n",
    "        predicted_label = \"SUPPORTS\"\n",
    "    else:\n",
    "        predicted_label = \"NOT ENOUGH INFO\"\n",
    "        \n",
    "    pages_by_confidence = [row[0] for row in pandasData]\n",
    "        \n",
    "    evidences = []\n",
    "    \n",
    "    if EVIDENCE_RETRIEVAL == \"depth\":\n",
    "        for page in pages_by_confidence:\n",
    "            idx = page_answers[page]\n",
    "            for element in idx:\n",
    "                if len(evidences) < 5:\n",
    "                    element_type, element_id = element.split(\"_\", 1)\n",
    "                    evidence = [page, element_type, element_id]\n",
    "                    if evidence not in evidences:\n",
    "                        evidences.append(evidence)\n",
    "                else:\n",
    "                    break\n",
    "    else:\n",
    "        while len(evidences) < 5:\n",
    "            for page in pages_by_confidence:\n",
    "                idx = page_answers[page]\n",
    "                for element in idx:\n",
    "                    if len(evidences) < 5:\n",
    "                        element_type, element_id = element.split(\"_\", 1)\n",
    "                        evidence = [page, element_type, element_id]\n",
    "                        if evidence not in evidences:\n",
    "                            evidences.append(evidence)\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        break\n",
    "        \n",
    "    \n",
    "    output = {\"id\": data[\"id\"], \"predicted_label\": predicted_label, \"predicted_evidence\": evidences}\n",
    "    \n",
    "    with open(output_path, 'a', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=None)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    print(claim)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
