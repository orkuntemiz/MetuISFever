{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "torch_device = 'cuda'token_type_ids\n",
    "\n",
    "QA_TOKENIZER = AutoTokenizer.from_pretrained(\"bert-large-cased-whole-word-masking-finetuned-squad\")\n",
    "QA_MODEL = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-cased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "QA_MODEL.to(torch_device)\n",
    "QA_MODEL.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 1024)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (12): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (13): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (14): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (15): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (16): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (17): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (18): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (19): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (20): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (21): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (22): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (23): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "#from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "#SUMMARY_TOKENIZER = PegasusTokenizer.from_pretrained('mayu0007/pegasus_large_covid')\n",
    "#SUMMARY_MODEL = PegasusForConditionalGeneration.from_pretrained('mayu0007/pegasus_large_covid')\n",
    "\n",
    "SUMMARY_MODEL.to(torch_device)\n",
    "SUMMARY_MODEL.eval()\n",
    "def Summarizer(string_all):    \n",
    "    if 1==1:\n",
    "        ## try generating an exacutive summary with bart abstractive summarizer\n",
    "        allAnswersTxt = string_all.replace('\\n','')\n",
    "\n",
    "        answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n",
    "        #answers_input_ids = SUMMARY_TOKENIZER.prepare_seq2seq_batch([allAnswersTxt], return_tensors='pt', truncation=True, padding='longest')['input_ids'].to(torch_device)\n",
    "        summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n",
    "                                               num_beams=4,\n",
    "                                               length_penalty=0.8,\n",
    "                                               repetition_penalty=2.0,\n",
    "                                               min_length=5,\n",
    "                                               no_repeat_ngram_size=0,\n",
    "                                                do_sample=False )\n",
    "\n",
    "        exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "        return exec_sum\n",
    "\n",
    "# Program to find most frequent  \n",
    "# element in a list \n",
    "  \n",
    "from collections import Counter \n",
    "  \n",
    "def most_frequent(List): \n",
    "    occurence_count = Counter(List) \n",
    "    return occurence_count.most_common(1)[0][0]\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,AutoModelForSequenceClassification\n",
    "#hg_model_hub_name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "hg_model_hub_name = \"ynie/xlnet-large-cased-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "\n",
    "tokenizer_nli = AutoTokenizer.from_pretrained(hg_model_hub_name, use_fast=False)\n",
    "model_nli = AutoModelForSequenceClassification.from_pretrained(hg_model_hub_name)\n",
    "model_nli.to(torch_device)\n",
    "model_nli.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import pysearch\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from anglicize import anglicize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import statistics\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"dev.jsonl\"\n",
    "output_path = dataset_path.replace(\".jsonl\", \"_predictions.jsonl\")\n",
    "evaluation_path = dataset_path.replace(\".jsonl\", \"_evaluation.jsonl\")\n",
    "\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sqla\n",
    "\n",
    "db_fullpath = \"feverous_wikiv1.db\"\n",
    "db = sqla.create_engine(\"sqlite:///{}\".format(db_fullpath))\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "Session = sessionmaker(bind=db)\n",
    "sql_session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucene_dir = 'anserini/indexes/fever/lucene-index-fever-paragraph'\n",
    "searcher = pysearch.SimpleSearcher(lucene_dir)\n",
    "\n",
    "def get_relevant_pages(claim, FULL_CLAIM=True, SPACY_ENTITIES=True, CASE_ENTITIES=True, LINKED_PAGES=False, LINKING_PAGES=False, N_RESULTS=70):\n",
    "    \n",
    "    if not SPACY_ENTITIES and not CASE_ENTITIES and not LINKED_PAGES:\n",
    "        FULL_CLAIM = True\n",
    "    \n",
    "    start_timer = timer()\n",
    "\n",
    "    claim = nlp(claim.replace(\" \", \" \").replace(\"\", \" \")) # Replaces the obnoxious space character with normal space\n",
    "\n",
    "    keywords = set()\n",
    "    entities = set()\n",
    "\n",
    "    if SPACY_ENTITIES:\n",
    "        spacy_entities = [entity.text for entity in claim.ents if entity.label_ != \"DATE\"]\n",
    "        entities.update(spacy_entities)\n",
    "\n",
    "    if CASE_ENTITIES:\n",
    "        case_entities = set()\n",
    "        chunks = claim.noun_chunks\n",
    "        for chunk in chunks:\n",
    "            for token in tokenizer(chunk.text):\n",
    "                if token.text[0].isupper():\n",
    "                    case_entities.add(chunk.text)\n",
    "                    break\n",
    "\n",
    "        entities.update(case_entities)\n",
    "        #print(case_entities)\n",
    "        #print(entities)\n",
    "        #sys.exit(0)\n",
    "\n",
    "    keywords.update(entities)\n",
    "\n",
    "    if not FULL_CLAIM:\n",
    "        search_query = (\", \".join(keywords) + '\"' + '\", \"'.join(entities) + '\"')\n",
    "#            search_query = (\", \".join(keywords) + \", \".join(entities))\n",
    "    else:\n",
    "        search_query = (claim.text + ' \"' + '\", \"'.join(keywords) + '\" \"' + '\", \"'.join(entities) + '\"')\n",
    "#            search_query = (claim.text + \" \" + \", \".join(keywords) + \", \".join(entities))\n",
    "\n",
    "    if FULL_CLAIM or keywords:\n",
    "        try:\n",
    "            lucene_hits = searcher.search(search_query.encode(\"utf-8\"), k=N_RESULTS)\n",
    "        except:\n",
    "            lucene_hits = None\n",
    "    else:\n",
    "        lucene_hits = None\n",
    "\n",
    "    hit_dictionary = {}\n",
    "\n",
    "    if lucene_hits:\n",
    "\n",
    "        for hit in lucene_hits:\n",
    "            hit_abs = hit.lucene_document.get(\"raw\").split(\" ~~ \")\n",
    "            step = 10\n",
    "            counter = 0\n",
    "            for i in range(0, len(hit_abs), step):\n",
    "                temp_abs = \" ~~ \".join(hit_abs[i:min([len(hit_abs), i+step])])\n",
    "                hit_dict = {\"abstract\": None, \"real_abstract\": None, \"title\": None}\n",
    "#                 hit_dict['abstract']=hit.lucene_document.get(\"raw\")\n",
    "#                 hit_dict['real_abstract']=hit.lucene_document.get(\"raw\")\n",
    "                hit_dict['title'] = str(hit.docid)\n",
    "                hit_dict['abstract'] = temp_abs\n",
    "                hit_dict['real_abstract'] = temp_abs\n",
    "                hit_dictionary[\"{}_{}\".format(hit.docid, counter)] = hit_dict\n",
    "                counter += 1\n",
    "    \n",
    "        linked_pages = set()\n",
    "        if LINKED_PAGES:\n",
    "            for hit in lucene_hits:\n",
    "                links = re.findall(r\"(?:\\[\\[)(.*?)(?:\\|)\", hit.raw)\n",
    "                for link in links:\n",
    "                    linked_pages.update([link.replace(\"_\", \" \").encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\")])\n",
    "                    \n",
    "        linking_pages = set()\n",
    "        if LINKING_PAGES and sql_session:\n",
    "            linking_results = sql_session.execute('select target, sources from inlinks where target IN (\"{}\")'.format('\", \"'.join([hit.docid.replace('\"', '\"\"') for hit in lucene_hits])))\n",
    "            for row in linking_results:\n",
    "                linking_pages.update(row[\"sources\"].split(\";\"))\n",
    "                    \n",
    "        found_pages = [hit.docid for hit in lucene_hits]\n",
    "        for i in range(0, len(found_pages)):\n",
    "\n",
    "            try:\n",
    "                found_pages[i] = found_pages[i].encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "                #print(\"Done:\",found_pages[i])\n",
    "            except:\n",
    "                import regex\n",
    "                print(found_pages[i], anglicize(found_pages[i]), regex.sub(r'[^\\p{Latin}]', '', found_pages[i]).encode(\"latin-1\").decode(\"utf-8\"))\n",
    "                \n",
    "        found_pages = set(found_pages)\n",
    "\n",
    "        hit_scores = [hit.score for hit in lucene_hits]\n",
    "        hit_score_min = min(hit_scores)\n",
    "        hit_score_25 = np.percentile(hit_scores, 25)\n",
    "        hit_score_mean = np.mean(hit_scores)\n",
    "        hit_score_median = statistics.median(hit_scores)\n",
    "        hit_score_75 = np.percentile(hit_scores, 75)\n",
    "        hit_score_max = max(hit_scores)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        is_found = False\n",
    "        found_pages = set()\n",
    "        entities = set()\n",
    "        keywords = set()\n",
    "        linked_pages = set()\n",
    "        linking_pages = set()\n",
    "        lucene_hits = []\n",
    "        hit_scores = []\n",
    "        hit_score_min = None\n",
    "        hit_score_25 = None\n",
    "        hit_score_mean = None\n",
    "        hit_score_median = None\n",
    "        hit_score_75 = None\n",
    "        hit_score_max = None\n",
    "\n",
    "    end_timer = timer()\n",
    "    elapsed = int(end_timer - start_timer)\n",
    "    elapsed_formatted = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))\n",
    "        \n",
    "    result = [claim.text, elapsed_formatted, \n",
    "                found_pages, \n",
    "                keywords, \n",
    "                linked_pages, \n",
    "                linking_pages,\n",
    "                len(lucene_hits), \n",
    "                hit_score_min, hit_score_25, hit_score_mean, hit_score_median, \n",
    "                hit_score_75, hit_score_max, hit_scores]\n",
    "\n",
    "    #print(result)\n",
    "    \n",
    "    result = pd.DataFrame([result], columns=[\"CLAIM\", \"ELAPSED\", \n",
    "                                             \"PAGES_FOUND\",\n",
    "                                             \"KEYWORDS\", \n",
    "                                             \"LINKED_PAGES\",\n",
    "                                             \"LINKING_PAGES\",\n",
    "                                             \"N_LUCENE_HITS\", \n",
    "                                             \"HIT_SCORE_MIN\", \"HIT_SCORE_25\", \"HIT_SCORE_MEAN\", \"HIT_SCORE_MEDIAN\", \n",
    "                                             \"HIT_SCORE_75\", \"HIT_SCORE_MAX\", \"HIT_SCORES\"])\n",
    "    \n",
    "    return result, hit_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckEntailmentNeturalorContradict(ranked_aswers, string_all, pandasData):\n",
    "    if 1 == 1:\n",
    "        premise_list = []\n",
    "        hypothesis_list = []\n",
    "        entailment_prob = []\n",
    "        neutral_prob = []\n",
    "        contradict_prob = []\n",
    "        final_result_list = []\n",
    "        if len(ranked_aswers) > 8:\n",
    "            v_range = 8\n",
    "        else:\n",
    "            v_range = len(ranked_aswers)\n",
    "        for i in range(v_range):\n",
    "            if pandasData[i][2] >= 0.4:\n",
    "                max_length = 512\n",
    "                candidate_answer = Summarizer(ranked_aswers[i])\n",
    "                # premise = \"symptoms of covid-19 range from none (asymptomatic) to severe pneumonia and it can be fatal. fever and cough are the most common symptoms in patients with 2019-ncov infection. Several people have muscle soreness or fatigue as well as ards. diarrhea, hemoptysis, headache, sore throat, shock, and other symptoms only occur in a small number of patients\"\n",
    "                if len(candidate_answer) > len(ranked_aswers[i]):\n",
    "                    premise = ranked_aswers[i]\n",
    "                else:\n",
    "                    premise = candidate_answer\n",
    "                # premise = \"covid-19 symptom is highly various in each patient, with fever, fatigue, shortness of breath, and cough as the main presenting symptoms. patient with covid-19 may shows severe symptom with severe pneumonia and ards, mild symptom resembling simple upper respiration tract infection, or even completely asymptomatic. approximately 80 % of cases is mild \"\n",
    "                hypothesis = string_all\n",
    "                tokenized_input_seq_pair = tokenizer_nli.encode_plus(\n",
    "                    premise,\n",
    "                    hypothesis,\n",
    "                    max_length=max_length,\n",
    "                    return_token_type_ids=True,\n",
    "                    truncation=True,\n",
    "                )\n",
    "\n",
    "                input_ids = (\n",
    "                    torch.Tensor(tokenized_input_seq_pair[\"input_ids\"])\n",
    "                    .long()\n",
    "                    .unsqueeze(0)\n",
    "                    .to(torch_device)\n",
    "                )\n",
    "                # remember bart doesn't have 'token_type_ids', remove the line below if you are using bart.\n",
    "                token_type_ids = (\n",
    "                    torch.Tensor(tokenized_input_seq_pair[\"token_type_ids\"])\n",
    "                    .long()\n",
    "                    .unsqueeze(0)\n",
    "                    .to(torch_device)\n",
    "                )\n",
    "                attention_mask = (\n",
    "                    torch.Tensor(tokenized_input_seq_pair[\"attention_mask\"])\n",
    "                    .long()\n",
    "                    .unsqueeze(0)\n",
    "                    .to(torch_device)\n",
    "                )\n",
    "\n",
    "                outputs = model_nli(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=None,\n",
    "                )\n",
    "                # Note:\n",
    "                # \"id2label\": {\n",
    "                #     \"0\": \"entailment\",\n",
    "                #     \"1\": \"neutral\",\n",
    "                #     \"2\": \"contradiction\"\n",
    "                # },\n",
    "\n",
    "                predicted_probability = torch.softmax(outputs[0], dim=1)[\n",
    "                    0\n",
    "                ].tolist()  # batch_size only one\n",
    "\n",
    "                # print(\"Premise:\", premise)\n",
    "                premise_list.append(premise)\n",
    "                # print(\"Hypothesis:\", hypothesis)\n",
    "                hypothesis_list.append(hypothesis)\n",
    "                # print(\"input Ans:\",ranked_aswers[i])\n",
    "                # print(\"Entailment:\", predicted_probability[0])\n",
    "                entailment_prob.append(predicted_probability[0])\n",
    "                # print(\"Neutral:\", predicted_probability[1])\n",
    "                neutral_prob.append(predicted_probability[1])\n",
    "                # print(\"Contradiction:\", predicted_probability[2])\n",
    "                contradict_prob.append(predicted_probability[2])\n",
    "        if len(premise_list) > 0:\n",
    "            avg_entailment = np.average(entailment_prob)\n",
    "            avg_neutral = np.average(neutral_prob)\n",
    "            avg_contradiction = np.average(contradict_prob)\n",
    "            zipped_list = zip(entailment_prob, neutral_prob, contradict_prob)\n",
    "            for key, val in enumerate(hypothesis_list):\n",
    "                if neutral_prob[key] > 0.8:\n",
    "                    final_result = \"Neutral\"\n",
    "                elif entailment_prob[key] > contradict_prob[key]:\n",
    "                    final_result = \"True\"\n",
    "                    final_result_list.append(final_result)\n",
    "                else:\n",
    "                    final_result = \"False\"\n",
    "                    final_result_list.append(final_result)\n",
    "            if len(final_result_list) > 0:\n",
    "                verdict = most_frequent(final_result_list)\n",
    "            else:\n",
    "                verdict = \"Neutral\"\n",
    "        else:\n",
    "            #       zipped_list = zip(0,1,0)\n",
    "            zipped_list = zip([0], [1], [0])\n",
    "            verdict = \"Neutral\"\n",
    "    return zipped_list, verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canonical_title(title):\n",
    "    return title.split(\"_\")[0]\n",
    "\n",
    "def get_entities(text):\n",
    "    claim = nlp(text.replace(\" \", \" \").replace(\"\", \" \")) # Replaces the obnoxious space character with normal space\n",
    "\n",
    "    entities = set()\n",
    "    \n",
    "    spacy_entities = [entity.text for entity in claim.ents if entity.label_ != \"DATE\"]\n",
    "    entities.update(spacy_entities)\n",
    "    \n",
    "    claim_processed = nlp(text.replace(\" \", \" \").replace(\"\", \" \").replace(\"'\", \". \"))\n",
    "    \n",
    "    spacy_entities_processed = [entity.text for entity in claim_processed.ents if entity.label_ != \"DATE\"]\n",
    "    entities.update(spacy_entities)\n",
    "\n",
    "    case_entities = set()\n",
    "    chunks = claim.noun_chunks\n",
    "    for chunk in chunks:\n",
    "        for token in tokenizer(chunk.text):\n",
    "            if token.text[0].isupper():\n",
    "                case_entities.add(chunk.text)\n",
    "                break\n",
    "\n",
    "        entities.update(case_entities)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def get_people(text):\n",
    "    \n",
    "    claim = nlp(text.replace(\" \", \" \").replace(\"\", \" \")) # Replaces the obnoxious space character with normal space\n",
    "    \n",
    "    people = set()\n",
    "\n",
    "    for ent in claim.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            people.add(ent.text)\n",
    "            \n",
    "    claim_processed = nlp(text.replace(\" \", \" \").replace(\"\", \" \").replace(\"'\", \". \"))\n",
    "    \n",
    "    for ent in claim_processed.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            people.add(ent.text)\n",
    "            \n",
    "    case_entities = set()\n",
    "    chunks = claim_processed.noun_chunks\n",
    "    for chunk in chunks:\n",
    "        for token in tokenizer(chunk.text):\n",
    "            if token.text[0].isupper():\n",
    "                case_entities.add(chunk.text)\n",
    "                break\n",
    "                \n",
    "#     print(case_entities)\n",
    "                \n",
    "    for entity in case_entities:\n",
    "        candidate_entities = nlp(entity).ents\n",
    "        for candidate in candidate_entities:\n",
    "            if candidate.label_ == \"PERSON\":\n",
    "                people.add(candidate.text)\n",
    "            \n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_cells(page, search_for):\n",
    "    import json\n",
    "    import Levenshtein\n",
    "\n",
    "    result = sql_session.execute('select * from wiki where id = \"{}\"'.format(page)).fetchone()\n",
    "    if not result:\n",
    "#         print(\"Page not found:\",page)\n",
    "        return []\n",
    "    data = json.loads(result[1])\n",
    "    tables = [element for element in data[\"order\"] if \"table_\" in element]\n",
    "\n",
    "    matches = []\n",
    "    for table in tables:\n",
    "        rows = data[table][\"table\"]\n",
    "        for row in rows:\n",
    "            for cell in row:\n",
    "                for searching in search_for:\n",
    "                    similarities = [\n",
    "                        Levenshtein.ratio(searching, cell[\"value\"]), \n",
    "                        Levenshtein.ratio(searching, re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", cell[\"value\"], flags=re.MULTILINE))\n",
    "                    ]\n",
    "\n",
    "                    if max(similarities) >= 0.8:\n",
    "                        matches.append([max(similarities), cell[\"id\"]])\n",
    "                        break\n",
    "\n",
    "    matches.sort(key=lambda x: float(x[0]), reverse=True)\n",
    "\n",
    "    return [match[1] for match in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def embed_useT(module):\n",
    "    with tf.Graph().as_default():\n",
    "        sentences = tf.compat.v1.placeholder(tf.string)\n",
    "        embed = hub.Module(module)\n",
    "        embeddings = embed(sentences)\n",
    "        session = tf.compat.v1.train.MonitoredSession()\n",
    "    return lambda x: session.run(embeddings, {sentences: x})\n",
    "embed_fn = embed_useT('/home/titanx/kaggle/working/sentence_wise_email/module/module_useT')\n",
    "\n",
    "def get_answers(query, hit_dictionary, display_table=False):\n",
    "    import re\n",
    "    for idx,v in hit_dictionary.items():\n",
    "        abs_dirty = v['abstract']\n",
    "        real_abs_dirty = v['real_abstract']\n",
    "        #abs_dirty = v['paragraph']\n",
    "        # looks like the abstract value can be an empty list\n",
    "        v['abstract_paragraphs'] = []\n",
    "        v['abstract_full'] = ''\n",
    "        v['real_abstract_full'] = ''\n",
    "        v['real_abstract_paragraphs']=[]\n",
    "\n",
    "        if abs_dirty:\n",
    "            # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
    "            # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
    "            # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
    "\n",
    "            if isinstance(abs_dirty, list):\n",
    "                for p in abs_dirty:\n",
    "                    v['abstract_paragraphs'].append(p['text'])\n",
    "                    v['abstract_full'] += p['text'] + ' \\n\\n'\n",
    "\n",
    "            # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
    "            if isinstance(abs_dirty, str):\n",
    "                v['abstract_paragraphs'].append(abs_dirty)\n",
    "                v['abstract_full'] += abs_dirty + ' \\n\\n'\n",
    "        if real_abs_dirty:\n",
    "            # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
    "            # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
    "            # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
    "\n",
    "\n",
    "            if isinstance(real_abs_dirty, list):\n",
    "                for p in real_abs_dirty:\n",
    "                    v['real_abstract_paragraphs'].append(p['text'])\n",
    "                    v['real_abstract_full'] += p['text'] + ' \\n\\n'\n",
    "\n",
    "            # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
    "            if isinstance(abs_dirty, str):\n",
    "                v['real_abstract_paragraphs'].append(real_abs_dirty)\n",
    "                v['real_abstract_full'] += real_abs_dirty + ' \\n\\n'\n",
    "        v[\"indexed_para\"]=v[\"abstract_full\"][len(v[\"real_abstract_full\"]):]\n",
    "        \n",
    "        \n",
    "        v[\"abstract_full_processed\"] = re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", v[\"abstract_full\"], flags=re.MULTILINE) # Link format removal\n",
    "        v[\"abstract_full_processed\"] = re.sub(r\"((sentence|table|cell|section|list|item)(_[0-9]+)+ )\", \"\", v[\"abstract_full_processed\"], flags=re.MULTILINE)\n",
    "        v[\"abstract_full_processed\"] = v[\"abstract_full_processed\"].replace(\"~~\", \". ~~ .\")\n",
    "#     v[\"abstract_full_processed\"] = v[\"abstract_full_processed\"].replace(\"~~\", \" ~~ \")\n",
    "        \n",
    "\n",
    "#     def embed_useT(module):\n",
    "#         with tf.Graph().as_default():\n",
    "#             sentences = tf.compat.v1.placeholder(tf.string)\n",
    "#             embed = hub.Module(module)\n",
    "#             embeddings = embed(sentences)\n",
    "#             session = tf.compat.v1.train.MonitoredSession()\n",
    "#         return lambda x: session.run(embeddings, {sentences: x})\n",
    "#     embed_fn = embed_useT('/home/titanx/kaggle/working/sentence_wise_email/module/module_useT')\n",
    "\n",
    "    import numpy as np\n",
    "    #Reconstruct text for generating the answer text from the BERT result, do the necessary text preprocessing and construct the answer.\n",
    "    def reconstructText(tokens, start=0, stop=-1):\n",
    "        tokens = tokens[start: stop]\n",
    "        if '[SEP]' in tokens:\n",
    "            sepind = tokens.index('[SEP]')\n",
    "            tokens = tokens[sepind+1:]\n",
    "        txt = ' '.join(tokens)\n",
    "        txt = txt.replace(' ##', '')\n",
    "        txt = txt.replace('##', '')\n",
    "        txt = txt.strip()\n",
    "        txt = \" \".join(txt.split())\n",
    "        txt = txt.replace(' .', '.')\n",
    "        txt = txt.replace('( ', '(')\n",
    "        txt = txt.replace(' )', ')')\n",
    "        txt = txt.replace(' - ', '-')\n",
    "        txt_list = txt.split(' , ')\n",
    "        txt = ''\n",
    "        nTxtL = len(txt_list)\n",
    "        if nTxtL == 1:\n",
    "            return txt_list[0]\n",
    "        newList =[]\n",
    "        for i,t in enumerate(txt_list):\n",
    "            if i < nTxtL -1:\n",
    "                if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n",
    "                    newList += [t,',']\n",
    "                else:\n",
    "                    newList += [t, ', ']\n",
    "            else:\n",
    "                newList += [t]\n",
    "        return ''.join(newList)\n",
    "\n",
    "\n",
    "    def makeBERTSQuADPrediction(title, document, question):\n",
    "        title = get_canonical_title(title)\n",
    "        ## we need to rewrite this function so that it chuncks the document into 250-300 word segments with\n",
    "        ## 50 word overlaps on either end so that it can understand and check longer abstracts\n",
    "        ## Get chuck of documents in order to overcome the BERT's max 512 token problem. First count the number of tokens. For example if it is\n",
    "        ##   larger than 2048 token determine the split size and overlapping token count. Split the document into pieces to fed into BERT encoding. Overlapping\n",
    "        ##      is necessary to maintain the coherence between the splits. If the token count is larger than some number 2560 token approximately this will fail.\n",
    "        nWords = len(document.split())\n",
    "        input_ids_all = QA_TOKENIZER.encode(question, document)\n",
    "        tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n",
    "        overlapFac = 1.1\n",
    "        if len(input_ids_all)*overlapFac > 2560:\n",
    "            nSearchWords = int(np.ceil(nWords/6))\n",
    "            fifth = int(np.ceil(nWords/5))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[fifth-int(nSearchWords*overlapFac/2):fifth+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[fifth*2-int(nSearchWords*overlapFac/2):fifth*2+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[fifth*3-int(nSearchWords*overlapFac/2):fifth*3+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[fifth*4-int(nSearchWords*overlapFac/2):fifth*4+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "\n",
    "        elif len(input_ids_all)*overlapFac > 2048:\n",
    "            nSearchWords = int(np.ceil(nWords/5))\n",
    "            quarter = int(np.ceil(nWords/4))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
    "            \n",
    "        elif len(input_ids_all)*overlapFac > 1536:\n",
    "            nSearchWords = int(np.ceil(nWords/4))\n",
    "            third = int(np.ceil(nWords/3))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
    "            \n",
    "        elif len(input_ids_all)*overlapFac > 1024:\n",
    "            nSearchWords = int(np.ceil(nWords/3))\n",
    "            middle = int(np.ceil(nWords/2))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "        elif len(input_ids_all)*overlapFac > 512:\n",
    "            nSearchWords = int(np.ceil(nWords/2))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "        else:\n",
    "            input_ids = [input_ids_all]\n",
    "        absTooLong = False    \n",
    "        \n",
    "        answers = []\n",
    "        cons = []\n",
    "        #print(input_ids)\n",
    "        for iptIds in input_ids:\n",
    "            tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n",
    "            #print(tokens)\n",
    "            sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)\n",
    "            num_seg_a = sep_index + 1\n",
    "            num_seg_b = len(iptIds) - num_seg_a\n",
    "            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "            assert len(segment_ids) == len(iptIds)\n",
    "            n_ids = len(segment_ids)\n",
    "            #print(n_ids)\n",
    "            if n_ids < 512:\n",
    "                outputs = QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
    "                                        token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n",
    "                #print(QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
    "                #                        token_type_ids=torch.tensor([segment_ids]).to(torch_device)))\n",
    "            else:\n",
    "                #this cuts off the text if its more than 512 words so it fits in model space\n",
    "                #need run multiple inferences for longer text. add to the todo. I will try to expand this to longer sequences if it is necessary. \n",
    "#                 print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n",
    "                absTooLong = True\n",
    "                outputs= QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device), \n",
    "                                        token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))\n",
    "            start_scores=outputs.start_logits\n",
    "            end_scores=outputs.end_logits\n",
    "            start_scores = start_scores[:,1:-1]\n",
    "            end_scores = end_scores[:,1:-1]\n",
    "            answer_start = torch.argmax(start_scores)\n",
    "            answer_end = torch.argmax(end_scores)\n",
    "            #print(answer_start, answer_end)\n",
    "            answer = reconstructText(tokens, answer_start, answer_end+2)\n",
    "        \n",
    "            if answer.startswith('. ') or answer.startswith(', '):\n",
    "                answer = answer[2:]\n",
    "                \n",
    "            c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n",
    "            answers.append(answer)\n",
    "            cons.append(c)\n",
    "        \n",
    "        maxC = max(cons)\n",
    "        iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n",
    "        \n",
    "        entities = get_entities(question)\n",
    "        people = get_people(question)\n",
    "        \n",
    "        title = get_canonical_title(title)\n",
    "        \n",
    "#         entity_similarities = [Levenshtein.ratio(entity, title) for entity in entities]\n",
    "#         person_similarities = [Levenshtein.ratio(person, title) for person in people]\n",
    "        \n",
    "        person_found = False\n",
    "        \n",
    "        for person in people:\n",
    "#             print(person, title)\n",
    "            if person in title or title in person:\n",
    "#                 print(\"buldu\")\n",
    "                person_found = True\n",
    "                break\n",
    "        \n",
    "#         if person_similarities and max(person_similarities) >= 0.8:\n",
    "#         if title in people:\n",
    "        if person_found:\n",
    "#             confidence = max([0.95, cons[iMaxC]])\n",
    "            confidence = 2*cons[iMaxC]\n",
    "            \n",
    "#             print(confidence)\n",
    "#         elif entity_similarities and max(entity_similarities) >= 0.8:\n",
    "#             confidence = max([0.9, cons[iMaxC]])\n",
    "        else:\n",
    "            confidence = cons[iMaxC]\n",
    "            \n",
    "        answer = answers[iMaxC]\n",
    "        \n",
    "        sep_index = tokens_all.index('[SEP]')\n",
    "        full_txt_tokens = tokens_all[sep_index+1:]\n",
    "        \n",
    "        abs_returned = reconstructText(full_txt_tokens)\n",
    "\n",
    "        ans={}\n",
    "        ans['answer'] = answer\n",
    "        #print(answer)\n",
    "        if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n",
    "            ans['confidence'] = -1000000\n",
    "        else:\n",
    "            #confidence = torch.max(start_scores) + torch.max(end_scores)\n",
    "            #confidence = np.log(confidence.item())\n",
    "            ans['confidence'] = confidence\n",
    "        #ans['start'] = answer_start.item()\n",
    "        #ans['end'] = answer_end.item()\n",
    "        ans['abstract_bert'] = abs_returned\n",
    "        ans['abs_too_long'] = absTooLong\n",
    "        return ans\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    def searchAbstracts(hit_dictionary, question):\n",
    "        abstractResults = {}\n",
    "#         for k,v in tqdm(hit_dictionary.items()):\n",
    "        for k,v in hit_dictionary.items():\n",
    "#             abstract = v['abstract_full']\n",
    "            abstract = v['abstract_full_processed']\n",
    "#             print(abstract)\n",
    "            indexed_para=v['indexed_para']\n",
    "            title = v[\"title\"]\n",
    "            if abstract:\n",
    "                ans = makeBERTSQuADPrediction(title, abstract, question)\n",
    "                if ans['answer']:\n",
    "                    confidence = ans['confidence']\n",
    "#                     print(title,\"-\",confidence)\n",
    "                    abstractResults[confidence]={}\n",
    "                    abstractResults[confidence]['answer'] = ans['answer']\n",
    "                    #abstractResults[confidence]['start'] = ans['start']\n",
    "                    #abstractResults[confidence]['end'] = ans['end']\n",
    "                    abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n",
    "                    abstractResults[confidence]['idx'] = k\n",
    "                    abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n",
    "\n",
    "                    \n",
    "        cList = list(abstractResults.keys())\n",
    "        if cList:\n",
    "            maxScore = max(cList)\n",
    "            total = 0.0\n",
    "            exp_scores = []\n",
    "            for c in cList:\n",
    "                s = np.exp(c-maxScore)\n",
    "                exp_scores.append(s)\n",
    "            total = sum(exp_scores)\n",
    "            for i,c in enumerate(cList):\n",
    "                abstractResults[exp_scores[i]/total] = abstractResults.pop(c)\n",
    "                \n",
    "        return abstractResults\n",
    "\n",
    "    answers = searchAbstracts(hit_dictionary, query)\n",
    "\n",
    "    workingPath = '/root/kaggle/working'\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "    #from summarizer import Summarizer\n",
    "    #summarizerModel = Summarizer()\n",
    "    def displayResults(hit_dictionary, answers, question, display_table=False):\n",
    "        \n",
    "        question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n",
    "        #all_HTML_txt = question_HTML\n",
    "        confidence = list(answers.keys())\n",
    "        confidence.sort(reverse=True)\n",
    "        \n",
    "        page_answers = dict()\n",
    "\n",
    "        for c in confidence:\n",
    "#             if c>0 and c <= 1 and len(answers[c]['answer']) != 0:\n",
    "            if c>0 and len(answers[c]['answer']) != 0:\n",
    "                if 'idx' not in  answers[c]:\n",
    "                    continue\n",
    "                rowData = []\n",
    "                idx = answers[c]['idx']\n",
    "                title = hit_dictionary[idx]['title']\n",
    "                \n",
    "                full_abs = answers[c]['abstract_bert']\n",
    "                bert_ans = answers[c]['answer']\n",
    "                \n",
    "                \n",
    "                #split_abs = full_abs.split(bert_ans)\n",
    "                #x=(split_abs[0][:split_abs[0].rfind('. ')+1])\n",
    "                #sentance_beginning = x[x.rfind('. ')+1:] + split_abs[0][split_abs[0].rfind('. ')+1:]\n",
    "                #sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n",
    "                x=''\n",
    "                y=''\n",
    "                z=''\n",
    "                t=''\n",
    "\n",
    "                # print(bert_ans)\n",
    "                split_abs = full_abs.split(bert_ans)\n",
    "                \n",
    "                if split_abs[0][-5:] == \"~ ~. \":\n",
    "                    answer_sentence_start = True\n",
    "                else:\n",
    "                    answer_sentence_start = False\n",
    "                \n",
    "#                 if get_canonical_title(title) == \"Abraham Annan\":\n",
    "#                     print(\"Buraya bak:\",split_abs[0])\n",
    "#                     print(split_abs[0][-5:])\n",
    "                    \n",
    "                if len(split_abs[0].split(\"~ ~\")) > 1:\n",
    "                    sentance_beginning = \" ~ ~ \".join(split_abs[0].split(\"~ ~\")[-2:]) # Also includes the previous sentence\n",
    "                else:\n",
    "                    sentance_beginning = split_abs[0].split(\"~ ~\")[-1] # Only the sentence that includes the answer\n",
    "                if len(split_abs) > 1:\n",
    "                    sentance_end = split_abs[1].split(\"~ ~\")[0]\n",
    "                else:\n",
    "                    sentance_end = \"\"\n",
    "                \n",
    "                if answer_sentence_start:\n",
    "                    sentance_full = sentance_beginning + \"~ ~\" + bert_ans + sentance_end\n",
    "                else:\n",
    "                    sentance_full = sentance_beginning + bert_ans + sentance_end\n",
    "                    \n",
    "#                 print(sentance_full)\n",
    "                \n",
    "                sentences = sentance_full.split(\"~ ~\")\n",
    "                \n",
    "                abstract_full = hit_dictionary[idx][\"abstract_full\"]\n",
    "                \n",
    "                all_answer_ids = set()\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                \n",
    "                    for element in abstract_full.split(\" ~~ \"):\n",
    "                        element = re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", element, flags=re.MULTILINE) # Link format removal\n",
    "                        answer_ids = re.findall(r\"((sentence|table|cell|section|list|item)(_[0-9]+)+ )\", element)\n",
    "\n",
    "                        if not answer_ids:\n",
    "                            continue\n",
    "\n",
    "                        answer_ids_formatted = set()\n",
    "\n",
    "                        for ids in answer_ids:\n",
    "                            if isinstance(ids, str):\n",
    "                                candidate = ids.replace(\" \", \"\")\n",
    "                                if \"_\" in candidate and candidate[0] != \"_\" and candidate not in [\"sentence\", \"table\", \"cell\", \"section\", \"list\", \"item\"]:  # very hacky\n",
    "                                    answer_ids_formatted.add(candidate)\n",
    "                            else:\n",
    "                                for id in ids:\n",
    "                                    candidate = id.replace(\" \", \"\")\n",
    "                                    if \"_\" in candidate and candidate[0] != \"_\" and candidate not in [\"sentence\", \"table\", \"cell\", \"section\", \"list\", \"item\"]:  # very hacky\n",
    "                                        answer_ids_formatted.add(candidate)\n",
    "                                        \n",
    "                                        \n",
    "#                         if get_canonical_title(title) == \"Abraham Annan\":\n",
    "#                             print(\"Abraham Annan\",answer_ids, answer_ids_formatted)\n",
    "                                        \n",
    "#                         all_answer_ids.extend(answer_ids_formatted)\n",
    "#                         print(answer_ids_formatted, title)\n",
    "                        for answer_id_formatted in answer_ids_formatted:\n",
    "#                             if get_canonical_title(title) == \"Abraham Annan\":\n",
    "#                                 print(element)\n",
    "#                                 print(answer_id_formatted)\n",
    "#                                 print(element.replace(answer_id_formatted, \"\").strip())\n",
    "#                                 print(sentence)\n",
    "#                                 print(\"=======\")\n",
    "                            if Levenshtein.ratio(element.replace(answer_id_formatted, \"\").strip(), sentence) >= 0.9:\n",
    "                                all_answer_ids.add(answer_id_formatted)\n",
    "                \n",
    "                \n",
    "#                 sentence_pre = abstract_full.split(sentance_full)[0].split(\"~~\")[-1]\n",
    "                \n",
    "                \n",
    "#                 print(sentance_full, full_abs)\n",
    "\n",
    "#                 answer_ids = re.findall(r\"((sentence|table|cell|section|list|item)( _ [0-9]+)+ )\", sentance_full)\n",
    "#                 answer_ids = re.findall(r\"((sentence|table|cell|section|list|item)(_[0-9]+)+ )\", sentence_pre)\n",
    "#                 answer_ids_formatted = []\n",
    "\n",
    "#                 for ids in answer_ids:\n",
    "#                     if isinstance(ids, str):\n",
    "#                         candidate = ids.replace(\" \", \"\")\n",
    "#                         if \"_\" in candidate and candidate[0] != \"_\" and candidate not in [\"sentence\", \"table\", \"cell\", \"section\", \"list\", \"item\"]:  # very hacky\n",
    "#                             answer_ids_formatted.append(candidate)\n",
    "#                     else:\n",
    "#                         for id in ids:\n",
    "#                             candidate = id.replace(\" \", \"\")\n",
    "#                             if \"_\" in candidate and candidate[0] != \"_\" and candidate not in [\"sentence\", \"table\", \"cell\", \"section\", \"list\", \"item\"]:  # very hacky\n",
    "#                                 answer_ids_formatted.append(candidate)\n",
    "                                \n",
    "#                 print(answer_ids_formatted)\n",
    "                \n",
    "#                 page_answers[title] = answer_ids_formatted\n",
    "                page_answers[get_canonical_title(title)] = list(all_answer_ids)\n",
    "#                 page_answers[title] = element_id\n",
    "\n",
    "                answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n",
    "                answers[c]['partial_answer'] = bert_ans+sentance_end\n",
    "                answers[c]['sentence_beginning'] = sentance_beginning\n",
    "                answers[c]['sentence_end'] = sentance_end\n",
    "                answers[c]['title'] = get_canonical_title(title)\n",
    "            else:\n",
    "                answers.pop(c)\n",
    "        \n",
    "        \n",
    "        ## now rerank based on semantic similarity of the answers to the question\n",
    "        ## Universal sentence encoder\n",
    "        cList = list(answers.keys())\n",
    "        allAnswers = [answers[c]['full_answer'] for c in cList]\n",
    "        \n",
    "        messages = [question]+allAnswers\n",
    "        \n",
    "        encoding_matrix = embed_fn(messages)\n",
    "        similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n",
    "        rankings = similarity_matrix[1:,0]\n",
    "        \n",
    "        for i,c in enumerate(cList):\n",
    "            answers[rankings[i]] = answers.pop(c)\n",
    "\n",
    "        ## now form pandas dv\n",
    "        confidence = list(answers.keys())\n",
    "        confidence.sort(reverse=True)\n",
    "        pandasData = []\n",
    "        ranked_aswers = []\n",
    "        full_abs_list=[]\n",
    "        for c in confidence:\n",
    "            rowData=[]\n",
    "            title = answers[c]['title']\n",
    "            idx = answers[c]['idx']\n",
    "            rowData += [idx]            \n",
    "            sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='red'>\"+answers[c]['answer']+\"</font> \"+answers[c]['sentence_end']+'</div>'\n",
    "            answer_key = answers[c]['answer'].split(\"\\t\")[-1].strip() if \"\\t\" in answers[c]['answer'] else answers[c]['answer'].strip()\n",
    "\n",
    "            # rowData += [sentance_html, answer_key, c,title]\n",
    "            rowData += [sentance_html, c,title]\n",
    "            pandasData.append(rowData)\n",
    "            ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n",
    "            full_abs_list.append(' '.join([answers[c]['abstract_bert']]))\n",
    "        \n",
    "        people = get_people(question)\n",
    "        \n",
    "        for row_id, row in enumerate(pandasData):\n",
    "            title = row[3]\n",
    "            for person in people:\n",
    "                if title in person or person in title:\n",
    "                    pandasData[row_id][2] = pandasData[row_id][2] * 2\n",
    "                    break\n",
    "                    \n",
    "        pandasData.sort(key=lambda x: float(x[2]), reverse=True)\n",
    "        \n",
    "        pdata2 = pandasData\n",
    "            \n",
    "        if display_table:\n",
    "            display(HTML(question_HTML))\n",
    "\n",
    "        #    display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>Body of Evidence:</b></div>'))\n",
    "\n",
    "            # df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Answer Key', 'Confidence', 'Title/Link'])\n",
    "            df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])\n",
    "\n",
    "            display(HTML(df.to_html(render_links=True, escape=False)))\n",
    "        return page_answers,full_abs_list,ranked_aswers,pandasData\n",
    "\n",
    "    return displayResults(hit_dictionary, answers, query, display_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [05:58<00:00, 11.19s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# depth or breadth\n",
    "# EVIDENCE_RETRIEVAL = \"breadth\"\n",
    "EVIDENCE_RETRIEVAL = \"depth\"\n",
    "\n",
    "EVAL = True\n",
    "\n",
    "# with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#     f.write('{\"id\": \"\", \"predicted_label\": \"\", \"predicted_evidence\": \"\"}\\n')\n",
    "\n",
    "# if os.path.isfile(output_path):\n",
    "#     os.remove(output_path)\n",
    "    \n",
    "# if EVAL and os.path.isfile(evaluation_path):\n",
    "#     os.remove(evaluation_path)\n",
    "    \n",
    "from tqdm import tqdm\n",
    "for data in tqdm(dataset[19:51]):\n",
    "    \n",
    "    claim = data[\"claim\"]\n",
    "    \n",
    "    if not claim:\n",
    "        continue\n",
    "        \n",
    "#     print(claim)\n",
    "    \n",
    "    hit_stats, hit_dictionary = get_relevant_pages(claim, N_RESULTS=10)\n",
    "    \n",
    "#     hit_dictionary\n",
    "#     break\n",
    "    \n",
    "    page_answers, full_abs_list, ranked_aswers, pandasData = get_answers(claim, hit_dictionary, display_table=False)\n",
    "#     print(page_answers)\n",
    "\n",
    "#     print(pandasData)\n",
    "#     continue\n",
    "\n",
    "    entities = get_entities(claim)\n",
    "    people = get_people(claim)\n",
    "\n",
    "    max_cell_evidence = 25\n",
    "    \n",
    "    cell_evidences = []\n",
    "    for row in pandasData:\n",
    "        if row[2] < 0.5:\n",
    "            continue\n",
    "            \n",
    "        page = row[3]\n",
    "        if len(cell_evidences) >= max_cell_evidence:\n",
    "            break\n",
    "            \n",
    "        cells = get_relevant_cells(page, [entity for entity in entities if entity not in people])\n",
    "        \n",
    "        if not cells:\n",
    "            continue\n",
    "        \n",
    "        cells = cells[0:min([len(cells),(max_cell_evidence-len(cell_evidences))])]\n",
    "        \n",
    "        for cell in cells:\n",
    "            cell_evidences.append([page, cell.split(\"_\", 1)[0], cell.split(\"_\", 1)[1]])\n",
    "        \n",
    "#         cell_evidences.extend(cells[0:min([len(cells),(max_cell_evidence-len(cell_evidences))])])\n",
    "        \n",
    "    cell_evidences = cell_evidences[0:max_cell_evidence] # Just to be sure\n",
    "    \n",
    "#     print(cell_evidences)\n",
    "\n",
    "    evidence_probs, verdict = CheckEntailmentNeturalorContradict(ranked_aswers,claim,pandasData)\n",
    "\n",
    "    if verdict == \"False\":\n",
    "        predicted_label = \"REFUTES\"\n",
    "    elif verdict == \"True\":\n",
    "        predicted_label = \"SUPPORTS\"\n",
    "    else:\n",
    "        predicted_label = \"NOT ENOUGH INFO\"\n",
    "        \n",
    "#     entities = get_entities(claim)\n",
    "    people = get_people(claim)\n",
    "    \n",
    "    for row_id, row in enumerate(pandasData):\n",
    "        title = row[3]\n",
    "        for person in people:\n",
    "            if title in person or person in title:\n",
    "                pandasData[row_id][2] = pandasData[row_id][2] * 2\n",
    "                break\n",
    "    \n",
    "    \n",
    "    pages_by_confidence = [row[0] for row in pandasData]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(pandasData[0][2])\n",
    "#     print(pages_by_confidence)\n",
    "#     print(page_answers[\"Abraham Annan\"])\n",
    "    \n",
    "    evidences = []\n",
    "    \n",
    "    if EVIDENCE_RETRIEVAL == \"depth\":\n",
    "        for page in pages_by_confidence:\n",
    "            page = get_canonical_title(page)\n",
    "            idx = page_answers[page]\n",
    "            for element in idx:\n",
    "                if len(evidences) < 5:\n",
    "                    element_type, element_id = element.split(\"_\", 1)\n",
    "                    evidence = [page, element_type, element_id]\n",
    "#                     if len(evidences) == 0:\n",
    "#                         print(evidence)\n",
    "                    if evidence not in evidences:\n",
    "                        evidences.append(evidence)\n",
    "                else:\n",
    "                    break\n",
    "    else:\n",
    "        while len(evidences) < 5:\n",
    "            for page in pages_by_confidence:\n",
    "                page = get_canonical_title(page)\n",
    "                idx = page_answers[page]\n",
    "                for element in idx:\n",
    "                    if len(evidences) < 5:\n",
    "                        element_type, element_id = element.split(\"_\", 1)\n",
    "                        evidence = [page, element_type, element_id]\n",
    "                        if evidence not in evidences:\n",
    "                            evidences.append(evidence)\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        break\n",
    "        \n",
    "    \n",
    "    evidences.extend(cell_evidences) # Cell evidences are included\n",
    "    \n",
    "    output = {\"id\": data[\"id\"], \"predicted_label\": predicted_label, \"predicted_evidence\": evidences}\n",
    "    \n",
    "    with open(output_path, 'a', encoding='utf-8') as f:\n",
    "        json.dump(output, f, ensure_ascii=False, indent=None)\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "    \n",
    "    if EVAL and data[\"label\"] and data[\"evidence\"]:\n",
    "\n",
    "        evaluation = {\"id\": data[\"id\"], \"claim\": data[\"claim\"], \"label\": data[\"label\"], \"predicted_label\": predicted_label, \n",
    "                      \"evidence\": data[\"evidence\"], \"predicted_evidence\": []}\n",
    "        \n",
    "        for evidence in evidences:\n",
    "            evidence_str = evidence[0] + \"_\" + evidence[1] + \"_\" + evidence[2]\n",
    "            evaluation[\"predicted_evidence\"].append(evidence_str)\n",
    "\n",
    "        with open(evaluation_path, 'a', encoding='utf-8') as f:\n",
    "            json.dump(evaluation, f, ensure_ascii=False, indent=None)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "#     print(claim)\n",
    "#     print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Jackanory', 'John Laurie', 'the Boat-Deck', 'the Bees'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'John Laurie'}"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_entities(\"In John Laurie's partial television credits, he was part of the Bees on the Boat-Deck in 1939 and Jackanory in 1971.\")\n",
    "# get_people(\"In John Laurie's partial television credits, he was part of the Bees on the Boat-Deck in 1939 and Jackanory in 1971.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_session.execute(\"select * from wiki where id = 'Vtor Oliveira'\").fetchone()\n",
    "\n",
    "json.loads(sql_session.execute(\"select * from wiki where id = 'John Laurie'\").fetchone()[1])[\"table_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_table(table):\n",
    "    rows_processed = []\n",
    "    shift_cells_for_n_rows = 0\n",
    "    for row in table:\n",
    "        for cell_id, cell in enumerate(row):\n",
    "            if shift_cells_for_n_rows == 0 and int(cell[\"row_span\"]) > 1 and int(cell[\"row_span\"]) > shift_cells_for_n_rows:\n",
    "                 shift_cells_for_n_rows = int(cell[\"row_span\"])\n",
    "        if shift_cells_for_n_rows > 0:\n",
    "            shift_cells_for_n_rows -= 1\n",
    "            rows_processed.append([\"\", ].extend([cell[\"value\"] for cell in row]))\n",
    "            print([{\"value\": \"\"}].extend(row))\n",
    "        else:\n",
    "            rows_processed.append(row)\n",
    "            \n",
    "    return rows_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_processed = expand_table(json.loads(sql_session.execute(\"select * from wiki where id = 'John Laurie'\").fetchone()[1])[\"table_1\"][\"table\"])\n",
    "\n",
    "for row in rows_processed:\n",
    "    print(\"\\t\".join([cell[\"value\"] for cell in row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def table_to_2d(table_tag):\n",
    "    rowspans = []  # track pending rowspans\n",
    "    rows = table_tag.find_all('tr')\n",
    "\n",
    "    # first scan, see how many columns we need\n",
    "    colcount = 0\n",
    "    for r, row in enumerate(rows):\n",
    "        cells = row.find_all(['td', 'th'], recursive=False)\n",
    "        # count columns (including spanned).\n",
    "        # add active rowspans from preceding rows\n",
    "        # we *ignore* the colspan value on the last cell, to prevent\n",
    "        # creating 'phantom' columns with no actual cells, only extended\n",
    "        # colspans. This is achieved by hardcoding the last cell width as 1. \n",
    "        # a colspan of 0 means fill until the end but can really only apply\n",
    "        # to the last cell; ignore it elsewhere. \n",
    "        colcount = max(\n",
    "            colcount,\n",
    "            sum(int(c.get('colspan', 1)) or 1 for c in cells[:-1]) + len(cells[-1:]) + len(rowspans))\n",
    "        # update rowspan bookkeeping; 0 is a span to the bottom. \n",
    "        rowspans += [int(c.get('rowspan', 1)) or len(rows) - r for c in cells]\n",
    "        rowspans = [s - 1 for s in rowspans if s > 1]\n",
    "\n",
    "    # it doesn't matter if there are still rowspan numbers 'active'; no extra\n",
    "    # rows to show in the table means the larger than 1 rowspan numbers in the\n",
    "    # last table row are ignored.\n",
    "\n",
    "    # build an empty matrix for all possible cells\n",
    "    table = [[None] * colcount for row in rows]\n",
    "\n",
    "    # fill matrix from row data\n",
    "    rowspans = {}  # track pending rowspans, column number mapping to count\n",
    "    for row, row_elem in enumerate(rows):\n",
    "        span_offset = 0  # how many columns are skipped due to row and colspans \n",
    "        for col, cell in enumerate(row_elem.find_all(['td', 'th'], recursive=False)):\n",
    "            # adjust for preceding row and colspans\n",
    "            col += span_offset\n",
    "            while rowspans.get(col, 0):\n",
    "                span_offset += 1\n",
    "                col += 1\n",
    "\n",
    "            # fill table data\n",
    "            rowspan = rowspans[col] = int(cell.get('rowspan', 1)) or len(rows) - row\n",
    "            colspan = int(cell.get('colspan', 1)) or colcount - col\n",
    "            # next column is offset by the colspan\n",
    "            span_offset += colspan - 1\n",
    "            value = cell.get_text()\n",
    "            for drow, dcol in product(range(rowspan), range(colspan)):\n",
    "                try:\n",
    "                    table[row + drow][col + dcol] = value\n",
    "                    rowspans[col + dcol] = rowspan\n",
    "                except IndexError:\n",
    "                    # rowspan or colspan outside the confines of the table\n",
    "                    pass\n",
    "\n",
    "        # update rowspan bookkeeping\n",
    "        rowspans = {c: s - 1 for c, s in rowspans.items() if s > 1}\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_table(table):\n",
    "\n",
    "    table_processed = []\n",
    "\n",
    "    for row in table:\n",
    "        table_processed.append([cell[\"value\"] for cell in row])\n",
    "        \n",
    "    return table_processed\n",
    "\n",
    "def get_html_table(table, remove_links):\n",
    "    html_table = \"<table>\"\n",
    "    for row in table:\n",
    "        html_table += \"<tr>\"\n",
    "        for cell in row:\n",
    "            if remove_links:\n",
    "                html_table += '<td rowspan=\"{}\" colspan=\"{}\">{}</td>'.format(cell[\"row_span\"], cell[\"column_span\"], re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", cell[\"value\"], flags=re.MULTILINE)) \n",
    "            else:\n",
    "                html_table += '<td rowspan=\"{}\" colspan=\"{}\">{}</td>'.format(cell[\"row_span\"], cell[\"column_span\"], cell[\"value\"])\n",
    "        html_table += \"</tr>\"\n",
    "                \n",
    "    html_table += \"</table>\"\n",
    "    return html_table\n",
    "\n",
    "# json.loads(sql_session.execute(\"select * from wiki where id = 'John Laurie'\").fetchone()[1])[\"table_1\"][\"table\"]\n",
    "\n",
    "def json_table_to_dict(table, remove_links=True):\n",
    "    if \"table\" in table:\n",
    "        table = table[\"table\"]\n",
    "    list_table = table_to_2d(BeautifulSoup(get_html_table(table, remove_links=remove_links)))\n",
    "    if table[0][0][\"is_header\"] != True:\n",
    "        n_col = len(table[0]) # column count\n",
    "        header = range(0, n_col)\n",
    "        list_table = header.extend(list_table)\n",
    "        \n",
    "    dict_table = {}\n",
    "    for col_id, column in enumerate(list_table[0]):\n",
    "        dict_table[column] = [row[col_id] for row in list_table[1:]]\n",
    "    return dict_table\n",
    "\n",
    "def get_numpy_table(table, remove_links=True):\n",
    "    if \"table\" in table:\n",
    "        table = table[\"table\"]\n",
    "    num_col = len(table[0])\n",
    "    num_rows = len(table)\n",
    "    data_x =np.empty(shape=(num_rows,num_col), dtype=object )\n",
    "\n",
    "    for elements in table:\n",
    "        for element in elements:\n",
    "            id = element[\"id\"].split(\"_\")\n",
    "            for j in range(int(element[\"column_span\"])):\n",
    "                for i in range(int(element[\"row_span\"])):\n",
    "                    k=0\n",
    "                    while (data_x[int(id[-2])+i][int(id[-1])+j+k] !=None):\n",
    "                        if ((int(id[-1])+j+k+1)<num_col):\n",
    "                            k = k+1\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    if remove_links:\n",
    "                        data_x[int(id[-2])+i][int(id[-1])+j+k] = re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", element[\"value\"], flags=re.MULTILINE)\n",
    "                    else:\n",
    "                        data_x[int(id[-2])+i][int(id[-1])+j+k] = element[\"value\"]                                                  \n",
    "                    # print(element[\"value\"])\n",
    "\n",
    "        # print(element[\"value\"])\n",
    "    return data_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nvidia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-479-4a6eb4b7a6dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnvidia\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msmi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nvidia' is not defined"
     ]
    }
   ],
   "source": [
    "# json_table_to_dict(json.loads(sql_session.execute(\"select * from wiki where id = 'John Laurie'\").fetchone()[1])[\"table_1\"][\"table\"])\n",
    "\n",
    "#get_numpy_table(json.loads(sql_session.execute(\"select * from wiki where id = 'John Laurie'\").fetchone()[1])[\"table_1\"][\"table\"], remove_links=True)\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.1+cu112.html\n",
      "Requirement already satisfied: torch-scatter in /home/titanx/anaconda3/lib/python3.8/site-packages (2.0.7)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/home/titanx/anaconda3/lib/python3.8/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c106detail12infer_schema20make_function_schemaENS_8ArrayRefINS1_11ArgumentDefEEES4_",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-480-2b82e1626abe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.1+cu112.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_scatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch_scatter/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'_version'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_scatter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_segment_csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_segment_coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n\u001b[0m\u001b[1;32m     14\u001b[0m             f'{library}_{suffix}', [osp.dirname(__file__)]).origin)\n\u001b[1;32m     15\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: /home/titanx/anaconda3/lib/python3.8/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c106detail12infer_schema20make_function_schemaENS_8ArrayRefINS1_11ArgumentDefEEES4_"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.1+cu111.html\n",
    "\n",
    "import torch_scatter\n",
    "\n",
    "!pip install transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTableQuestionAnswering\n",
    "\n",
    "  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForTableQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TapasTokenizer, TapasForQuestionAnswering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = table_processed\n",
    "queries = [\"What is the occupation of John Laurine\"]\n",
    "table = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in data.items() ])).replace(np.nan,' ')\n",
    "inputs = tokenizer(table=table, queries=queries, padding='max_length', return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(\n",
    "         inputs,\n",
    "         outputs.logits.detach(),\n",
    "         outputs.logits_aggregation.detach()\n",
    ")\n",
    "# let's print out the results:\n",
    "id2aggregation = {0: \"NONE\", 1: \"SUM\", 2: \"AVERAGE\", 3:\"COUNT\"}\n",
    "aggregation_predictions_string = [id2aggregation[x] for x in predicted_aggregation_indices]\n",
    "answers = []\n",
    "for coordinates in predicted_answer_coordinates:\n",
    "  if len(coordinates) == 1:\n",
    "     # only a single cell:\n",
    "     answers.append(table.iat[coordinates[0]])\n",
    "  else:\n",
    "     # multiple cells\n",
    "     cell_values = []\n",
    "     for coordinate in coordinates:\n",
    "        cell_values.append(table.iat[coordinate])\n",
    "     answers.append(\", \".join(cell_values))\n",
    "display(table)\n",
    "print(\"\")\n",
    "for query, answer, predicted_agg in zip(queries, answers, aggregation_predictions_string):\n",
    "   print(query)\n",
    "   if predicted_agg == \"NONE\":\n",
    "     print(\"Predicted answer: \" + answer)\n",
    "   else:\n",
    "     print(\"Predicted answer: \" + predicted_agg + \" > \" + answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Year': ['1930',\n",
       "  '1934',\n",
       "  '1935',\n",
       "  '1935',\n",
       "  '1935',\n",
       "  '1936',\n",
       "  '1936',\n",
       "  '1936',\n",
       "  '1937',\n",
       "  '1937',\n",
       "  '1937',\n",
       "  '1937',\n",
       "  '1937',\n",
       "  '1938',\n",
       "  '1938',\n",
       "  '1938',\n",
       "  '1938',\n",
       "  '1938',\n",
       "  '1938',\n",
       "  '1939',\n",
       "  '1939',\n",
       "  '1939',\n",
       "  '1939',\n",
       "  '1940',\n",
       "  '1940',\n",
       "  '1940',\n",
       "  '1941',\n",
       "  '1941',\n",
       "  '1941',\n",
       "  '1942',\n",
       "  '1943',\n",
       "  '1943',\n",
       "  '1943',\n",
       "  '1943',\n",
       "  '1943',\n",
       "  '1944',\n",
       "  '1944',\n",
       "  '1944',\n",
       "  '1944',\n",
       "  '1944',\n",
       "  '1945',\n",
       "  '1945',\n",
       "  '1945',\n",
       "  '1945',\n",
       "  '1945',\n",
       "  '1945',\n",
       "  '1946',\n",
       "  '1946',\n",
       "  '1946',\n",
       "  '1946',\n",
       "  '1947',\n",
       "  '1947',\n",
       "  '1947',\n",
       "  '1947',\n",
       "  '1948',\n",
       "  '1948',\n",
       "  '1949',\n",
       "  '1950',\n",
       "  '1950',\n",
       "  '1950',\n",
       "  '1950',\n",
       "  '1951',\n",
       "  '1951',\n",
       "  '1951',\n",
       "  '1951',\n",
       "  '1952',\n",
       "  '1952',\n",
       "  '1952',\n",
       "  '1952',\n",
       "  '1953',\n",
       "  '1953',\n",
       "  '1953',\n",
       "  '1953',\n",
       "  '1953',\n",
       "  '1953',\n",
       "  '1953',\n",
       "  '1953',\n",
       "  '1954',\n",
       "  '1954',\n",
       "  '1954',\n",
       "  '1954',\n",
       "  '1954',\n",
       "  '1955',\n",
       "  '1956',\n",
       "  '1956',\n",
       "  '1957',\n",
       "  '1957',\n",
       "  '1958',\n",
       "  '1958',\n",
       "  '1960',\n",
       "  '1961',\n",
       "  '1961',\n",
       "  '1963',\n",
       "  '1963',\n",
       "  '1964',\n",
       "  '1966',\n",
       "  '1967',\n",
       "  '1970',\n",
       "  '1971',\n",
       "  '1971',\n",
       "  '1974',\n",
       "  '1975',\n",
       "  '1976',\n",
       "  '1979'],\n",
       " 'Title': ['[[Juno_and_the_Paycock_(film)|Juno and the Paycock]]',\n",
       "  '[[Red_Ensign_(film)|Red Ensign]]',\n",
       "  '[[The_39_Steps_(1935_film)|The 39 Steps]]',\n",
       "  'Her Last Affaire',\n",
       "  '[[Tudor_Rose_(film)|Tudor Rose]]',\n",
       "  '[[Born_That_Way_(film)|Born That Way]]',\n",
       "  '[[East_Meets_West_(1936_film)|East Meets West]]',\n",
       "  '[[As_You_Like_It_(1936_film)|As You Like It]]',\n",
       "  '[[The_Windmill_(1937_film)|The Windmill]]',\n",
       "  '[[Farewell_Again|Farewell Again]]',\n",
       "  '[[Jericho_(1937_film)|Jericho]]',\n",
       "  '[[The_Edge_of_the_World|The Edge of the World]]',\n",
       "  '[[There_Was_a_Young_Man|There Was a Young Man]]',\n",
       "  'The Duchess of Malfi',\n",
       "  '[[The_Claydon_Treasure_Mystery|The Claydon Treasure Mystery]]',\n",
       "  'White Secret',\n",
       "  '[[A_Royal_Divorce_(1938_film)|A Royal Divorce]]',\n",
       "  'The Last Voyage of Captain Grant',\n",
       "  '[[The_Ware_Case_(1938_film)|The Ware Case]]',\n",
       "  'Mary Rose',\n",
       "  'Bees on the Boat-Deck',\n",
       "  '[[Q_Planes|Q Planes]]',\n",
       "  '[[The_Four_Feathers_(1939_film)|The Four Feathers]]',\n",
       "  '[[Laugh_It_Off_(1940_film)|Laugh It Off]]',\n",
       "  '[[Convoy_(1940_film)|Convoy]]',\n",
       "  '[[Sailors_Three|Sailors Three]]',\n",
       "  \"The Ghost of St. Michael's\",\n",
       "  \"[[Old_Mother_Riley's_Ghosts|Old Mother Riley's Ghosts]]\",\n",
       "  '[[Dangerous_Moonlight|Dangerous Moonlight]]',\n",
       "  '[[Ships_with_Wings|Ships with Wings]]',\n",
       "  'The Gentle Sex',\n",
       "  '[[The_Life_and_Death_of_Colonel_Blimp|The Life and Death of Colonel Blimp]]',\n",
       "  '[[The_Demi-Paradise|The Demi-Paradise]]',\n",
       "  '[[The_Lamp_Still_Burns|The Lamp Still Burns]]',\n",
       "  '[[The_New_Lot|The New Lot]]',\n",
       "  '[[Fanny_by_Gaslight_(film)|Fanny by Gaslight]]',\n",
       "  '[[The_Way_Ahead|The Way Ahead]]',\n",
       "  '[[Medal_for_the_General|Medal for the General]]',\n",
       "  '[[Henry_V_(1944_film)|Henry V]]',\n",
       "  'Men of Rochdale',\n",
       "  '[[The_World_Owes_Me_a_Living|The World Owes Me a Living]]',\n",
       "  '[[Great_Day_(1945_film)|Great Day]]',\n",
       "  '[[The_Agitator|The Agitator]]',\n",
       "  \"[[I_Know_Where_I'm_Going!|I Know Where I'm Going!]]\",\n",
       "  '[[Caesar_and_Cleopatra_(film)|Caesar and Cleopatra]]',\n",
       "  'Read All About It',\n",
       "  '[[Gaiety_George|Gaiety George]]',\n",
       "  'Jeannie',\n",
       "  'Two Gentlemen of Soho',\n",
       "  '[[School_for_Secrets|School for Secrets]]',\n",
       "  '[[The_Brothers_(1947_film)|The Brothers]]',\n",
       "  '[[Jassy_(film)|Jassy]]',\n",
       "  '[[Uncle_Silas_(film)|Uncle Silas]]',\n",
       "  '[[Mine_Own_Executioner|Mine Own Executioner]]',\n",
       "  '[[Hamlet_(1948_film)|Hamlet]]',\n",
       "  '[[Bonnie_Prince_Charlie_(1948_film)|Bonnie Prince Charlie]]',\n",
       "  '[[Floodtide|Floodtide]]',\n",
       "  '[[Madeleine_(1950_film)|Madeleine]]',\n",
       "  '[[Treasure_Island_(1950_film)|Treasure Island]]',\n",
       "  '[[Trio_(film)|Trio]]',\n",
       "  '[[No_Trace|No Trace]]',\n",
       "  'Pandora and the Flying Dutchman',\n",
       "  '[[Happy_Go_Lovely|Happy Go Lovely]]',\n",
       "  '[[Laughter_in_Paradise|Laughter in Paradise]]',\n",
       "  '[[Encore_(1951_film)|Encore]]',\n",
       "  '[[Saturday_Island|Saturday Island]]',\n",
       "  '[[Tread_Softly_(1952_film)|Tread Softly]]',\n",
       "  'Too Many Detectives',\n",
       "  'Potter of the Yard',\n",
       "  '[[The_Great_Game_(1953_film)|The Great Game]]',\n",
       "  \"Captain Brassbound's Conversion\",\n",
       "  'Henry V',\n",
       "  '[[The_Fake_(1953_film)|The Fake]]',\n",
       "  '[[Johnny_on_the_Run|Johnny on the Run]]',\n",
       "  '[[Strange_Stories_(film)|Strange Stories]]',\n",
       "  'Mr. Beamish Goes South',\n",
       "  '[[Love_in_Pawn|Love in Pawn]]',\n",
       "  \"[[Hobson's_Choice_(1954_film)|Hobson's Choice]]\",\n",
       "  'Calling Scotland Yard: The Sable Scarf',\n",
       "  '[[Devil_Girl_from_Mars|Devil Girl from Mars]]',\n",
       "  '[[The_Black_Knight_(film)|The Black Knight]]',\n",
       "  'Destination Milan',\n",
       "  '[[Richard_III_(1955_film)|Richard III]]',\n",
       "  'Festival Fever',\n",
       "  'A Day of Grace',\n",
       "  'Murder Reported',\n",
       "  \"[[Campbell's_Kingdom|Campbell's Kingdom]]\",\n",
       "  '[[Next_to_No_Time|Next to No Time]]',\n",
       "  '[[Rockets_Galore!|Rockets Galore!]]',\n",
       "  '[[Kidnapped_(1960_film)|Kidnapped]]',\n",
       "  \"[[Don't_Bother_to_Knock|Don't Bother to Knock]]\",\n",
       "  'One Way Pendulum',\n",
       "  '[[Siege_of_the_Saxons|Siege of the Saxons]]',\n",
       "  '[[Ladies_Who_Do|Ladies Who Do]]',\n",
       "  'Eagle Rock',\n",
       "  '[[The_Reptile|The Reptile]]',\n",
       "  '[[Mister_Ten_Per_Cent|Mister Ten Per Cent]]',\n",
       "  'Step Laughing Into the Grave',\n",
       "  \"[[Dad's_Army_(1971_film)|Dad's Army]]\",\n",
       "  '[[The_Abominable_Dr._Phibes|The Abominable Dr. Phibes]]',\n",
       "  \"Charles Dickens' World of Christmas\",\n",
       "  '[[One_of_Our_Dinosaurs_Is_Missing|One of Our Dinosaurs Is Missing]]',\n",
       "  'Crime Casebook',\n",
       "  '[[The_Prisoner_of_Zenda_(1979_film)|The Prisoner of Zenda]]'],\n",
       " 'Role': ['Johnny Boyle',\n",
       "  'Forsyth',\n",
       "  'John the crofter',\n",
       "  'Robb',\n",
       "  'John Knox',\n",
       "  'Mc Tavish',\n",
       "  'Dr. Fergusson',\n",
       "  'Oliver',\n",
       "  'Mons. Coutard',\n",
       "  'Private McAllister',\n",
       "  'Hassan',\n",
       "  'Peter Manson',\n",
       "  'Stranger',\n",
       "  'Ferdinand of Aragon',\n",
       "  'Wilson - the Valet',\n",
       "  'MacDonald',\n",
       "  'Joseph Bonaparte',\n",
       "  'Captain Grant',\n",
       "  'Henson, the gamekeeper',\n",
       "  'Cameron',\n",
       "  'Gaster',\n",
       "  'Newspaper Editor',\n",
       "  'The Khalifa',\n",
       "  'Jock',\n",
       "  'Gates',\n",
       "  'McNab',\n",
       "  'Jamie',\n",
       "  'McAdam',\n",
       "  'Wing Commander',\n",
       "  'Lt. Comdr. Reid',\n",
       "  'Alexander Balfour, Scots corporal',\n",
       "  'Murdoch',\n",
       "  'British Sailor',\n",
       "  'Mr. Hervey',\n",
       "  'Harry Fyfe',\n",
       "  'William Hopwood',\n",
       "  'Pvt. Luke',\n",
       "  'McNab',\n",
       "  'Jamy',\n",
       "  'Mr. Ferguson',\n",
       "  'Matthews',\n",
       "  'Scottish sergeant',\n",
       "  'Tom Tetley',\n",
       "  'John Campbell',\n",
       "  '1st. Auxiliary Sentinel',\n",
       "  'John',\n",
       "  'MacTavish',\n",
       "  'Father',\n",
       "  'Sneak',\n",
       "  'Dr. Jock McVitie',\n",
       "  'Dugald McLeod / Alistair MacDonald',\n",
       "  'Tom Woodroofe',\n",
       "  'Giles',\n",
       "  'Dr. James Garsten',\n",
       "  'Francisco',\n",
       "  'Blind Jamie',\n",
       "  'Joe Drummond',\n",
       "  'Scots Divine',\n",
       "  'Blind Pew',\n",
       "  'Mr. Campbell',\n",
       "  'Inspector MacDougall',\n",
       "  'Angus',\n",
       "  'Jonskill',\n",
       "  'Gordon Webb',\n",
       "  'Andrews, Engineer',\n",
       "  'Grimshaw',\n",
       "  'Angus McDonald',\n",
       "  'Edward Potter',\n",
       "  'Edward Potter',\n",
       "  'Mac Wells',\n",
       "  'Rankin',\n",
       "  'Pistol',\n",
       "  'Henry Mason',\n",
       "  'Policeman',\n",
       "  'Mr. Bartleby',\n",
       "  'Edward Potter',\n",
       "  'McCutcheon',\n",
       "  'Dr. McFarlane',\n",
       "  '',\n",
       "  '\"Jamie\" Jamieson',\n",
       "  'James, the servant',\n",
       "  'Walter McHarry',\n",
       "  'Lovel',\n",
       "  \"Annie's father\",\n",
       "  'Uncle Henry',\n",
       "  'Mac North - Editor',\n",
       "  'Mac',\n",
       "  'Abercrombie, Scottish Director',\n",
       "  'Capt. MacKechnie',\n",
       "  'Ebenezer Balfour',\n",
       "  'Taxi Driver',\n",
       "  'Judge',\n",
       "  '[[Merlin|Merlin]]',\n",
       "  'Dr. MacGregor',\n",
       "  'Mr. McTavish',\n",
       "  'Mad Peter',\n",
       "  'The Scotsman',\n",
       "  '',\n",
       "  'Private Frazer',\n",
       "  'Darrow',\n",
       "  '',\n",
       "  'Jock',\n",
       "  'George Winterman / Sellens',\n",
       "  'Archbishop'],\n",
       " 'Notes': ['',\n",
       "  'Uncredited',\n",
       "  '',\n",
       "  '',\n",
       "  'Uncredited',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Also known as Dark Sands',\n",
       "  '',\n",
       "  '',\n",
       "  'TV',\n",
       "  'Uncredited',\n",
       "  'TV',\n",
       "  '',\n",
       "  'TV',\n",
       "  '',\n",
       "  'TV',\n",
       "  'TV',\n",
       "  'Uncredited',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Short, Uncredited',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Short',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Short, Uncredited',\n",
       "  '',\n",
       "  'TV',\n",
       "  'TV',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Uncredited',\n",
       "  '',\n",
       "  '(segment \"Sanatorium\")',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '(segment \"Winter Cruise\")',\n",
       "  '',\n",
       "  '',\n",
       "  'Short',\n",
       "  'Short',\n",
       "  '',\n",
       "  'TV',\n",
       "  'TV',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Short',\n",
       "  '',\n",
       "  '',\n",
       "  'Short',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'TV',\n",
       "  'Short',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Uncredited',\n",
       "  '',\n",
       "  '',\n",
       "  'TV',\n",
       "  '',\n",
       "  '',\n",
       "  'Voice',\n",
       "  '',\n",
       "  '',\n",
       "  'TV',\n",
       "  '',\n",
       "  '',\n",
       "  'TV',\n",
       "  '',\n",
       "  'Short',\n",
       "  '(final film role)']}"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_table = table_to_2d(BeautifulSoup(get_html_table(json.loads(sql_session.execute(\"select * from wiki where id = 'John Laurie'\").fetchone()[1])[\"table_1\"][\"table\"])))\n",
    "\n",
    "dict_table = {}\n",
    "\n",
    "for row_id, row in enumerate(list_table):\n",
    "    if row_id == 0:\n",
    "        for col_id, column in enumerate(row):\n",
    "            dict_table[column] = [row[col_id] for row in list_table[1:]]\n",
    "    break\n",
    "    \n",
    "dict_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Year ', 'Title ', 'Role ', 'Notes '],\n",
       " ['1930 ', 'Juno and the Paycock ', 'Johnny Boyle ', ' '],\n",
       " ['1934 ', 'Red Ensign ', 'Forsyth ', 'Uncredited '],\n",
       " ['1935 ', 'The 39 Steps ', 'John the crofter ', ' '],\n",
       " ['1935 ', 'Her Last Affaire ', 'Robb ', ' '],\n",
       " ['1935 ', 'Tudor Rose ', 'John Knox ', 'Uncredited '],\n",
       " ['1936 ', 'Born That Way ', 'Mc Tavish ', ' '],\n",
       " ['1936 ', 'East Meets West ', 'Dr. Fergusson ', ' '],\n",
       " ['1936 ', 'As You Like It ', 'Oliver ', ' '],\n",
       " ['1937 ', 'The Windmill ', 'Mons. Coutard ', ' '],\n",
       " ['1937 ', 'Farewell Again ', 'Private McAllister ', ' '],\n",
       " ['1937 ', 'Jericho ', 'Hassan ', 'Also known as Dark Sands '],\n",
       " ['1937 ', 'The Edge of the World ', 'Peter Manson ', ' '],\n",
       " ['1937 ', 'There Was a Young Man ', 'Stranger ', ' '],\n",
       " ['1938 ', 'The Duchess of Malfi ', 'Ferdinand of Aragon ', 'TV '],\n",
       " ['1938 ',\n",
       "  'The Claydon Treasure Mystery ',\n",
       "  'Wilson - the Valet ',\n",
       "  'Uncredited '],\n",
       " ['1938 ', 'White Secret ', 'MacDonald ', 'TV '],\n",
       " ['1938 ', 'A Royal Divorce ', 'Joseph Bonaparte ', ' '],\n",
       " ['1938 ', 'The Last Voyage of Captain Grant ', 'Captain Grant ', 'TV '],\n",
       " ['1938 ', 'The Ware Case ', 'Henson, the gamekeeper ', ' '],\n",
       " ['1939 ', 'Mary Rose ', 'Cameron ', 'TV '],\n",
       " ['1939 ', 'Bees on the Boat-Deck ', 'Gaster ', 'TV '],\n",
       " ['1939 ', 'Q Planes ', 'Newspaper Editor ', 'Uncredited '],\n",
       " ['1939 ', 'The Four Feathers ', 'The Khalifa ', ' '],\n",
       " ['1940 ', 'Laugh It Off ', 'Jock ', ' '],\n",
       " ['1940 ', 'Convoy ', 'Gates ', ' '],\n",
       " ['1940 ', 'Sailors Three ', 'McNab ', ' '],\n",
       " ['1941 ', \"The Ghost of St. Michael's \", 'Jamie ', ' '],\n",
       " ['1941 ', \"Old Mother Riley's Ghosts \", 'McAdam ', ' '],\n",
       " ['1941 ', 'Dangerous Moonlight ', 'Wing Commander ', ' '],\n",
       " ['1942 ', 'Ships with Wings ', 'Lt. Comdr. Reid ', ' '],\n",
       " ['1943 ', 'The Gentle Sex ', 'Alexander Balfour, Scots corporal ', ' '],\n",
       " ['1943 ', 'The Life and Death of Colonel Blimp ', 'Murdoch ', ' '],\n",
       " ['1943 ', 'The Demi-Paradise ', 'British Sailor ', ' '],\n",
       " ['1943 ', 'The Lamp Still Burns ', 'Mr. Hervey ', ' '],\n",
       " ['1943 ', 'The New Lot ', 'Harry Fyfe ', 'Short, Uncredited '],\n",
       " ['1944 ', 'Fanny by Gaslight ', 'William Hopwood ', ' '],\n",
       " ['1944 ', 'The Way Ahead ', 'Pvt. Luke ', ' '],\n",
       " ['1944 ', 'Medal for the General ', 'McNab ', ' '],\n",
       " ['1944 ', 'Henry V ', 'Jamy ', ' '],\n",
       " ['1944 ', 'Men of Rochdale ', 'Mr. Ferguson ', 'Short '],\n",
       " ['1945 ', 'The World Owes Me a Living ', 'Matthews ', ' '],\n",
       " ['1945 ', 'Great Day ', 'Scottish sergeant ', ' '],\n",
       " ['1945 ', 'The Agitator ', 'Tom Tetley ', ' '],\n",
       " ['1945 ', \"I Know Where I'm Going! \", 'John Campbell ', ' '],\n",
       " ['1945 ', 'Caesar and Cleopatra ', '1st. Auxiliary Sentinel ', ' '],\n",
       " ['1945 ', 'Read All About It ', 'John ', 'Short, Uncredited '],\n",
       " ['1946 ', 'Gaiety George ', 'MacTavish ', ' '],\n",
       " ['1946 ', 'Jeannie ', 'Father ', 'TV '],\n",
       " ['1946 ', 'Two Gentlemen of Soho ', 'Sneak ', 'TV '],\n",
       " ['1946 ', 'School for Secrets ', 'Dr. Jock McVitie ', ' '],\n",
       " ['1947 ', 'The Brothers ', 'Dugald McLeod / Alistair MacDonald ', ' '],\n",
       " ['1947 ', 'Jassy ', 'Tom Woodroofe ', ' '],\n",
       " ['1947 ', 'Uncle Silas ', 'Giles ', ' '],\n",
       " ['1947 ', 'Mine Own Executioner ', 'Dr. James Garsten ', ' '],\n",
       " ['1948 ', 'Hamlet ', 'Francisco ', ' '],\n",
       " ['1948 ', 'Bonnie Prince Charlie ', 'Blind Jamie ', ' '],\n",
       " ['1949 ', 'Floodtide ', 'Joe Drummond ', ' '],\n",
       " ['1950 ', 'Madeleine ', 'Scots Divine ', 'Uncredited '],\n",
       " ['1950 ', 'Treasure Island ', 'Blind Pew ', ' '],\n",
       " ['1950 ', 'Trio ', 'Mr. Campbell ', '(segment \"Sanatorium\") '],\n",
       " ['1950 ', 'No Trace ', 'Inspector MacDougall ', ' '],\n",
       " ['1951 ', 'Pandora and the Flying Dutchman ', 'Angus ', ' '],\n",
       " ['1951 ', 'Happy Go Lovely ', 'Jonskill ', ' '],\n",
       " ['1951 ', 'Laughter in Paradise ', 'Gordon Webb ', ' '],\n",
       " ['1951 ', 'Encore ', 'Andrews, Engineer ', '(segment \"Winter Cruise\") '],\n",
       " ['1952 ', 'Saturday Island ', 'Grimshaw ', ' '],\n",
       " ['1952 ', 'Tread Softly ', 'Angus McDonald ', ' '],\n",
       " ['1952 ', 'Too Many Detectives ', 'Edward Potter ', 'Short '],\n",
       " ['1952 ', 'Potter of the Yard ', 'Edward Potter ', 'Short '],\n",
       " ['1953 ', 'The Great Game ', 'Mac Wells ', ' '],\n",
       " ['1953 ', \"Captain Brassbound's Conversion \", 'Rankin ', 'TV '],\n",
       " ['1953 ', 'Henry V ', 'Pistol ', 'TV '],\n",
       " ['1953 ', 'The Fake ', 'Henry Mason ', ' '],\n",
       " ['1953 ', 'Johnny on the Run ', 'Policeman ', ' '],\n",
       " ['1953 ', 'Strange Stories ', 'Mr. Bartleby ', ' '],\n",
       " ['1953 ', 'Mr. Beamish Goes South ', 'Edward Potter ', 'Short '],\n",
       " ['1953 ', 'Love in Pawn ', 'McCutcheon ', ' '],\n",
       " ['1954 ', \"Hobson's Choice \", 'Dr. McFarlane ', ' '],\n",
       " ['1954 ', 'Calling Scotland Yard: The Sable Scarf ', ' ', 'Short '],\n",
       " ['1954 ', 'Devil Girl from Mars ', '\"Jamie\" Jamieson ', ' '],\n",
       " ['1954 ', 'The Black Knight ', 'James, the servant ', ' '],\n",
       " ['1954 ', 'Destination Milan ', 'Walter McHarry ', ' '],\n",
       " ['1955 ', 'Richard III ', 'Lovel ', ' '],\n",
       " ['1956 ', 'Festival Fever ', \"Annie's father \", 'TV '],\n",
       " ['1956 ', 'A Day of Grace ', 'Uncle Henry ', 'Short '],\n",
       " ['1957 ', 'Murder Reported ', 'Mac North - Editor ', ' '],\n",
       " ['1957 ', \"Campbell's Kingdom \", 'Mac ', ' '],\n",
       " ['1958 ', 'Next to No Time ', 'Abercrombie, Scottish Director ', ' '],\n",
       " ['1958 ', 'Rockets Galore! ', 'Capt. MacKechnie ', 'Uncredited '],\n",
       " ['1960 ', 'Kidnapped ', 'Ebenezer Balfour ', ' '],\n",
       " ['1961 ', \"Don't Bother to Knock \", 'Taxi Driver ', ' '],\n",
       " ['1961 ', 'One Way Pendulum ', 'Judge ', 'TV '],\n",
       " ['1963 ', 'Siege of the Saxons ', 'Merlin ', ' '],\n",
       " ['1963 ', 'Ladies Who Do ', 'Dr. MacGregor ', ' '],\n",
       " ['1964 ', 'Eagle Rock ', 'Mr. McTavish ', 'Voice '],\n",
       " ['1966 ', 'The Reptile ', 'Mad Peter ', ' '],\n",
       " ['1967 ', 'Mister Ten Per Cent ', 'The Scotsman ', ' '],\n",
       " ['1970 ', 'Step Laughing Into the Grave ', ' ', 'TV '],\n",
       " ['1971 ', \"Dad's Army \", 'Private Frazer ', ' '],\n",
       " ['1971 ', 'The Abominable Dr. Phibes ', 'Darrow ', ' '],\n",
       " ['1974 ', \"Charles Dickens' World of Christmas \", ' ', 'TV '],\n",
       " ['1975 ', 'One of Our Dinosaurs Is Missing ', 'Jock ', ' '],\n",
       " ['1976 ', 'Crime Casebook ', 'George Winterman / Sellens ', 'Short '],\n",
       " ['1979 ', 'The Prisoner of Zenda ', 'Archbishop ', '(final film role) ']]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table_to_2d(simplify_table(json.loads(sql_session.execute(\"select * from wiki where id = 'John Laurie'\").fetchone()[1])[\"table_1\"][\"table\"]))\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "table_to_2d(BeautifulSoup('''<table class=\"wikitable sortable jquery-tablesorter\"> <thead><tr> <th class=\"headerSort\" tabindex=\"0\" role=\"columnheader button\" title=\"Sort ascending\">Year </th> <th class=\"headerSort\" tabindex=\"0\" role=\"columnheader button\" title=\"Sort ascending\">Title </th> <th class=\"headerSort\" tabindex=\"0\" role=\"columnheader button\" title=\"Sort ascending\">Role </th> <th class=\"headerSort\" tabindex=\"0\" role=\"columnheader button\" title=\"Sort ascending\">Notes </th></tr></thead><tbody> <tr> <td>1930 </td> <td><i><a href=\"/wiki/Juno_and_the_Paycock_(film)\" title=\"Juno and the Paycock (film)\">Juno and the Paycock</a></i> </td> <td>Johnny Boyle </td> <td> </td></tr> <tr> <td>1934 </td> <td><i><a href=\"/wiki/Red_Ensign_(film)\" title=\"Red Ensign (film)\">Red Ensign</a></i> </td> <td>Forsyth </td> <td>Uncredited </td></tr> <tr> <td rowspan=\"3\">1935 </td> <td><i><a href=\"/wiki/The_39_Steps_(1935_film)\" title=\"The 39 Steps (1935 film)\">The 39 Steps</a></i> </td> <td>John the crofter </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Her_Last_Affaire\" title=\"Her Last Affaire\">Her Last Affaire</a></i> </td> <td>Robb </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Tudor_Rose_(film)\" title=\"Tudor Rose (film)\">Tudor Rose</a></i> </td> <td>John Knox </td> <td>Uncredited </td></tr> <tr> <td rowspan=\"3\">1936 </td> <td><i><a href=\"/wiki/Born_That_Way_(film)\" title=\"Born That Way (film)\">Born That Way</a></i> </td> <td>Mc Tavish </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/East_Meets_West_(1936_film)\" title=\"East Meets West (1936 film)\">East Meets West</a></i> </td> <td>Dr. Fergusson </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/As_You_Like_It_(1936_film)\" title=\"As You Like It (1936 film)\">As You Like It</a></i> </td> <td>Oliver </td> <td> </td></tr> <tr> <td rowspan=\"5\">1937 </td> <td><i><a href=\"/wiki/The_Windmill_(1937_film)\" title=\"The Windmill (1937 film)\">The Windmill</a></i> </td> <td>Mons. Coutard </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Farewell_Again\" title=\"Farewell Again\">Farewell Again</a></i> </td> <td>Private McAllister </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Jericho_(1937_film)\" title=\"Jericho (1937 film)\">Jericho</a></i> </td> <td>Hassan </td> <td>Also known as <i>Dark Sands</i> </td></tr> <tr> <td><i><a href=\"/wiki/The_Edge_of_the_World\" title=\"The Edge of the World\">The Edge of the World</a></i> </td> <td>Peter Manson </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/There_Was_a_Young_Man\" title=\"There Was a Young Man\">There Was a Young Man</a></i> </td> <td>Stranger </td> <td> </td></tr> <tr> <td rowspan=\"6\">1938 </td> <td><i>The Duchess of Malfi</i> </td> <td>Ferdinand of Aragon </td> <td>TV </td></tr> <tr> <td><i><a href=\"/wiki/The_Claydon_Treasure_Mystery\" title=\"The Claydon Treasure Mystery\">The Claydon Treasure Mystery</a></i> </td> <td>Wilson - the Valet </td> <td>Uncredited </td></tr> <tr> <td><i>White Secret</i> </td> <td>MacDonald </td> <td>TV </td></tr> <tr> <td><i><a href=\"/wiki/A_Royal_Divorce_(1938_film)\" title=\"A Royal Divorce (1938 film)\">A Royal Divorce</a></i> </td> <td>Joseph Bonaparte </td> <td> </td></tr> <tr> <td><i>The Last Voyage of Captain Grant</i> </td> <td>Captain Grant </td> <td>TV </td></tr> <tr> <td><i><a href=\"/wiki/The_Ware_Case_(1938_film)\" title=\"The Ware Case (1938 film)\">The Ware Case</a></i> </td> <td>Henson, the gamekeeper </td> <td> </td></tr> <tr> <td rowspan=\"4\">1939 </td> <td><i>Mary Rose</i> </td> <td>Cameron </td> <td>TV </td></tr> <tr> <td><i>Bees on the Boat-Deck</i> </td> <td>Gaster </td> <td>TV </td></tr> <tr> <td><i><a href=\"/wiki/Q_Planes\" title=\"Q Planes\">Q Planes</a></i> </td> <td>Newspaper Editor </td> <td>Uncredited </td></tr> <tr> <td><i><a href=\"/wiki/The_Four_Feathers_(1939_film)\" title=\"The Four Feathers (1939 film)\">The Four Feathers</a></i> </td> <td>The Khalifa </td> <td> </td></tr> <tr> <td rowspan=\"3\">1940 </td> <td><i><a href=\"/wiki/Laugh_It_Off_(1940_film)\" title=\"Laugh It Off (1940 film)\">Laugh It Off</a></i> </td> <td>Jock </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Convoy_(1940_film)\" title=\"Convoy (1940 film)\">Convoy</a></i> </td> <td>Gates </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Sailors_Three\" title=\"Sailors Three\">Sailors Three</a></i> </td> <td>McNab </td> <td> </td></tr> <tr> <td rowspan=\"3\">1941 </td> <td><i><a href=\"/wiki/The_Ghost_of_St._Michael%27s\" title=\"The Ghost of St. Michael's\">The Ghost of St. Michael's</a></i> </td> <td>Jamie </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Old_Mother_Riley%27s_Ghosts\" title=\"Old Mother Riley's Ghosts\">Old Mother Riley's Ghosts</a></i> </td> <td>McAdam </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Dangerous_Moonlight\" title=\"Dangerous Moonlight\">Dangerous Moonlight</a></i> </td> <td>Wing Commander </td> <td> </td></tr> <tr> <td>1942 </td> <td><i><a href=\"/wiki/Ships_with_Wings\" title=\"Ships with Wings\">Ships with Wings</a></i> </td> <td>Lt. Comdr. Reid </td> <td> </td></tr> <tr> <td rowspan=\"5\">1943 </td> <td><i><a href=\"/wiki/The_Gentle_Sex\" title=\"The Gentle Sex\">The Gentle Sex</a></i> </td> <td>Alexander Balfour, Scots corporal </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/The_Life_and_Death_of_Colonel_Blimp\" title=\"The Life and Death of Colonel Blimp\">The Life and Death of Colonel Blimp</a></i> </td> <td>Murdoch </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/The_Demi-Paradise\" title=\"The Demi-Paradise\">The Demi-Paradise</a></i> </td> <td>British Sailor </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/The_Lamp_Still_Burns\" title=\"The Lamp Still Burns\">The Lamp Still Burns</a></i> </td> <td>Mr. Hervey </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/The_New_Lot\" title=\"The New Lot\">The New Lot</a></i> </td> <td>Harry Fyfe </td> <td>Short, Uncredited </td></tr> <tr> <td rowspan=\"5\">1944 </td> <td><i><a href=\"/wiki/Fanny_by_Gaslight_(film)\" title=\"Fanny by Gaslight (film)\">Fanny by Gaslight</a></i> </td> <td>William Hopwood </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/The_Way_Ahead\" title=\"The Way Ahead\">The Way Ahead</a></i> </td> <td>Pvt. Luke </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Medal_for_the_General\" title=\"Medal for the General\">Medal for the General</a></i> </td> <td>McNab </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Henry_V_(1944_film)\" title=\"Henry V (1944 film)\">Henry V</a></i> </td> <td>Jamy </td> <td> </td></tr> <tr> <td><i>Men of Rochdale</i> </td> <td>Mr. Ferguson </td> <td>Short </td></tr> <tr> <td rowspan=\"6\">1945 </td> <td><i><a href=\"/wiki/The_World_Owes_Me_a_Living\" title=\"The World Owes Me a Living\">The World Owes Me a Living</a></i> </td> <td>Matthews </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Great_Day_(1945_film)\" title=\"Great Day (1945 film)\">Great Day</a></i> </td> <td>Scottish sergeant </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/The_Agitator\" title=\"The Agitator\">The Agitator</a></i> </td> <td>Tom Tetley </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/I_Know_Where_I%27m_Going!\" title=\"I Know Where I'm Going!\">I Know Where I'm Going!</a></i> </td> <td>John Campbell </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Caesar_and_Cleopatra_(film)\" title=\"Caesar and Cleopatra (film)\">Caesar and Cleopatra</a></i> </td> <td>1st. Auxiliary Sentinel </td> <td> </td></tr> <tr> <td><i>Read All About It</i> </td> <td>John </td> <td>Short, Uncredited </td></tr> <tr> <td rowspan=\"4\">1946 </td> <td><i><a href=\"/wiki/Gaiety_George\" title=\"Gaiety George\">Gaiety George</a></i> </td> <td>MacTavish </td> <td> </td></tr> <tr> <td><i>Jeannie</i> </td> <td>Father </td> <td>TV </td></tr> <tr> <td><i>Two Gentlemen of Soho</i> </td> <td>Sneak </td> <td>TV </td></tr> <tr> <td><i><a href=\"/wiki/School_for_Secrets\" title=\"School for Secrets\">School for Secrets</a></i> </td> <td>Dr. Jock McVitie </td> <td> </td></tr> <tr> <td rowspan=\"4\">1947 </td> <td><i><a href=\"/wiki/The_Brothers_(1947_film)\" title=\"The Brothers (1947 film)\">The Brothers</a></i> </td> <td>Dugald McLeod / Alistair MacDonald </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Jassy_(film)\" title=\"Jassy (film)\">Jassy</a></i> </td> <td>Tom Woodroofe </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Uncle_Silas_(film)\" title=\"Uncle Silas (film)\">Uncle Silas</a></i> </td> <td>Giles </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Mine_Own_Executioner\" title=\"Mine Own Executioner\">Mine Own Executioner</a></i> </td> <td>Dr. James Garsten </td> <td> </td></tr> <tr> <td rowspan=\"2\">1948 </td> <td><i><a href=\"/wiki/Hamlet_(1948_film)\" title=\"Hamlet (1948 film)\">Hamlet</a></i> </td> <td>Francisco </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Bonnie_Prince_Charlie_(1948_film)\" title=\"Bonnie Prince Charlie (1948 film)\">Bonnie Prince Charlie</a></i> </td> <td>Blind Jamie </td> <td> </td></tr> <tr> <td>1949 </td> <td><i><a href=\"/wiki/Floodtide\" title=\"Floodtide\">Floodtide</a></i> </td> <td>Joe Drummond </td> <td> </td></tr> <tr> <td rowspan=\"4\">1950 </td> <td><i><a href=\"/wiki/Madeleine_(1950_film)\" title=\"Madeleine (1950 film)\">Madeleine</a></i> </td> <td>Scots Divine </td> <td>Uncredited </td></tr> <tr> <td><i><a href=\"/wiki/Treasure_Island_(1950_film)\" title=\"Treasure Island (1950 film)\">Treasure Island</a></i> </td> <td>Blind Pew </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Trio_(film)\" title=\"Trio (film)\">Trio</a></i> </td> <td>Mr. Campbell </td> <td>(segment \"Sanatorium\") </td></tr> <tr> <td><i><a href=\"/wiki/No_Trace_(1950_film)\" title=\"No Trace (1950 film)\">No Trace</a></i> </td> <td>Inspector MacDougall </td> <td> </td></tr> <tr> <td rowspan=\"4\">1951 </td> <td><i><a href=\"/wiki/Pandora_and_the_Flying_Dutchman\" title=\"Pandora and the Flying Dutchman\">Pandora and the Flying Dutchman</a></i> </td> <td>Angus </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Happy_Go_Lovely\" title=\"Happy Go Lovely\">Happy Go Lovely</a></i> </td> <td>Jonskill </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Laughter_in_Paradise\" title=\"Laughter in Paradise\">Laughter in Paradise</a></i> </td> <td>Gordon Webb </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Encore_(1951_film)\" title=\"Encore (1951 film)\">Encore</a></i> </td> <td>Andrews, Engineer </td> <td>(segment \"Winter Cruise\") </td></tr> <tr> <td rowspan=\"4\">1952 </td> <td><i><a href=\"/wiki/Saturday_Island\" title=\"Saturday Island\">Saturday Island</a></i> </td> <td>Grimshaw </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Tread_Softly_(1952_film)\" title=\"Tread Softly (1952 film)\">Tread Softly</a></i> </td> <td>Angus McDonald </td> <td> </td></tr> <tr> <td><i>Too Many Detectives</i> </td> <td rowspan=\"2\">Edward Potter </td> <td>Short </td></tr> <tr> <td><i>Potter of the Yard</i> </td> <td>Short </td></tr> <tr> <td rowspan=\"8\">1953 </td> <td><i><a href=\"/wiki/The_Great_Game_(1953_film)\" title=\"The Great Game (1953 film)\">The Great Game</a></i> </td> <td>Mac Wells </td> <td> </td></tr> <tr> <td><i>Captain Brassbound's Conversion</i> </td> <td>Rankin </td> <td>TV </td></tr> <tr> <td><i>Henry V</i> </td> <td>Pistol </td> <td>TV </td></tr> <tr> <td><i><a href=\"/wiki/The_Fake_(1953_film)\" title=\"The Fake (1953 film)\">The Fake</a></i> </td> <td>Henry Mason </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Johnny_on_the_Run\" title=\"Johnny on the Run\">Johnny on the Run</a></i> </td> <td>Policeman </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Strange_Stories_(film)\" title=\"Strange Stories (film)\">Strange Stories</a></i> </td> <td>Mr. Bartleby </td> <td> </td></tr> <tr> <td><i>Mr. Beamish Goes South</i> </td> <td>Edward Potter </td> <td>Short </td></tr> <tr> <td><i><a href=\"/wiki/Love_in_Pawn\" title=\"Love in Pawn\">Love in Pawn</a></i> </td> <td>McCutcheon </td> <td> </td></tr> <tr> <td rowspan=\"5\">1954 </td> <td><i><a href=\"/wiki/Hobson%27s_Choice_(1954_film)\" title=\"Hobson's Choice (1954 film)\">Hobson's Choice</a></i> </td> <td>Dr. McFarlane </td> <td> </td></tr> <tr> <td><i>Calling Scotland Yard: The Sable Scarf</i> </td> <td> </td> <td>Short </td></tr> <tr> <td><i><a href=\"/wiki/Devil_Girl_from_Mars\" title=\"Devil Girl from Mars\">Devil Girl from Mars</a></i> </td> <td>\"Jamie\" Jamieson </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/The_Black_Knight_(film)\" title=\"The Black Knight (film)\">The Black Knight</a></i> </td> <td>James, the servant </td> <td> </td></tr> <tr> <td><i>Destination Milan</i> </td> <td>Walter McHarry </td> <td> </td></tr> <tr> <td>1955 </td> <td><i><a href=\"/wiki/Richard_III_(1955_film)\" title=\"Richard III (1955 film)\">Richard III</a></i> </td> <td>Lovel </td> <td> </td></tr> <tr> <td rowspan=\"2\">1956 </td> <td><i>Festival Fever</i> </td> <td>Annie's father </td> <td>TV </td></tr> <tr> <td><i>A Day of Grace</i> </td> <td>Uncle Henry </td> <td>Short </td></tr> <tr> <td rowspan=\"2\">1957 </td> <td><i>Murder Reported</i> </td> <td>Mac North - Editor </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Campbell%27s_Kingdom\" title=\"Campbell's Kingdom\">Campbell's Kingdom</a></i> </td> <td>Mac </td> <td> </td></tr> <tr> <td rowspan=\"2\">1958 </td> <td><i><a href=\"/wiki/Next_to_No_Time\" title=\"Next to No Time\">Next to No Time</a></i> </td> <td>Abercrombie, Scottish Director </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Rockets_Galore!\" title=\"Rockets Galore!\">Rockets Galore!</a></i> </td> <td>Capt. MacKechnie </td> <td>Uncredited </td></tr> <tr> <td>1960 </td> <td><i><a href=\"/wiki/Kidnapped_(1960_film)\" title=\"Kidnapped (1960 film)\">Kidnapped</a></i> </td> <td>Ebenezer Balfour </td> <td> </td></tr> <tr> <td rowspan=\"2\">1961 </td> <td><i><a href=\"/wiki/Don%27t_Bother_to_Knock_(1961_film)\" title=\"Don't Bother to Knock (1961 film)\">Don't Bother to Knock</a></i> </td> <td>Taxi Driver </td> <td> </td></tr> <tr> <td><i>One Way Pendulum</i> </td> <td>Judge </td> <td>TV </td></tr> <tr> <td rowspan=\"2\">1963 </td> <td><i><a href=\"/wiki/Siege_of_the_Saxons\" title=\"Siege of the Saxons\">Siege of the Saxons</a></i> </td> <td><a href=\"/wiki/Merlin\" title=\"Merlin\">Merlin</a> </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/Ladies_Who_Do\" title=\"Ladies Who Do\">Ladies Who Do</a></i> </td> <td>Dr. MacGregor </td> <td> </td></tr> <tr> <td>1964 </td> <td><i>Eagle Rock</i> </td> <td>Mr. McTavish </td> <td>Voice </td></tr> <tr> <td>1966 </td> <td><i><a href=\"/wiki/The_Reptile\" title=\"The Reptile\">The Reptile</a></i> </td> <td>Mad Peter </td> <td> </td></tr> <tr> <td>1967 </td> <td><i><a href=\"/wiki/Mister_Ten_Per_Cent\" title=\"Mister Ten Per Cent\">Mister Ten Per Cent</a></i> </td> <td>The Scotsman </td> <td> </td></tr> <tr> <td>1970 </td> <td><i>Step Laughing Into the Grave</i> </td> <td> </td> <td>TV </td></tr> <tr> <td rowspan=\"2\">1971 </td> <td><i><a href=\"/wiki/Dad%27s_Army_(1971_film)\" title=\"Dad's Army (1971 film)\">Dad's Army</a></i> </td> <td><a href=\"/wiki/Private_Frazer\" title=\"Private Frazer\">Private Frazer</a> </td> <td> </td></tr> <tr> <td><i><a href=\"/wiki/The_Abominable_Dr._Phibes\" title=\"The Abominable Dr. Phibes\">The Abominable Dr. Phibes</a></i> </td> <td>Darrow </td> <td> </td></tr> <tr> <td>1974 </td> <td><i>Charles Dickens' World of Christmas</i> </td> <td> </td> <td>TV </td></tr> <tr> <td>1975 </td> <td><i><a href=\"/wiki/One_of_Our_Dinosaurs_Is_Missing\" title=\"One of Our Dinosaurs Is Missing\">One of Our Dinosaurs Is Missing</a></i> </td> <td>Jock </td> <td> </td></tr> <tr> <td>1976 </td> <td><i>Crime Casebook</i> </td> <td>George Winterman / Sellens </td> <td>Short </td></tr> <tr> <td>1979 </td> <td><i><a href=\"/wiki/The_Prisoner_of_Zenda_(1979_film)\" title=\"The Prisoner of Zenda (1979 film)\">The Prisoner of Zenda</a></i> </td> <td>Archbishop </td> <td>(final film role) </td></tr></tbody><tfoot></tfoot></table>'''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
