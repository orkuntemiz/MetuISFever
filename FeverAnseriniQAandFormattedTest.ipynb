{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-FGu2I_yqT-"
   },
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQtJd7m0H_gC"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt-get install maven -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKkh78zferE8"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUoccBGhIaiP"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone --recurse-submodules https://github.com/castorini/anserini.git\n",
    "%cd anserini\n",
    "!cd tools/eval && tar xvfz trec_eval.9.0.4.tar.gz && cd trec_eval.9.0.4 && make && cd ../../..\n",
    "!mvn clean package appassembler:assemble -DskipTests -Dmaven.javadoc.skip=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRWvO9ftIcdB",
    "outputId": "25942963-5edb-4f44-8916-b9d4b407f059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-19 19:27:49--  https://s3-eu-west-1.amazonaws.com/fever.public/feverous/feverous-wiki-pages.zip\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.92.163\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.92.163|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9906569155 (9.2G) [application/zip]\n",
      "Saving to: ‘collections/fever/feverous-wiki-pages.zip’\n",
      "\n",
      "feverous-wiki-pages 100%[===================>]   9.23G  21.4MB/s    in 7m 20s  \n",
      "\n",
      "2021-07-19 19:35:10 (21.5 MB/s) - ‘collections/fever/feverous-wiki-pages.zip’ saved [9906569155/9906569155]\n",
      "\n",
      "Archive:  collections/fever/feverous-wiki-pages.zip\n",
      "   creating: collections/fever/FeverousWikiv1/\n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_283.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_402.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_177.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_287.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_483.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_210.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_451.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_220.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_110.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_320.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_425.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_040.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_238.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_336.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_033.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_008.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_298.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_189.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_326.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_198.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_159.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_426.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_131.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_137.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_069.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_311.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_230.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_526.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_038.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_349.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_428.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_174.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_239.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_392.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_254.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_361.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_480.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_401.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_063.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_318.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_452.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_379.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_003.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_205.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_099.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_173.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_236.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_116.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_393.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_015.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_302.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_217.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_502.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_180.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_193.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_141.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_176.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_257.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_411.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_303.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_247.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_046.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_048.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_344.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_309.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_044.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_342.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_051.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_140.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_413.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_242.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_028.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_115.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_530.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_509.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_385.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_072.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_163.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_510.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_167.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_519.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_057.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_132.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_061.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_415.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_443.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_035.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_092.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_360.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_481.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_340.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_136.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_098.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_076.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_486.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_190.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_474.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_100.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_016.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_104.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_409.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_259.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_388.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_029.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_292.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_525.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_507.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_146.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_383.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_312.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_228.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_358.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_289.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_215.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_601.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_246.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_306.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_290.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_001.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_606.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_417.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_339.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_325.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_000.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_041.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_485.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_271.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_160.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_206.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_119.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_148.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_107.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_448.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_521.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_114.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_195.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_017.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_418.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_497.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_070.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_145.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_408.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_023.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_356.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_218.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_399.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_406.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_187.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_071.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_030.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_026.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_012.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_291.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_468.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_245.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_352.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_433.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_088.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_405.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_602.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_208.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_477.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_412.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_224.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_087.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_427.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_094.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_517.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_101.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_364.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_499.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_475.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_310.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_372.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_300.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_382.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_064.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_125.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_332.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_049.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_231.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_316.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_429.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_184.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_282.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_232.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_274.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_234.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_523.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_032.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_095.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_391.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_214.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_075.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_374.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_455.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_262.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_068.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_089.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_295.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_453.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_147.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_084.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_149.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_060.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_256.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_605.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_288.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_450.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_122.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_221.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_459.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_251.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_447.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_462.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_348.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_467.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_407.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_482.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_233.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_249.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_512.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_416.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_351.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_389.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_161.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_400.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_050.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_010.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_508.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_152.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_473.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_059.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_255.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_105.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_018.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_394.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_258.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_494.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_192.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_337.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_322.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_449.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_346.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_500.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_097.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_165.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_155.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_034.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_381.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_111.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_007.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_085.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_377.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_074.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_212.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_511.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_345.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_194.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_281.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_533.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_479.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_438.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_191.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_005.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_285.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_273.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_103.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_127.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_367.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_266.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_534.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_083.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_264.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_207.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_219.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_333.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_047.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_444.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_398.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_144.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_157.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_077.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_216.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_020.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_515.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_373.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_013.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_263.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_404.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_492.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_253.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_209.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_472.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_488.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_117.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_461.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_043.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_432.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_168.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_156.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_252.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_162.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_225.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_204.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_275.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_229.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_053.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_284.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_268.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_011.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_158.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_260.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_607.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_495.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_498.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_503.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_042.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_380.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_307.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_505.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_024.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_314.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_265.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_065.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_128.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_324.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_501.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_424.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_313.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_489.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_054.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_330.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_270.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_021.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_308.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_169.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_478.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_327.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_513.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_395.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_341.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_121.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_241.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_172.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_150.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_384.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_269.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_369.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_002.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_004.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_126.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_430.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_223.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_368.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_133.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_299.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_504.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_419.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_056.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_343.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_532.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_378.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_202.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_240.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_244.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_151.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_082.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_297.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_397.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_138.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_237.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_476.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_055.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_603.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_226.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_331.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_387.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_457.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_437.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_524.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_436.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_293.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_182.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_319.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_484.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_143.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_506.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_520.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_006.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_079.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_052.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_609.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_420.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_353.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_102.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_014.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_250.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_009.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_493.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_197.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_036.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_196.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_154.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_350.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_458.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_045.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_203.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_454.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_109.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_186.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_366.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_359.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_516.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_370.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_371.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_178.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_025.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_067.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_175.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_108.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_261.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_465.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_363.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_200.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_466.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_375.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_134.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_403.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_354.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_090.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_445.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_376.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_086.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_321.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_027.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_460.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_338.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_490.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_078.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_357.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_091.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_410.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_201.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_470.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_130.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_022.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_062.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_355.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_166.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_434.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_080.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_301.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_464.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_305.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_435.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_124.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_423.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_170.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_129.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_487.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_422.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_317.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_179.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_439.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_518.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_081.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_066.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_276.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_113.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_600.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_106.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_414.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_294.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_267.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_315.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_142.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_531.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_037.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_235.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_112.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_496.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_248.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_456.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_440.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_222.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_608.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_604.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_334.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_277.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_446.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_039.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_135.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_093.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_153.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_171.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_469.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_390.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_164.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_280.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_335.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_019.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_491.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_421.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_031.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_471.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_527.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_329.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_211.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_610.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_386.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_188.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_442.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_396.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_120.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_463.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_213.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_441.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_347.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_199.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_365.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_073.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_304.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_328.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_362.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_123.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_522.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_272.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_058.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_183.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_286.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_243.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_096.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_431.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_279.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_139.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_514.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_323.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_181.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_278.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_118.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_185.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_296.jsonl  \n",
      "  inflating: collections/fever/FeverousWikiv1/wiki_227.jsonl  \n",
      "--2021-07-19 19:45:11--  https://s3-eu-west-1.amazonaws.com/fever.public/feverous/train.jsonl\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.57.227\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.57.227|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 175493294 (167M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘collections/fever/train.jsonl’\n",
      "\n",
      "train.jsonl         100%[===================>] 167.36M  21.3MB/s    in 8.8s    \n",
      "\n",
      "2021-07-19 19:45:21 (19.1 MB/s) - ‘collections/fever/train.jsonl’ saved [175493294/175493294]\n",
      "\n",
      "--2021-07-19 19:45:21--  https://s3-eu-west-1.amazonaws.com/fever.public/feverous/dev.jsonl\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.109.115\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.109.115|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17827949 (17M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘collections/fever/dev.jsonl’\n",
      "\n",
      "dev.jsonl           100%[===================>]  17.00M  10.4MB/s    in 1.6s    \n",
      "\n",
      "2021-07-19 19:45:23 (10.4 MB/s) - ‘collections/fever/dev.jsonl’ saved [17827949/17827949]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the necessary data (i am not sure this is the latest data)\n",
    "!mkdir collections/fever\n",
    "!mkdir indexes/fever\n",
    "\n",
    "!wget https://s3-eu-west-1.amazonaws.com/fever.public/feverous/feverous-wiki-pages.zip -P collections/fever\n",
    "!unzip collections/fever/feverous-wiki-pages.zip -d collections/fever\n",
    "\n",
    "!wget https://s3-eu-west-1.amazonaws.com/fever.public/feverous/train.jsonl -P collections/fever\n",
    "!wget https://s3-eu-west-1.amazonaws.com/fever.public/feverous/dev.jsonl -P collections/fever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fmNJ0RIncWBy",
    "outputId": "8e5f9c00-67c9-4b50-94b0-c49113af4029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_000.jsonl\twiki_109.jsonl\twiki_218.jsonl\twiki_327.jsonl\twiki_436.jsonl\n",
      "wiki_001.jsonl\twiki_110.jsonl\twiki_219.jsonl\twiki_328.jsonl\twiki_437.jsonl\n",
      "wiki_002.jsonl\twiki_111.jsonl\twiki_220.jsonl\twiki_329.jsonl\twiki_438.jsonl\n",
      "wiki_003.jsonl\twiki_112.jsonl\twiki_221.jsonl\twiki_330.jsonl\twiki_439.jsonl\n",
      "wiki_004.jsonl\twiki_113.jsonl\twiki_222.jsonl\twiki_331.jsonl\twiki_440.jsonl\n",
      "wiki_005.jsonl\twiki_114.jsonl\twiki_223.jsonl\twiki_332.jsonl\twiki_441.jsonl\n",
      "wiki_006.jsonl\twiki_115.jsonl\twiki_224.jsonl\twiki_333.jsonl\twiki_442.jsonl\n",
      "wiki_007.jsonl\twiki_116.jsonl\twiki_225.jsonl\twiki_334.jsonl\twiki_443.jsonl\n",
      "wiki_008.jsonl\twiki_117.jsonl\twiki_226.jsonl\twiki_335.jsonl\twiki_444.jsonl\n",
      "wiki_009.jsonl\twiki_118.jsonl\twiki_227.jsonl\twiki_336.jsonl\twiki_445.jsonl\n",
      "wiki_010.jsonl\twiki_119.jsonl\twiki_228.jsonl\twiki_337.jsonl\twiki_446.jsonl\n",
      "wiki_011.jsonl\twiki_120.jsonl\twiki_229.jsonl\twiki_338.jsonl\twiki_447.jsonl\n",
      "wiki_012.jsonl\twiki_121.jsonl\twiki_230.jsonl\twiki_339.jsonl\twiki_448.jsonl\n",
      "wiki_013.jsonl\twiki_122.jsonl\twiki_231.jsonl\twiki_340.jsonl\twiki_449.jsonl\n",
      "wiki_014.jsonl\twiki_123.jsonl\twiki_232.jsonl\twiki_341.jsonl\twiki_450.jsonl\n",
      "wiki_015.jsonl\twiki_124.jsonl\twiki_233.jsonl\twiki_342.jsonl\twiki_451.jsonl\n",
      "wiki_016.jsonl\twiki_125.jsonl\twiki_234.jsonl\twiki_343.jsonl\twiki_452.jsonl\n",
      "wiki_017.jsonl\twiki_126.jsonl\twiki_235.jsonl\twiki_344.jsonl\twiki_453.jsonl\n",
      "wiki_018.jsonl\twiki_127.jsonl\twiki_236.jsonl\twiki_345.jsonl\twiki_454.jsonl\n",
      "wiki_019.jsonl\twiki_128.jsonl\twiki_237.jsonl\twiki_346.jsonl\twiki_455.jsonl\n",
      "wiki_020.jsonl\twiki_129.jsonl\twiki_238.jsonl\twiki_347.jsonl\twiki_456.jsonl\n",
      "wiki_021.jsonl\twiki_130.jsonl\twiki_239.jsonl\twiki_348.jsonl\twiki_457.jsonl\n",
      "wiki_022.jsonl\twiki_131.jsonl\twiki_240.jsonl\twiki_349.jsonl\twiki_458.jsonl\n",
      "wiki_023.jsonl\twiki_132.jsonl\twiki_241.jsonl\twiki_350.jsonl\twiki_459.jsonl\n",
      "wiki_024.jsonl\twiki_133.jsonl\twiki_242.jsonl\twiki_351.jsonl\twiki_460.jsonl\n",
      "wiki_025.jsonl\twiki_134.jsonl\twiki_243.jsonl\twiki_352.jsonl\twiki_461.jsonl\n",
      "wiki_026.jsonl\twiki_135.jsonl\twiki_244.jsonl\twiki_353.jsonl\twiki_462.jsonl\n",
      "wiki_027.jsonl\twiki_136.jsonl\twiki_245.jsonl\twiki_354.jsonl\twiki_463.jsonl\n",
      "wiki_028.jsonl\twiki_137.jsonl\twiki_246.jsonl\twiki_355.jsonl\twiki_464.jsonl\n",
      "wiki_029.jsonl\twiki_138.jsonl\twiki_247.jsonl\twiki_356.jsonl\twiki_465.jsonl\n",
      "wiki_030.jsonl\twiki_139.jsonl\twiki_248.jsonl\twiki_357.jsonl\twiki_466.jsonl\n",
      "wiki_031.jsonl\twiki_140.jsonl\twiki_249.jsonl\twiki_358.jsonl\twiki_467.jsonl\n",
      "wiki_032.jsonl\twiki_141.jsonl\twiki_250.jsonl\twiki_359.jsonl\twiki_468.jsonl\n",
      "wiki_033.jsonl\twiki_142.jsonl\twiki_251.jsonl\twiki_360.jsonl\twiki_469.jsonl\n",
      "wiki_034.jsonl\twiki_143.jsonl\twiki_252.jsonl\twiki_361.jsonl\twiki_470.jsonl\n",
      "wiki_035.jsonl\twiki_144.jsonl\twiki_253.jsonl\twiki_362.jsonl\twiki_471.jsonl\n",
      "wiki_036.jsonl\twiki_145.jsonl\twiki_254.jsonl\twiki_363.jsonl\twiki_472.jsonl\n",
      "wiki_037.jsonl\twiki_146.jsonl\twiki_255.jsonl\twiki_364.jsonl\twiki_473.jsonl\n",
      "wiki_038.jsonl\twiki_147.jsonl\twiki_256.jsonl\twiki_365.jsonl\twiki_474.jsonl\n",
      "wiki_039.jsonl\twiki_148.jsonl\twiki_257.jsonl\twiki_366.jsonl\twiki_475.jsonl\n",
      "wiki_040.jsonl\twiki_149.jsonl\twiki_258.jsonl\twiki_367.jsonl\twiki_476.jsonl\n",
      "wiki_041.jsonl\twiki_150.jsonl\twiki_259.jsonl\twiki_368.jsonl\twiki_477.jsonl\n",
      "wiki_042.jsonl\twiki_151.jsonl\twiki_260.jsonl\twiki_369.jsonl\twiki_478.jsonl\n",
      "wiki_043.jsonl\twiki_152.jsonl\twiki_261.jsonl\twiki_370.jsonl\twiki_479.jsonl\n",
      "wiki_044.jsonl\twiki_153.jsonl\twiki_262.jsonl\twiki_371.jsonl\twiki_480.jsonl\n",
      "wiki_045.jsonl\twiki_154.jsonl\twiki_263.jsonl\twiki_372.jsonl\twiki_481.jsonl\n",
      "wiki_046.jsonl\twiki_155.jsonl\twiki_264.jsonl\twiki_373.jsonl\twiki_482.jsonl\n",
      "wiki_047.jsonl\twiki_156.jsonl\twiki_265.jsonl\twiki_374.jsonl\twiki_483.jsonl\n",
      "wiki_048.jsonl\twiki_157.jsonl\twiki_266.jsonl\twiki_375.jsonl\twiki_484.jsonl\n",
      "wiki_049.jsonl\twiki_158.jsonl\twiki_267.jsonl\twiki_376.jsonl\twiki_485.jsonl\n",
      "wiki_050.jsonl\twiki_159.jsonl\twiki_268.jsonl\twiki_377.jsonl\twiki_486.jsonl\n",
      "wiki_051.jsonl\twiki_160.jsonl\twiki_269.jsonl\twiki_378.jsonl\twiki_487.jsonl\n",
      "wiki_052.jsonl\twiki_161.jsonl\twiki_270.jsonl\twiki_379.jsonl\twiki_488.jsonl\n",
      "wiki_053.jsonl\twiki_162.jsonl\twiki_271.jsonl\twiki_380.jsonl\twiki_489.jsonl\n",
      "wiki_054.jsonl\twiki_163.jsonl\twiki_272.jsonl\twiki_381.jsonl\twiki_490.jsonl\n",
      "wiki_055.jsonl\twiki_164.jsonl\twiki_273.jsonl\twiki_382.jsonl\twiki_491.jsonl\n",
      "wiki_056.jsonl\twiki_165.jsonl\twiki_274.jsonl\twiki_383.jsonl\twiki_492.jsonl\n",
      "wiki_057.jsonl\twiki_166.jsonl\twiki_275.jsonl\twiki_384.jsonl\twiki_493.jsonl\n",
      "wiki_058.jsonl\twiki_167.jsonl\twiki_276.jsonl\twiki_385.jsonl\twiki_494.jsonl\n",
      "wiki_059.jsonl\twiki_168.jsonl\twiki_277.jsonl\twiki_386.jsonl\twiki_495.jsonl\n",
      "wiki_060.jsonl\twiki_169.jsonl\twiki_278.jsonl\twiki_387.jsonl\twiki_496.jsonl\n",
      "wiki_061.jsonl\twiki_170.jsonl\twiki_279.jsonl\twiki_388.jsonl\twiki_497.jsonl\n",
      "wiki_062.jsonl\twiki_171.jsonl\twiki_280.jsonl\twiki_389.jsonl\twiki_498.jsonl\n",
      "wiki_063.jsonl\twiki_172.jsonl\twiki_281.jsonl\twiki_390.jsonl\twiki_499.jsonl\n",
      "wiki_064.jsonl\twiki_173.jsonl\twiki_282.jsonl\twiki_391.jsonl\twiki_500.jsonl\n",
      "wiki_065.jsonl\twiki_174.jsonl\twiki_283.jsonl\twiki_392.jsonl\twiki_501.jsonl\n",
      "wiki_066.jsonl\twiki_175.jsonl\twiki_284.jsonl\twiki_393.jsonl\twiki_502.jsonl\n",
      "wiki_067.jsonl\twiki_176.jsonl\twiki_285.jsonl\twiki_394.jsonl\twiki_503.jsonl\n",
      "wiki_068.jsonl\twiki_177.jsonl\twiki_286.jsonl\twiki_395.jsonl\twiki_504.jsonl\n",
      "wiki_069.jsonl\twiki_178.jsonl\twiki_287.jsonl\twiki_396.jsonl\twiki_505.jsonl\n",
      "wiki_070.jsonl\twiki_179.jsonl\twiki_288.jsonl\twiki_397.jsonl\twiki_506.jsonl\n",
      "wiki_071.jsonl\twiki_180.jsonl\twiki_289.jsonl\twiki_398.jsonl\twiki_507.jsonl\n",
      "wiki_072.jsonl\twiki_181.jsonl\twiki_290.jsonl\twiki_399.jsonl\twiki_508.jsonl\n",
      "wiki_073.jsonl\twiki_182.jsonl\twiki_291.jsonl\twiki_400.jsonl\twiki_509.jsonl\n",
      "wiki_074.jsonl\twiki_183.jsonl\twiki_292.jsonl\twiki_401.jsonl\twiki_510.jsonl\n",
      "wiki_075.jsonl\twiki_184.jsonl\twiki_293.jsonl\twiki_402.jsonl\twiki_511.jsonl\n",
      "wiki_076.jsonl\twiki_185.jsonl\twiki_294.jsonl\twiki_403.jsonl\twiki_512.jsonl\n",
      "wiki_077.jsonl\twiki_186.jsonl\twiki_295.jsonl\twiki_404.jsonl\twiki_513.jsonl\n",
      "wiki_078.jsonl\twiki_187.jsonl\twiki_296.jsonl\twiki_405.jsonl\twiki_514.jsonl\n",
      "wiki_079.jsonl\twiki_188.jsonl\twiki_297.jsonl\twiki_406.jsonl\twiki_515.jsonl\n",
      "wiki_080.jsonl\twiki_189.jsonl\twiki_298.jsonl\twiki_407.jsonl\twiki_516.jsonl\n",
      "wiki_081.jsonl\twiki_190.jsonl\twiki_299.jsonl\twiki_408.jsonl\twiki_517.jsonl\n",
      "wiki_082.jsonl\twiki_191.jsonl\twiki_300.jsonl\twiki_409.jsonl\twiki_518.jsonl\n",
      "wiki_083.jsonl\twiki_192.jsonl\twiki_301.jsonl\twiki_410.jsonl\twiki_519.jsonl\n",
      "wiki_084.jsonl\twiki_193.jsonl\twiki_302.jsonl\twiki_411.jsonl\twiki_520.jsonl\n",
      "wiki_085.jsonl\twiki_194.jsonl\twiki_303.jsonl\twiki_412.jsonl\twiki_521.jsonl\n",
      "wiki_086.jsonl\twiki_195.jsonl\twiki_304.jsonl\twiki_413.jsonl\twiki_522.jsonl\n",
      "wiki_087.jsonl\twiki_196.jsonl\twiki_305.jsonl\twiki_414.jsonl\twiki_523.jsonl\n",
      "wiki_088.jsonl\twiki_197.jsonl\twiki_306.jsonl\twiki_415.jsonl\twiki_524.jsonl\n",
      "wiki_089.jsonl\twiki_198.jsonl\twiki_307.jsonl\twiki_416.jsonl\twiki_525.jsonl\n",
      "wiki_090.jsonl\twiki_199.jsonl\twiki_308.jsonl\twiki_417.jsonl\twiki_526.jsonl\n",
      "wiki_091.jsonl\twiki_200.jsonl\twiki_309.jsonl\twiki_418.jsonl\twiki_527.jsonl\n",
      "wiki_092.jsonl\twiki_201.jsonl\twiki_310.jsonl\twiki_419.jsonl\twiki_530.jsonl\n",
      "wiki_093.jsonl\twiki_202.jsonl\twiki_311.jsonl\twiki_420.jsonl\twiki_531.jsonl\n",
      "wiki_094.jsonl\twiki_203.jsonl\twiki_312.jsonl\twiki_421.jsonl\twiki_532.jsonl\n",
      "wiki_095.jsonl\twiki_204.jsonl\twiki_313.jsonl\twiki_422.jsonl\twiki_533.jsonl\n",
      "wiki_096.jsonl\twiki_205.jsonl\twiki_314.jsonl\twiki_423.jsonl\twiki_534.jsonl\n",
      "wiki_097.jsonl\twiki_206.jsonl\twiki_315.jsonl\twiki_424.jsonl\twiki_600.jsonl\n",
      "wiki_098.jsonl\twiki_207.jsonl\twiki_316.jsonl\twiki_425.jsonl\twiki_601.jsonl\n",
      "wiki_099.jsonl\twiki_208.jsonl\twiki_317.jsonl\twiki_426.jsonl\twiki_602.jsonl\n",
      "wiki_100.jsonl\twiki_209.jsonl\twiki_318.jsonl\twiki_427.jsonl\twiki_603.jsonl\n",
      "wiki_101.jsonl\twiki_210.jsonl\twiki_319.jsonl\twiki_428.jsonl\twiki_604.jsonl\n",
      "wiki_102.jsonl\twiki_211.jsonl\twiki_320.jsonl\twiki_429.jsonl\twiki_605.jsonl\n",
      "wiki_103.jsonl\twiki_212.jsonl\twiki_321.jsonl\twiki_430.jsonl\twiki_606.jsonl\n",
      "wiki_104.jsonl\twiki_213.jsonl\twiki_322.jsonl\twiki_431.jsonl\twiki_607.jsonl\n",
      "wiki_105.jsonl\twiki_214.jsonl\twiki_323.jsonl\twiki_432.jsonl\twiki_608.jsonl\n",
      "wiki_106.jsonl\twiki_215.jsonl\twiki_324.jsonl\twiki_433.jsonl\twiki_609.jsonl\n",
      "wiki_107.jsonl\twiki_216.jsonl\twiki_325.jsonl\twiki_434.jsonl\twiki_610.jsonl\n",
      "wiki_108.jsonl\twiki_217.jsonl\twiki_326.jsonl\twiki_435.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls collections/fever/FeverousWikiv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCY3dd7lYnuQ",
    "outputId": "13c030c6-422d-468f-df9b-ed06ab67a0a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 file(s) will be processed.\n",
      "Formatting is complete after 0.67 minute(s)\n"
     ]
    }
   ],
   "source": [
    "# A folder path, a file path, or a list of file paths is needed\n",
    "\n",
    "# DATA_LOCATION = \"wiki_pages\"\n",
    "# DATA_LOCATION = \"wiki_pages/wiki_000.jsonl\"\n",
    "DATA_LOCATION = [\n",
    "    \"collections/fever/FeverousWikiv1/wiki_000.jsonl\",\n",
    "    \"collections/fever/FeverousWikiv1/wiki_001.jsonl\",\n",
    "    \"collections/fever/FeverousWikiv1/wiki_002.jsonl\",\n",
    "    \"collections/fever/FeverousWikiv1/wiki_003.jsonl\",\n",
    "    \"collections/fever/FeverousWikiv1/wiki_004.jsonl\",\n",
    "]\n",
    "OUTPUT_FOLDER = \"wiki_pages_anserini\"\n",
    "\n",
    "SIMPLIFY_PAGE_LINKS = False\n",
    "PREFER_URL_FOR_LINKS = False\n",
    "\n",
    "# If SIMPLIFY_PAGE_LINKS and not PREFER_URL_FOR_LINKS: [[Page_(disambiguation)|Page Title]] -> Page Title\n",
    "# If SIMPLIFY_PAGE_LINKS and PREFER_URL_FOR_LINKS: [[Page_(disambiguation)|Page Title]] -> Page (disambiguation)\n",
    "# Note that the URL basename can sometimes contain extra information, but it can also redirect to a page that is different than the visible label.\n",
    "\n",
    "SKIP_TABLES = True\n",
    "SKIP_SECTIONS = True\n",
    "SKIP_LISTS = True\n",
    "\n",
    "MERGE_CHILDREN = False\n",
    "\n",
    "SUBSECTION_SEP = \" ~~\"\n",
    "SUBSUBSECTION_SEP = \" ~~\"\n",
    "# ID_SEP = \"\\t\"\n",
    "ID_SEP = \" \"\n",
    "# ITEM_SEP = \"\\n\"\n",
    "ITEM_SEP = \" ~~\"\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "to_process = None\n",
    "\n",
    "if type(DATA_LOCATION) == list:  # A list of file paths\n",
    "    to_process = [file for file in DATA_LOCATION]\n",
    "elif os.path.isfile(DATA_LOCATION):  # A file path\n",
    "    to_process = [DATA_LOCATION]\n",
    "elif os.path.isdir(DATA_LOCATION):  # A folder\n",
    "    to_process = glob.glob(\"{}/*.jsonl\".format(DATA_LOCATION))\n",
    "else:\n",
    "    raise ValueError(\"Data location is not a valid file or folder.\")\n",
    "\n",
    "print(\"{} file(s) will be processed.\".format(len(to_process)))\n",
    "\n",
    "import json\n",
    "import re\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "def clean_value(value):\n",
    "    \"\"\"Replaces unnecessary space (including \\t and such) with a single space\n",
    "    character.\n",
    "\n",
    "    Arg:\n",
    "        value: A string.\n",
    "\n",
    "    Returns:\n",
    "        The simplified version of the provided string.\n",
    "    \"\"\"\n",
    "    return \" \".join(value.split())\n",
    "\n",
    "\n",
    "def final_clean(\n",
    "    field,\n",
    "    simplify_page_links=SIMPLIFY_PAGE_LINKS,\n",
    "    prefer_url_for_links=PREFER_URL_FOR_LINKS,\n",
    "):\n",
    "    \"\"\"Removes multiple space characters with a single space character. Removes\n",
    "    leading and trailing spaces. Simplifies the page links according to the\n",
    "    parameters.\n",
    "\n",
    "    Args:\n",
    "        field (str): A field of the processed page data.\n",
    "        simplify_page_links (Boolean): A Boolean indicating whether the page\n",
    "            links must be simplified. By default, it is set to\n",
    "            SIMPLIFY_PAGE_LINKS.\n",
    "        prefer_url_for_links (Boolean): A Boolean indicating whether the page\n",
    "            links must be simplified to the URL basename instead of the link's\n",
    "            label. By default, it is set to SIMPLIFY_PAGE_LINKS.\n",
    "\n",
    "    Returns:\n",
    "        str: The simplified version of the provided string.\n",
    "    \"\"\"\n",
    "    field = re.sub(\" +\", \" \", field.strip(), flags=re.MULTILINE)\n",
    "    if SIMPLIFY_PAGE_LINKS and PREFER_URL_FOR_LINKS:\n",
    "        # Simplifies links by replacing them with the URL basename (replaces underscores)\n",
    "        # Underscores are replaced with space\n",
    "        field = re.sub(r\"(?:\\[\\[)(.*)?(?:\\|)\", field.replace(\"_\", \" \"), field)\n",
    "        field = re.sub(r\"(\\|).*?(\\]\\])|\\[\\[\", \"\", field, flags=re.MULTILINE)\n",
    "    elif SIMPLIFY_PAGE_LINKS:\n",
    "        # Simplifies links by replacing them with the titles\n",
    "        field = re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", field, flags=re.MULTILINE)\n",
    "    return field\n",
    "\n",
    "\n",
    "if not os.path.exists(OUTPUT_FOLDER):  # Creates the output folder if it does not exist\n",
    "    os.mkdir(OUTPUT_FOLDER)\n",
    "\n",
    "timer_start = timer()\n",
    "\n",
    "counter = 0\n",
    "for file_path in to_process:\n",
    "    file_basename = os.path.basename(file_path)\n",
    "    output_path = os.path.join(OUTPUT_FOLDER, file_basename)\n",
    "\n",
    "    open(output_path, mode=\"w\", encoding=\"utf-8\").close()  # Creating an empty output file\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_data = [json.loads(line) for line in f]\n",
    "\n",
    "    for page in file_data:\n",
    "\n",
    "        page_data_processed = {\"id\": page[\"title\"], \"text\": \"\", \"lines\": \"\"}\n",
    "\n",
    "        # The order list of the page will be used to parse the items\n",
    "        for item_id, item in enumerate(page[\"order\"]):\n",
    "            if item.startswith(\"sentence_\"):  # Sentence\n",
    "                clean_element = clean_value(page[item])\n",
    "                page_data_processed[\"text\"] += \" {}\".format(clean_element)\n",
    "                page_data_processed[\"lines\"] += \" {}{}{}{}\".format(item, ID_SEP, clean_element, ITEM_SEP)\n",
    "            elif item.startswith(\"table_\"):  # Table\n",
    "                if SKIP_TABLES:\n",
    "                    continue\n",
    "                page_data_processed[\"text\"] += \" \"\n",
    "                page_data_processed[\"lines\"] += \"{}{}\".format(item, ID_SEP)\n",
    "                table_rows = []\n",
    "                for row in page[item][\"table\"]:\n",
    "                    row_items = []\n",
    "                    for cell in row:\n",
    "                        if MERGE_CHILDREN:\n",
    "                            row_items.append(clean_value(cell[\"value\"]))\n",
    "                        else:\n",
    "                            page_data_processed[\"text\"] += \" {}\".format(clean_value(cell[\"value\"]))\n",
    "                            page_data_processed[\"lines\"] += \" {}{}{}{}\".format(cell[\"id\"], ID_SEP, clean_value(cell[\"value\"]), ITEM_SEP)\n",
    "\n",
    "                    if MERGE_CHILDREN:\n",
    "                        row_text = \"{}\".format(SUBSUBSECTION_SEP).join(row_items)\n",
    "                        table_rows.append(row_text)\n",
    "\n",
    "                if \"caption\" in page[item]:\n",
    "                    if MERGE_CHILDREN:\n",
    "                        table_rows.append(clean_value(page[item][\"caption\"]))\n",
    "                    else:\n",
    "                        page_data_processed[\"text\"] += \" {}\".format(clean_value(page[item][\"caption\"]))\n",
    "                        page_data_processed[\"lines\"] += \" {}{}{}{}\".format(item_id, ID_SEP, clean_value(page[item][\"caption\"]), ITEM_SEP)\n",
    "\n",
    "                if MERGE_CHILDREN:\n",
    "                    table_text = \"{}\".format(SUBSECTION_SEP).join(table_rows)\n",
    "                    page_data_processed[\"text\"] += table_text\n",
    "                    page_data_processed[\"lines\"] += table_text\n",
    "                    page_data_processed[\"lines\"] += ITEM_SEP\n",
    "            elif item.startswith(\"section_\"):  # Section\n",
    "                if SKIP_SECTIONS:\n",
    "                    continue\n",
    "                clean_element = clean_value(page[item][\"value\"])\n",
    "                page_data_processed[\"text\"] += \" {}\".format(clean_element)\n",
    "                page_data_processed[\"lines\"] += \" {}{}{}{}\".format(item, ID_SEP, clean_element, ITEM_SEP)\n",
    "            elif item.startswith(\"list_\"):  # List\n",
    "                if SKIP_LISTS:\n",
    "                    continue\n",
    "                list_items = []\n",
    "                for list_item in page[item][\"list\"]:\n",
    "                    if MERGE_CHILDREN:\n",
    "                        list_items.append(clean_value(list_item[\"value\"]))\n",
    "                    else:\n",
    "                        page_data_processed[\"text\"] += \" {}\".format(clean_value(list_item[\"value\"]))\n",
    "                        page_data_processed[\"lines\"] += \" {}{}{}{}\".format(list_item[\"id\"], ID_SEP, clean_value(list_item[\"value\"]), ITEM_SEP)\n",
    "                \n",
    "                if MERGE_CHILDREN:\n",
    "                    list_text = \"{}\".format(SUBSECTION_SEP).join(list_items)\n",
    "                    page_data_processed[\"text\"] += list_text\n",
    "                    page_data_processed[\"lines\"] += \" {}{}{}{}\".format(item, ID_SEP, list_text, ITEM_SEP)\n",
    "            else:  # All alternatives must be handled and the code must not reach here\n",
    "                raise ValueError(\"Unidentified page element found.\")\n",
    "\n",
    "        page_data_processed[\"text\"] = final_clean(page_data_processed[\"text\"])\n",
    "        page_data_processed[\"lines\"] = final_clean(page_data_processed[\"lines\"])\n",
    "\n",
    "        # Appending the processed page data to the file\n",
    "        with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            json.dump(page_data_processed, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "timer_end = timer()\n",
    "\n",
    "print(\"Formatting is complete after {:.2f} minute(s)\".format((timer_end - timer_start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkZAGTDasS3m"
   },
   "outputs": [],
   "source": [
    "!sh target/appassembler/bin/IndexCollection -collection FeverParagraphCollection \\\n",
    " -input wiki_pages_anserini \\\n",
    " -index indexes/fever/lucene-index-fever-paragraph \\\n",
    " -generator DefaultLuceneDocumentGenerator \\\n",
    " -threads 1 -storePositions -storeDocvectors -storeRaw \\\n",
    "  >& logs/log.fever &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3jcK9thJgKo",
    "outputId": "45a2b42d-c2de-40b2-fbd5-b5dc5dc93fd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Removing leading `/' from member names\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1.nvm\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1.si\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/segments_2\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1.fnm\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1_Lucene50_0.doc\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/write.lock\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1_Lucene80_0.dvm\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1.fdt\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1_Lucene50_0.tip\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1.tvd\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1.nvd\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1.tvx\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1_Lucene50_0.pos\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1_Lucene50_0.tim\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1_Lucene80_0.dvd\n",
      "/content/anserini/indexes/fever/lucene-index-fever-paragraph/_1.fdx\n"
     ]
    }
   ],
   "source": [
    "!tar -cvf \"/content/anserini/indexes/fever/lucene-index-fever-paragraph.tar\" \"/content/anserini/indexes/fever/lucene-index-fever-paragraph\" #Tar operation for exporting the index and make it reusable\n",
    "# Only one indexing will be enough we will use the tar file after that use the code below to untar it and equate to the LucuneDir\n",
    "#!tar xvfz lucene-index-fever-paragraph.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzHeiKMIWjLk"
   },
   "outputs": [],
   "source": [
    "luceneDir = 'indexes/fever/lucene-index-fever-paragraph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8zhAZVwfAmm"
   },
   "outputs": [],
   "source": [
    "USE_SUMMARY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IM5KN40Peyit",
    "outputId": "06525edb-e7aa-4b7d-ca3e-1427a6da2524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  178M  100  178M    0     0   141M      0  0:00:01  0:00:01 --:--:--  141M\n",
      "jdk-11.0.2/bin/jaotc\n",
      "jdk-11.0.2/bin/jar\n",
      "jdk-11.0.2/bin/jarsigner\n",
      "jdk-11.0.2/bin/java\n",
      "jdk-11.0.2/bin/javac\n",
      "jdk-11.0.2/bin/javadoc\n",
      "jdk-11.0.2/bin/javap\n",
      "jdk-11.0.2/bin/jcmd\n",
      "jdk-11.0.2/bin/jconsole\n",
      "jdk-11.0.2/bin/jdb\n",
      "jdk-11.0.2/bin/jdeprscan\n",
      "jdk-11.0.2/bin/jdeps\n",
      "jdk-11.0.2/bin/jhsdb\n",
      "jdk-11.0.2/bin/jimage\n",
      "jdk-11.0.2/bin/jinfo\n",
      "jdk-11.0.2/bin/jjs\n",
      "jdk-11.0.2/bin/jlink\n",
      "jdk-11.0.2/bin/jmap\n",
      "jdk-11.0.2/bin/jmod\n",
      "jdk-11.0.2/bin/jps\n",
      "jdk-11.0.2/bin/jrunscript\n",
      "jdk-11.0.2/bin/jshell\n",
      "jdk-11.0.2/bin/jstack\n",
      "jdk-11.0.2/bin/jstat\n",
      "jdk-11.0.2/bin/jstatd\n",
      "jdk-11.0.2/bin/keytool\n",
      "jdk-11.0.2/bin/pack200\n",
      "jdk-11.0.2/bin/rmic\n",
      "jdk-11.0.2/bin/rmid\n",
      "jdk-11.0.2/bin/rmiregistry\n",
      "jdk-11.0.2/bin/serialver\n",
      "jdk-11.0.2/bin/unpack200\n",
      "jdk-11.0.2/conf/logging.properties\n",
      "jdk-11.0.2/conf/management/jmxremote.access\n",
      "jdk-11.0.2/conf/management/jmxremote.password.template\n",
      "jdk-11.0.2/conf/management/management.properties\n",
      "jdk-11.0.2/conf/net.properties\n",
      "jdk-11.0.2/conf/security/java.policy\n",
      "jdk-11.0.2/conf/security/java.security\n",
      "jdk-11.0.2/conf/security/policy/README.txt\n",
      "jdk-11.0.2/conf/security/policy/limited/default_US_export.policy\n",
      "jdk-11.0.2/conf/security/policy/limited/default_local.policy\n",
      "jdk-11.0.2/conf/security/policy/limited/exempt_local.policy\n",
      "jdk-11.0.2/conf/security/policy/unlimited/default_US_export.policy\n",
      "jdk-11.0.2/conf/security/policy/unlimited/default_local.policy\n",
      "jdk-11.0.2/conf/sound.properties\n",
      "jdk-11.0.2/include/classfile_constants.h\n",
      "jdk-11.0.2/include/jawt.h\n",
      "jdk-11.0.2/include/jdwpTransport.h\n",
      "jdk-11.0.2/include/jni.h\n",
      "jdk-11.0.2/include/jvmti.h\n",
      "jdk-11.0.2/include/jvmticmlr.h\n",
      "jdk-11.0.2/include/linux/jawt_md.h\n",
      "jdk-11.0.2/include/linux/jni_md.h\n",
      "jdk-11.0.2/jmods/java.base.jmod\n",
      "jdk-11.0.2/jmods/java.compiler.jmod\n",
      "jdk-11.0.2/jmods/java.datatransfer.jmod\n",
      "jdk-11.0.2/jmods/java.desktop.jmod\n",
      "jdk-11.0.2/jmods/java.instrument.jmod\n",
      "jdk-11.0.2/jmods/java.logging.jmod\n",
      "jdk-11.0.2/jmods/java.management.jmod\n",
      "jdk-11.0.2/jmods/java.management.rmi.jmod\n",
      "jdk-11.0.2/jmods/java.naming.jmod\n",
      "jdk-11.0.2/jmods/java.net.http.jmod\n",
      "jdk-11.0.2/jmods/java.prefs.jmod\n",
      "jdk-11.0.2/jmods/java.rmi.jmod\n",
      "jdk-11.0.2/jmods/java.scripting.jmod\n",
      "jdk-11.0.2/jmods/java.se.jmod\n",
      "jdk-11.0.2/jmods/java.security.jgss.jmod\n",
      "jdk-11.0.2/jmods/java.security.sasl.jmod\n",
      "jdk-11.0.2/jmods/java.smartcardio.jmod\n",
      "jdk-11.0.2/jmods/java.sql.jmod\n",
      "jdk-11.0.2/jmods/java.sql.rowset.jmod\n",
      "jdk-11.0.2/jmods/java.transaction.xa.jmod\n",
      "jdk-11.0.2/jmods/java.xml.crypto.jmod\n",
      "jdk-11.0.2/jmods/java.xml.jmod\n",
      "jdk-11.0.2/jmods/jdk.accessibility.jmod\n",
      "jdk-11.0.2/jmods/jdk.aot.jmod\n",
      "jdk-11.0.2/jmods/jdk.attach.jmod\n",
      "jdk-11.0.2/jmods/jdk.charsets.jmod\n",
      "jdk-11.0.2/jmods/jdk.compiler.jmod\n",
      "jdk-11.0.2/jmods/jdk.crypto.cryptoki.jmod\n",
      "jdk-11.0.2/jmods/jdk.crypto.ec.jmod\n",
      "jdk-11.0.2/jmods/jdk.dynalink.jmod\n",
      "jdk-11.0.2/jmods/jdk.editpad.jmod\n",
      "jdk-11.0.2/jmods/jdk.hotspot.agent.jmod\n",
      "jdk-11.0.2/jmods/jdk.httpserver.jmod\n",
      "jdk-11.0.2/jmods/jdk.internal.ed.jmod\n",
      "jdk-11.0.2/jmods/jdk.internal.jvmstat.jmod\n",
      "jdk-11.0.2/jmods/jdk.internal.le.jmod\n",
      "jdk-11.0.2/jmods/jdk.internal.opt.jmod\n",
      "jdk-11.0.2/jmods/jdk.internal.vm.ci.jmod\n",
      "jdk-11.0.2/jmods/jdk.internal.vm.compiler.jmod\n",
      "jdk-11.0.2/jmods/jdk.internal.vm.compiler.management.jmod\n",
      "jdk-11.0.2/jmods/jdk.jartool.jmod\n",
      "jdk-11.0.2/jmods/jdk.javadoc.jmod\n",
      "jdk-11.0.2/jmods/jdk.jcmd.jmod\n",
      "jdk-11.0.2/jmods/jdk.jconsole.jmod\n",
      "jdk-11.0.2/jmods/jdk.jdeps.jmod\n",
      "jdk-11.0.2/jmods/jdk.jdi.jmod\n",
      "jdk-11.0.2/jmods/jdk.jdwp.agent.jmod\n",
      "jdk-11.0.2/jmods/jdk.jfr.jmod\n",
      "jdk-11.0.2/jmods/jdk.jlink.jmod\n",
      "jdk-11.0.2/jmods/jdk.jshell.jmod\n",
      "jdk-11.0.2/jmods/jdk.jsobject.jmod\n",
      "jdk-11.0.2/jmods/jdk.jstatd.jmod\n",
      "jdk-11.0.2/jmods/jdk.localedata.jmod\n",
      "jdk-11.0.2/jmods/jdk.management.agent.jmod\n",
      "jdk-11.0.2/jmods/jdk.management.jfr.jmod\n",
      "jdk-11.0.2/jmods/jdk.management.jmod\n",
      "jdk-11.0.2/jmods/jdk.naming.dns.jmod\n",
      "jdk-11.0.2/jmods/jdk.naming.rmi.jmod\n",
      "jdk-11.0.2/jmods/jdk.net.jmod\n",
      "jdk-11.0.2/jmods/jdk.pack.jmod\n",
      "jdk-11.0.2/jmods/jdk.rmic.jmod\n",
      "jdk-11.0.2/jmods/jdk.scripting.nashorn.jmod\n",
      "jdk-11.0.2/jmods/jdk.scripting.nashorn.shell.jmod\n",
      "jdk-11.0.2/jmods/jdk.sctp.jmod\n",
      "jdk-11.0.2/jmods/jdk.security.auth.jmod\n",
      "jdk-11.0.2/jmods/jdk.security.jgss.jmod\n",
      "jdk-11.0.2/jmods/jdk.unsupported.desktop.jmod\n",
      "jdk-11.0.2/jmods/jdk.unsupported.jmod\n",
      "jdk-11.0.2/jmods/jdk.xml.dom.jmod\n",
      "jdk-11.0.2/jmods/jdk.zipfs.jmod\n",
      "jdk-11.0.2/legal/java.base/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.base/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.base/LICENSE\n",
      "jdk-11.0.2/legal/java.base/aes.md\n",
      "jdk-11.0.2/legal/java.base/asm.md\n",
      "jdk-11.0.2/legal/java.base/c-libutl.md\n",
      "jdk-11.0.2/legal/java.base/cldr.md\n",
      "jdk-11.0.2/legal/java.base/icu.md\n",
      "jdk-11.0.2/legal/java.base/public_suffix.md\n",
      "jdk-11.0.2/legal/java.base/unicode.md\n",
      "jdk-11.0.2/legal/java.compiler/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.compiler/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.compiler/LICENSE\n",
      "jdk-11.0.2/legal/java.datatransfer/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.datatransfer/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.datatransfer/LICENSE\n",
      "jdk-11.0.2/legal/java.desktop/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.desktop/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.desktop/LICENSE\n",
      "jdk-11.0.2/legal/java.desktop/colorimaging.md\n",
      "jdk-11.0.2/legal/java.desktop/giflib.md\n",
      "jdk-11.0.2/legal/java.desktop/harfbuzz.md\n",
      "jdk-11.0.2/legal/java.desktop/jpeg.md\n",
      "jdk-11.0.2/legal/java.desktop/lcms.md\n",
      "jdk-11.0.2/legal/java.desktop/libpng.md\n",
      "jdk-11.0.2/legal/java.desktop/mesa3d.md\n",
      "jdk-11.0.2/legal/java.desktop/opengl.md\n",
      "jdk-11.0.2/legal/java.desktop/xwindows.md\n",
      "jdk-11.0.2/legal/java.instrument/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.instrument/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.instrument/LICENSE\n",
      "jdk-11.0.2/legal/java.logging/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.logging/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.logging/LICENSE\n",
      "jdk-11.0.2/legal/java.management.rmi/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.management.rmi/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.management.rmi/LICENSE\n",
      "jdk-11.0.2/legal/java.management/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.management/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.management/LICENSE\n",
      "jdk-11.0.2/legal/java.naming/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.naming/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.naming/LICENSE\n",
      "jdk-11.0.2/legal/java.net.http/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.net.http/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.net.http/LICENSE\n",
      "jdk-11.0.2/legal/java.prefs/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.prefs/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.prefs/LICENSE\n",
      "jdk-11.0.2/legal/java.rmi/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.rmi/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.rmi/LICENSE\n",
      "jdk-11.0.2/legal/java.scripting/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.scripting/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.scripting/LICENSE\n",
      "jdk-11.0.2/legal/java.se/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.se/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.se/LICENSE\n",
      "jdk-11.0.2/legal/java.security.jgss/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.security.jgss/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.security.jgss/LICENSE\n",
      "jdk-11.0.2/legal/java.security.sasl/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.security.sasl/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.security.sasl/LICENSE\n",
      "jdk-11.0.2/legal/java.smartcardio/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.smartcardio/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.smartcardio/LICENSE\n",
      "jdk-11.0.2/legal/java.smartcardio/pcsclite.md\n",
      "jdk-11.0.2/legal/java.sql.rowset/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.sql.rowset/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.sql.rowset/LICENSE\n",
      "jdk-11.0.2/legal/java.sql/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.sql/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.sql/LICENSE\n",
      "jdk-11.0.2/legal/java.transaction.xa/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.transaction.xa/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.transaction.xa/LICENSE\n",
      "jdk-11.0.2/legal/java.xml.crypto/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.xml.crypto/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.xml.crypto/LICENSE\n",
      "jdk-11.0.2/legal/java.xml.crypto/santuario.md\n",
      "jdk-11.0.2/legal/java.xml/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/java.xml/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/java.xml/LICENSE\n",
      "jdk-11.0.2/legal/java.xml/bcel.md\n",
      "jdk-11.0.2/legal/java.xml/dom.md\n",
      "jdk-11.0.2/legal/java.xml/jcup.md\n",
      "jdk-11.0.2/legal/java.xml/xalan.md\n",
      "jdk-11.0.2/legal/java.xml/xerces.md\n",
      "jdk-11.0.2/legal/jdk.accessibility/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.accessibility/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.accessibility/LICENSE\n",
      "jdk-11.0.2/legal/jdk.aot/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.aot/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.aot/LICENSE\n",
      "jdk-11.0.2/legal/jdk.attach/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.attach/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.attach/LICENSE\n",
      "jdk-11.0.2/legal/jdk.charsets/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.charsets/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.charsets/LICENSE\n",
      "jdk-11.0.2/legal/jdk.compiler/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.compiler/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.compiler/LICENSE\n",
      "jdk-11.0.2/legal/jdk.crypto.cryptoki/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.crypto.cryptoki/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.crypto.cryptoki/LICENSE\n",
      "jdk-11.0.2/legal/jdk.crypto.cryptoki/pkcs11cryptotoken.md\n",
      "jdk-11.0.2/legal/jdk.crypto.cryptoki/pkcs11wrapper.md\n",
      "jdk-11.0.2/legal/jdk.crypto.ec/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.crypto.ec/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.crypto.ec/LICENSE\n",
      "jdk-11.0.2/legal/jdk.crypto.ec/ecc.md\n",
      "jdk-11.0.2/legal/jdk.dynalink/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.dynalink/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.dynalink/LICENSE\n",
      "jdk-11.0.2/legal/jdk.dynalink/dynalink.md\n",
      "jdk-11.0.2/legal/jdk.editpad/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.editpad/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.editpad/LICENSE\n",
      "jdk-11.0.2/legal/jdk.hotspot.agent/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.hotspot.agent/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.hotspot.agent/LICENSE\n",
      "jdk-11.0.2/legal/jdk.httpserver/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.httpserver/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.httpserver/LICENSE\n",
      "jdk-11.0.2/legal/jdk.internal.ed/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.internal.ed/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.internal.ed/LICENSE\n",
      "jdk-11.0.2/legal/jdk.internal.jvmstat/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.internal.jvmstat/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.internal.jvmstat/LICENSE\n",
      "jdk-11.0.2/legal/jdk.internal.le/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.internal.le/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.internal.le/LICENSE\n",
      "jdk-11.0.2/legal/jdk.internal.le/jline.md\n",
      "jdk-11.0.2/legal/jdk.internal.opt/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.internal.opt/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.internal.opt/LICENSE\n",
      "jdk-11.0.2/legal/jdk.internal.opt/jopt-simple.md\n",
      "jdk-11.0.2/legal/jdk.internal.vm.ci/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.internal.vm.ci/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.internal.vm.ci/LICENSE\n",
      "jdk-11.0.2/legal/jdk.internal.vm.compiler.management/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.internal.vm.compiler.management/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.internal.vm.compiler.management/LICENSE\n",
      "jdk-11.0.2/legal/jdk.internal.vm.compiler/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.internal.vm.compiler/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.internal.vm.compiler/LICENSE\n",
      "jdk-11.0.2/legal/jdk.jartool/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.jartool/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.jartool/LICENSE\n",
      "jdk-11.0.2/legal/jdk.javadoc/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.javadoc/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.javadoc/LICENSE\n",
      "jdk-11.0.2/legal/jdk.javadoc/jquery-migrate.md\n",
      "jdk-11.0.2/legal/jdk.javadoc/jquery.md\n",
      "jdk-11.0.2/legal/jdk.javadoc/jqueryUI.md\n",
      "jdk-11.0.2/legal/jdk.javadoc/jszip.md\n",
      "jdk-11.0.2/legal/jdk.javadoc/pako.md\n",
      "jdk-11.0.2/legal/jdk.jcmd/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.jcmd/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.jcmd/LICENSE\n",
      "jdk-11.0.2/legal/jdk.jconsole/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.jconsole/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.jconsole/LICENSE\n",
      "jdk-11.0.2/legal/jdk.jdeps/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.jdeps/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.jdeps/LICENSE\n",
      "jdk-11.0.2/legal/jdk.jdi/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.jdi/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.jdi/LICENSE\n",
      "jdk-11.0.2/legal/jdk.jdwp.agent/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.jdwp.agent/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.jdwp.agent/LICENSE\n",
      "jdk-11.0.2/legal/jdk.jfr/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.jfr/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.jfr/LICENSE\n",
      "jdk-11.0.2/legal/jdk.jlink/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.jlink/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.jlink/LICENSE\n",
      "jdk-11.0.2/legal/jdk.jshell/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.jshell/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.jshell/LICENSE\n",
      "jdk-11.0.2/legal/jdk.jsobject/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.jsobject/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.jsobject/LICENSE\n",
      "jdk-11.0.2/legal/jdk.jstatd/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.jstatd/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.jstatd/LICENSE\n",
      "jdk-11.0.2/legal/jdk.localedata/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.localedata/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.localedata/LICENSE\n",
      "jdk-11.0.2/legal/jdk.localedata/cldr.md\n",
      "jdk-11.0.2/legal/jdk.localedata/thaidict.md\n",
      "jdk-11.0.2/legal/jdk.management.agent/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.management.agent/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.management.agent/LICENSE\n",
      "jdk-11.0.2/legal/jdk.management.jfr/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.management.jfr/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.management.jfr/LICENSE\n",
      "jdk-11.0.2/legal/jdk.management/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.management/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.management/LICENSE\n",
      "jdk-11.0.2/legal/jdk.naming.dns/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.naming.dns/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.naming.dns/LICENSE\n",
      "jdk-11.0.2/legal/jdk.naming.rmi/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.naming.rmi/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.naming.rmi/LICENSE\n",
      "jdk-11.0.2/legal/jdk.net/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.net/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.net/LICENSE\n",
      "jdk-11.0.2/legal/jdk.pack/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.pack/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.pack/LICENSE\n",
      "jdk-11.0.2/legal/jdk.rmic/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.rmic/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.rmic/LICENSE\n",
      "jdk-11.0.2/legal/jdk.scripting.nashorn.shell/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.scripting.nashorn.shell/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.scripting.nashorn.shell/LICENSE\n",
      "jdk-11.0.2/legal/jdk.scripting.nashorn/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.scripting.nashorn/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.scripting.nashorn/LICENSE\n",
      "jdk-11.0.2/legal/jdk.scripting.nashorn/double-conversion.md\n",
      "jdk-11.0.2/legal/jdk.scripting.nashorn/joni.md\n",
      "jdk-11.0.2/legal/jdk.sctp/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.sctp/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.sctp/LICENSE\n",
      "jdk-11.0.2/legal/jdk.security.auth/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.security.auth/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.security.auth/LICENSE\n",
      "jdk-11.0.2/legal/jdk.security.jgss/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.security.jgss/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.security.jgss/LICENSE\n",
      "jdk-11.0.2/legal/jdk.unsupported.desktop/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.unsupported.desktop/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.unsupported.desktop/LICENSE\n",
      "jdk-11.0.2/legal/jdk.unsupported/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.unsupported/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.unsupported/LICENSE\n",
      "jdk-11.0.2/legal/jdk.xml.dom/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.xml.dom/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.xml.dom/LICENSE\n",
      "jdk-11.0.2/legal/jdk.zipfs/ADDITIONAL_LICENSE_INFO\n",
      "jdk-11.0.2/legal/jdk.zipfs/ASSEMBLY_EXCEPTION\n",
      "jdk-11.0.2/legal/jdk.zipfs/LICENSE\n",
      "jdk-11.0.2/lib/classlist\n",
      "jdk-11.0.2/lib/ct.sym\n",
      "jdk-11.0.2/lib/jexec\n",
      "jdk-11.0.2/lib/jfr/default.jfc\n",
      "jdk-11.0.2/lib/jfr/profile.jfc\n",
      "jdk-11.0.2/lib/jli/libjli.so\n",
      "jdk-11.0.2/lib/jrt-fs.jar\n",
      "jdk-11.0.2/lib/jvm.cfg\n",
      "jdk-11.0.2/lib/libattach.so\n",
      "jdk-11.0.2/lib/libawt.so\n",
      "jdk-11.0.2/lib/libawt_headless.so\n",
      "jdk-11.0.2/lib/libawt_xawt.so\n",
      "jdk-11.0.2/lib/libdt_socket.so\n",
      "jdk-11.0.2/lib/libextnet.so\n",
      "jdk-11.0.2/lib/libfontmanager.so\n",
      "jdk-11.0.2/lib/libinstrument.so\n",
      "jdk-11.0.2/lib/libj2gss.so\n",
      "jdk-11.0.2/lib/libj2pcsc.so\n",
      "jdk-11.0.2/lib/libj2pkcs11.so\n",
      "jdk-11.0.2/lib/libjaas.so\n",
      "jdk-11.0.2/lib/libjava.so\n",
      "jdk-11.0.2/lib/libjavajpeg.so\n",
      "jdk-11.0.2/lib/libjawt.so\n",
      "jdk-11.0.2/lib/libjdwp.so\n",
      "jdk-11.0.2/lib/libjimage.so\n",
      "jdk-11.0.2/lib/libjsig.so\n",
      "jdk-11.0.2/lib/libjsound.so\n",
      "jdk-11.0.2/lib/liblcms.so\n",
      "jdk-11.0.2/lib/libmanagement.so\n",
      "jdk-11.0.2/lib/libmanagement_agent.so\n",
      "jdk-11.0.2/lib/libmanagement_ext.so\n",
      "jdk-11.0.2/lib/libmlib_image.so\n",
      "jdk-11.0.2/lib/libnet.so\n",
      "jdk-11.0.2/lib/libnio.so\n",
      "jdk-11.0.2/lib/libprefs.so\n",
      "jdk-11.0.2/lib/librmi.so\n",
      "jdk-11.0.2/lib/libsaproc.so\n",
      "jdk-11.0.2/lib/libsctp.so\n",
      "jdk-11.0.2/lib/libsplashscreen.so\n",
      "jdk-11.0.2/lib/libsunec.so\n",
      "jdk-11.0.2/lib/libunpack.so\n",
      "jdk-11.0.2/lib/libverify.so\n",
      "jdk-11.0.2/lib/libzip.so\n",
      "jdk-11.0.2/lib/modules\n",
      "jdk-11.0.2/lib/psfont.properties.ja\n",
      "jdk-11.0.2/lib/psfontj2d.properties\n",
      "jdk-11.0.2/lib/security/blacklisted.certs\n",
      "jdk-11.0.2/lib/security/cacerts\n",
      "jdk-11.0.2/lib/security/default.policy\n",
      "jdk-11.0.2/lib/security/public_suffix_list.dat\n",
      "jdk-11.0.2/lib/server/Xusage.txt\n",
      "jdk-11.0.2/lib/server/libjsig.so\n",
      "jdk-11.0.2/lib/server/libjvm.so\n",
      "jdk-11.0.2/lib/src.zip\n",
      "jdk-11.0.2/lib/tzdb.dat\n",
      "jdk-11.0.2/release\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#%%capture\n",
    "!curl -O https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz\n",
    "!mv openjdk-11.0.2_linux-x64_bin.tar.gz /usr/lib/jvm/; cd /usr/lib/jvm/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n",
    "!update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-11.0.2/bin/java 1\n",
    "!update-alternatives --set java /usr/lib/jvm/jdk-11.0.2/bin/java\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/jdk-11.0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-U3oN2Ge3Tk",
    "outputId": "ef25e8bc-d0f8-44a6-93d2-348984f77c8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/root/kaggle’: File exists\n",
      "mkdir: cannot create directory ‘/root/kaggle/working/’: File exists\n",
      "mkdir: cannot create directory ‘/root/kaggle/working/sentence_wise_email/’: File exists\n",
      "mkdir: cannot create directory ‘/root/kaggle/working/sentence_wise_email/module/’: File exists\n",
      "mkdir: cannot create directory ‘/root/kaggle/working/sentence_wise_email/module/module_useT’: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "./\n",
      "./tfhub_module.pb\n",
      "./variables/\n",
      "./variables/variables.data-00000-of-00001\n",
      " 99  745M   99  744M    0     0  45.9M      0  0:00:16  0:00:16 --:--:-- 46.1M./variables/variables.index\n",
      "./assets/\n",
      "./saved_model.pb\n",
      "100  745M  100  745M    0     0  45.9M      0  0:00:16  0:00:16 --:--:-- 47.5M\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "!mkdir ~/kaggle\n",
    "!mkdir ~/kaggle/working/\n",
    "!mkdir ~/kaggle/working/sentence_wise_email/\n",
    "!mkdir ~/kaggle/working/sentence_wise_email/module/\n",
    "!mkdir ~/kaggle/working/sentence_wise_email/module/module_useT\n",
    "# Download the module, and uncompress it to the destination folder. \n",
    "!curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed\" | tar -zxvC ~/kaggle/working/sentence_wise_email/module/module_useT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RSVu4N7Pe6OK",
    "outputId": "3f57f506-1b17-4466-d3da-1f5054f5c255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "!pip install transformers\n",
    "!pip install --no-cache-dir transformers sentencepiece\n",
    "#from transformers import BertForQuestionAnswering\n",
    "#from transformers import BertTokenizer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "torch_device = 'cuda'\n",
    "\n",
    "#dmis-lab/biobert-base-cased-v1.1-squad\n",
    "#abhijithneilabraham/longformer_covid_qa\n",
    "#graviraja/covidbert_squad \n",
    "\n",
    "QA_TOKENIZER = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1-squad\")\n",
    "QA_MODEL = AutoModelForQuestionAnswering.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1-squad\")\n",
    "\n",
    "#QA_MODEL = AutoModelForQuestionAnswering.from_pretrained(\"NeuML/bert-small-cord19qa\")\n",
    "#QA_TOKENIZER = AutoTokenizer.from_pretrained(\"NeuML/bert-small-cord19qa\")\n",
    "\n",
    "#QA_MODEL = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "#QA_TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "QA_MODEL.to(torch_device)\n",
    "QA_MODEL.eval()\n",
    "#Auto Summarization with BART\n",
    "if USE_SUMMARY:\n",
    "    SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "    SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "    SUMMARY_MODEL.to(torch_device)\n",
    "    SUMMARY_MODEL.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BAh4RL0RecR",
    "outputId": "a39b4926-9cd5-428b-ec89-85ec0405c80d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyserini==0.8.1.0 in /usr/local/lib/python3.7/dist-packages (0.8.1.0)\n",
      "Requirement already satisfied: pyjnius in /usr/local/lib/python3.7/dist-packages (from pyserini==0.8.1.0) (1.3.0)\n",
      "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from pyserini==0.8.1.0) (0.29.23)\n",
      "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pyjnius->pyserini==0.8.1.0) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "!pip install pyserini==0.8.1.0\n",
    "from pyserini.search import pysearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjZfCGUbXYDI"
   },
   "outputs": [],
   "source": [
    "query_processed = 'As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?'\n",
    "query = 'As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?'\n",
    "\n",
    "keywords = '2019-nCoV, SARS-CoV-2, COVID-19, symptoms, hospitalization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TElPXtbMXKaI"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "searcher = pysearch.SimpleSearcher(luceneDir)\n",
    "hits = searcher.search(query_processed + '. ' + keywords,k=50) # We can retrieve the documents according to concatanated string of query and keywords we can change this setup.\n",
    "n_hits = len(hits)\n",
    "## collect the relevant data in a hit dictionary\n",
    "hit_dictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0lPvHURepbj"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "#'As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?'\n",
    "\n",
    "searcher = pysearch.SimpleSearcher(luceneDir)\n",
    "hits = searcher.search(query_processed + '. ' + keywords,k=50)\n",
    "n_hits = len(hits)\n",
    "## collect the relevant data in a hit dictionary\n",
    "hit_dictionary = {}\n",
    "for i in range(0, n_hits):\n",
    "  if 1==1:\n",
    "    doc_json=dict()\n",
    "    idx = str(hits[i].docid)\n",
    "    hit_dictionary[idx] = doc_json\n",
    "    hit_dictionary[idx]['abstract']=hits[i].lucene_document.get(\"raw\")\n",
    "    hit_dictionary[idx]['real_abstract']=hits[i].lucene_document.get(\"raw\")\n",
    "    hit_dictionary[idx]['title'] = str(hits[i].docid)\n",
    "\n",
    "## scrub the abstracts in prep for BERT-SQuAD\n",
    "for idx,v in hit_dictionary.items():\n",
    "    abs_dirty = v['abstract']\n",
    "    real_abs_dirty = v['real_abstract']\n",
    "    #abs_dirty = v['paragraph']\n",
    "    # looks like the abstract value can be an empty list\n",
    "    v['abstract_paragraphs'] = []\n",
    "    v['abstract_full'] = ''\n",
    "    v['real_abstract_full'] = ''\n",
    "    v['real_abstract_paragraphs']=[]\n",
    "\n",
    "    if abs_dirty:\n",
    "        # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
    "        # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
    "        # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
    "\n",
    "\n",
    "        if isinstance(abs_dirty, list):\n",
    "            for p in abs_dirty:\n",
    "                v['abstract_paragraphs'].append(p['text'])\n",
    "                v['abstract_full'] += p['text'] + ' \\n\\n'\n",
    "\n",
    "        # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
    "        if isinstance(abs_dirty, str):\n",
    "            v['abstract_paragraphs'].append(abs_dirty)\n",
    "            v['abstract_full'] += abs_dirty + ' \\n\\n'\n",
    "    if real_abs_dirty:\n",
    "        # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
    "        # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
    "        # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
    "\n",
    "\n",
    "        if isinstance(real_abs_dirty, list):\n",
    "            for p in real_abs_dirty:\n",
    "                v['real_abstract_paragraphs'].append(p['text'])\n",
    "                v['real_abstract_full'] += p['text'] + ' \\n\\n'\n",
    "\n",
    "        # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
    "        if isinstance(abs_dirty, str):\n",
    "            v['real_abstract_paragraphs'].append(real_abs_dirty)\n",
    "            v['real_abstract_full'] += real_abs_dirty + ' \\n\\n'\n",
    "    v[\"indexed_para\"]=v[\"abstract_full\"][len(v[\"real_abstract_full\"]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TZVSOX4XiFw",
    "outputId": "31e598fc-fd9e-4298-ef22-938209ce46c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "def embed_useT(module):\n",
    "    with tf.Graph().as_default():\n",
    "        sentences = tf.compat.v1.placeholder(tf.string)\n",
    "        embed = hub.Module(module)\n",
    "        embeddings = embed(sentences)\n",
    "        session = tf.compat.v1.train.MonitoredSession()\n",
    "    return lambda x: session.run(embeddings, {sentences: x})\n",
    "embed_fn = embed_useT('/root/kaggle/working/sentence_wise_email/module/module_useT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQ22nEB1rYdw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Reconstruct text for generating the answer text from the BERT result, do the necessary text preprocessing and construct the answer.\n",
    "def reconstructText(tokens, start=0, stop=-1):\n",
    "    tokens = tokens[start: stop]\n",
    "    if '[SEP]' in tokens:\n",
    "        sepind = tokens.index('[SEP]')\n",
    "        tokens = tokens[sepind+1:]\n",
    "    txt = ' '.join(tokens)\n",
    "    txt = txt.replace(' ##', '')\n",
    "    txt = txt.replace('##', '')\n",
    "    txt = txt.strip()\n",
    "    txt = \" \".join(txt.split())\n",
    "    txt = txt.replace(' .', '.')\n",
    "    txt = txt.replace('( ', '(')\n",
    "    txt = txt.replace(' )', ')')\n",
    "    txt = txt.replace(' - ', '-')\n",
    "    txt_list = txt.split(' , ')\n",
    "    txt = ''\n",
    "    nTxtL = len(txt_list)\n",
    "    if nTxtL == 1:\n",
    "        return txt_list[0]\n",
    "    newList =[]\n",
    "    for i,t in enumerate(txt_list):\n",
    "        if i < nTxtL -1:\n",
    "            if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n",
    "                newList += [t,',']\n",
    "            else:\n",
    "                newList += [t, ', ']\n",
    "        else:\n",
    "            newList += [t]\n",
    "    return ''.join(newList)\n",
    "\n",
    "\n",
    "def makeBERTSQuADPrediction(document, question):\n",
    "    ## we need to rewrite this function so that it chuncks the document into 250-300 word segments with\n",
    "    ## 50 word overlaps on either end so that it can understand and check longer abstracts\n",
    "    ## Get chuck of documents in order to overcome the BERT's max 512 token problem. First count the number of tokens. For example if it is\n",
    "    ##   larger than 2048 token determine the split size and overlapping token count. Split the document into pieces to fed into BERT encoding. Overlapping\n",
    "    ##      is necessary to maintain the coherence between the splits. If the token count is larger than some number 2560 token approximately this will fail.\n",
    "    nWords = len(document.split())\n",
    "    input_ids_all = QA_TOKENIZER.encode(question, document)\n",
    "    tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n",
    "    overlapFac = 1.1\n",
    "    if len(input_ids_all)*overlapFac > 2560:\n",
    "        nSearchWords = int(np.ceil(nWords/6))\n",
    "        fifth = int(np.ceil(nWords/5))\n",
    "        docSplit = document.split()\n",
    "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                     ' '.join(docSplit[fifth-int(nSearchWords*overlapFac/2):fifth+int(fifth*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[fifth*2-int(nSearchWords*overlapFac/2):fifth*2+int(fifth*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[fifth*3-int(nSearchWords*overlapFac/2):fifth*3+int(fifth*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[fifth*4-int(nSearchWords*overlapFac/2):fifth*4+int(fifth*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "\n",
    "    elif len(input_ids_all)*overlapFac > 2048:\n",
    "        nSearchWords = int(np.ceil(nWords/5))\n",
    "        quarter = int(np.ceil(nWords/4))\n",
    "        docSplit = document.split()\n",
    "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                     ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
    "        \n",
    "    elif len(input_ids_all)*overlapFac > 1536:\n",
    "        nSearchWords = int(np.ceil(nWords/4))\n",
    "        third = int(np.ceil(nWords/3))\n",
    "        docSplit = document.split()\n",
    "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                     ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
    "        \n",
    "    elif len(input_ids_all)*overlapFac > 1024:\n",
    "        nSearchWords = int(np.ceil(nWords/3))\n",
    "        middle = int(np.ceil(nWords/2))\n",
    "        docSplit = document.split()\n",
    "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                     ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "    elif len(input_ids_all)*overlapFac > 512:\n",
    "        nSearchWords = int(np.ceil(nWords/2))\n",
    "        docSplit = document.split()\n",
    "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "    else:\n",
    "        input_ids = [input_ids_all]\n",
    "    absTooLong = False    \n",
    "    \n",
    "    answers = []\n",
    "    cons = []\n",
    "    #print(input_ids)\n",
    "    for iptIds in input_ids:\n",
    "        tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n",
    "        #print(tokens)\n",
    "        sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)\n",
    "        num_seg_a = sep_index + 1\n",
    "        num_seg_b = len(iptIds) - num_seg_a\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "        assert len(segment_ids) == len(iptIds)\n",
    "        n_ids = len(segment_ids)\n",
    "        #print(n_ids)\n",
    "        if n_ids < 512:\n",
    "            outputs = QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
    "                                     token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n",
    "            #print(QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
    "             #                        token_type_ids=torch.tensor([segment_ids]).to(torch_device)))\n",
    "        else:\n",
    "            #this cuts off the text if its more than 512 words so it fits in model space\n",
    "            #need run multiple inferences for longer text. add to the todo. I will try to expand this to longer sequences if it is necessary. \n",
    "            print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n",
    "            absTooLong = True\n",
    "            outputs= QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device), \n",
    "                                     token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))\n",
    "        start_scores=outputs.start_logits\n",
    "        end_scores=outputs.end_logits\n",
    "        start_scores = start_scores[:,1:-1]\n",
    "        end_scores = end_scores[:,1:-1]\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "        #print(answer_start, answer_end)\n",
    "        answer = reconstructText(tokens, answer_start, answer_end+2)\n",
    "    \n",
    "        if answer.startswith('. ') or answer.startswith(', '):\n",
    "            answer = answer[2:]\n",
    "            \n",
    "        c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n",
    "        answers.append(answer)\n",
    "        cons.append(c)\n",
    "    \n",
    "    maxC = max(cons)\n",
    "    iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n",
    "    confidence = cons[iMaxC]\n",
    "    answer = answers[iMaxC]\n",
    "    \n",
    "    sep_index = tokens_all.index('[SEP]')\n",
    "    full_txt_tokens = tokens_all[sep_index+1:]\n",
    "    \n",
    "    abs_returned = reconstructText(full_txt_tokens)\n",
    "\n",
    "    ans={}\n",
    "    ans['answer'] = answer\n",
    "    #print(answer)\n",
    "    if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n",
    "        ans['confidence'] = -1000000\n",
    "    else:\n",
    "        #confidence = torch.max(start_scores) + torch.max(end_scores)\n",
    "        #confidence = np.log(confidence.item())\n",
    "        ans['confidence'] = confidence\n",
    "    #ans['start'] = answer_start.item()\n",
    "    #ans['end'] = answer_end.item()\n",
    "    ans['abstract_bert'] = abs_returned\n",
    "    ans['abs_too_long'] = absTooLong\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKNQBMURrbv9"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def searchAbstracts(hit_dictionary, question):\n",
    "    abstractResults = {}\n",
    "    for k,v in tqdm(hit_dictionary.items()):\n",
    "        abstract = v['abstract_full']\n",
    "        indexed_para=v['indexed_para']\n",
    "        if abstract:\n",
    "            ans = makeBERTSQuADPrediction(abstract, question)\n",
    "            if ans['answer']:\n",
    "                confidence = ans['confidence']\n",
    "                abstractResults[confidence]={}\n",
    "                abstractResults[confidence]['answer'] = ans['answer']\n",
    "                #abstractResults[confidence]['start'] = ans['start']\n",
    "                #abstractResults[confidence]['end'] = ans['end']\n",
    "                abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n",
    "                abstractResults[confidence]['idx'] = k\n",
    "                abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n",
    "\n",
    "                \n",
    "    cList = list(abstractResults.keys())\n",
    "\n",
    "    if cList:\n",
    "        maxScore = max(cList)\n",
    "        total = 0.0\n",
    "        exp_scores = []\n",
    "        for c in cList:\n",
    "            s = np.exp(c-maxScore)\n",
    "            exp_scores.append(s)\n",
    "        total = sum(exp_scores)\n",
    "        for i,c in enumerate(cList):\n",
    "            abstractResults[exp_scores[i]/total] = abstractResults.pop(c)\n",
    "    return abstractResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KZsPI-2PreQC",
    "outputId": "f34dfce9-5037-4e3f-beea-d2f6a2bf6f42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 3836 words long.  There are 1552 tokens\n",
      "****** warning only considering first 512 tokens, document is 3836 words long.  There are 1730 tokens\n",
      "****** warning only considering first 512 tokens, document is 3836 words long.  There are 1409 tokens\n",
      "****** warning only considering first 512 tokens, document is 3836 words long.  There are 1689 tokens\n",
      "****** warning only considering first 512 tokens, document is 3836 words long.  There are 1800 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:29,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 3836 words long.  There are 1951 tokens\n",
      "****** warning only considering first 512 tokens, document is 2444 words long.  There are 718 tokens\n",
      "****** warning only considering first 512 tokens, document is 2444 words long.  There are 781 tokens\n",
      "****** warning only considering first 512 tokens, document is 2444 words long.  There are 806 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:00<00:24,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2444 words long.  There are 720 tokens\n",
      "****** warning only considering first 512 tokens, document is 2444 words long.  There are 909 tokens\n",
      "****** warning only considering first 512 tokens, document is 2444 words long.  There are 893 tokens\n",
      "****** warning only considering first 512 tokens, document is 2371 words long.  There are 957 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:01<00:21,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2371 words long.  There are 945 tokens\n",
      "****** warning only considering first 512 tokens, document is 2371 words long.  There are 917 tokens\n",
      "****** warning only considering first 512 tokens, document is 2371 words long.  There are 810 tokens\n",
      "****** warning only considering first 512 tokens, document is 2371 words long.  There are 731 tokens\n",
      "****** warning only considering first 512 tokens, document is 2371 words long.  There are 692 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:01<00:18,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2418 words long.  There are 856 tokens\n",
      "****** warning only considering first 512 tokens, document is 2418 words long.  There are 1287 tokens\n",
      "****** warning only considering first 512 tokens, document is 2418 words long.  There are 927 tokens\n",
      "****** warning only considering first 512 tokens, document is 2418 words long.  There are 1097 tokens\n",
      "****** warning only considering first 512 tokens, document is 2418 words long.  There are 842 tokens\n",
      "****** warning only considering first 512 tokens, document is 2418 words long.  There are 841 tokens\n",
      "****** warning only considering first 512 tokens, document is 1727 words long.  There are 617 tokens\n",
      "****** warning only considering first 512 tokens, document is 1727 words long.  There are 690 tokens\n",
      "****** warning only considering first 512 tokens, document is 1727 words long.  There are 569 tokens\n",
      "****** warning only considering first 512 tokens, document is 1727 words long.  There are 595 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:01<00:17,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1727 words long.  There are 579 tokens\n",
      "****** warning only considering first 512 tokens, document is 1727 words long.  There are 561 tokens\n",
      "****** warning only considering first 512 tokens, document is 7618 words long.  There are 2604 tokens\n",
      "****** warning only considering first 512 tokens, document is 7618 words long.  There are 2820 tokens\n",
      "****** warning only considering first 512 tokens, document is 7618 words long.  There are 2964 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:03<00:26,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 7618 words long.  There are 4308 tokens\n",
      "****** warning only considering first 512 tokens, document is 7618 words long.  There are 3558 tokens\n",
      "****** warning only considering first 512 tokens, document is 7618 words long.  There are 2854 tokens\n",
      "****** warning only considering first 512 tokens, document is 4168 words long.  There are 1130 tokens\n",
      "****** warning only considering first 512 tokens, document is 4168 words long.  There are 1110 tokens\n",
      "****** warning only considering first 512 tokens, document is 4168 words long.  There are 1200 tokens\n",
      "****** warning only considering first 512 tokens, document is 4168 words long.  There are 1298 tokens\n",
      "****** warning only considering first 512 tokens, document is 4168 words long.  There are 1277 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:03<00:24,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 4168 words long.  There are 1465 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:03<00:18,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 7193 words long.  There are 2477 tokens\n",
      "****** warning only considering first 512 tokens, document is 7193 words long.  There are 2517 tokens\n",
      "****** warning only considering first 512 tokens, document is 7193 words long.  There are 3200 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:04<00:21,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 7193 words long.  There are 3763 tokens\n",
      "****** warning only considering first 512 tokens, document is 7193 words long.  There are 3001 tokens\n",
      "****** warning only considering first 512 tokens, document is 7193 words long.  There are 2549 tokens\n",
      "****** warning only considering first 512 tokens, document is 1810 words long.  There are 573 tokens\n",
      "****** warning only considering first 512 tokens, document is 1810 words long.  There are 702 tokens\n",
      "****** warning only considering first 512 tokens, document is 1810 words long.  There are 572 tokens\n",
      "****** warning only considering first 512 tokens, document is 1810 words long.  There are 640 tokens\n",
      "****** warning only considering first 512 tokens, document is 1810 words long.  There are 593 tokens\n",
      "****** warning only considering first 512 tokens, document is 1810 words long.  There are 532 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:04<00:18,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2089 words long.  There are 829 tokens\n",
      "****** warning only considering first 512 tokens, document is 2089 words long.  There are 945 tokens\n",
      "****** warning only considering first 512 tokens, document is 2089 words long.  There are 817 tokens\n",
      "****** warning only considering first 512 tokens, document is 2089 words long.  There are 703 tokens\n",
      "****** warning only considering first 512 tokens, document is 2089 words long.  There are 748 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:05<00:16,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2089 words long.  There are 815 tokens\n",
      "****** warning only considering first 512 tokens, document is 2207 words long.  There are 767 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [00:05<00:16,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2207 words long.  There are 821 tokens\n",
      "****** warning only considering first 512 tokens, document is 2207 words long.  There are 860 tokens\n",
      "****** warning only considering first 512 tokens, document is 2207 words long.  There are 902 tokens\n",
      "****** warning only considering first 512 tokens, document is 2207 words long.  There are 800 tokens\n",
      "****** warning only considering first 512 tokens, document is 2207 words long.  There are 704 tokens\n",
      "****** warning only considering first 512 tokens, document is 3448 words long.  There are 1808 tokens\n",
      "****** warning only considering first 512 tokens, document is 3448 words long.  There are 1673 tokens\n",
      "****** warning only considering first 512 tokens, document is 3448 words long.  There are 1371 tokens\n",
      "****** warning only considering first 512 tokens, document is 3448 words long.  There are 1264 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [00:06<00:17,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 3448 words long.  There are 1419 tokens\n",
      "****** warning only considering first 512 tokens, document is 3448 words long.  There are 1468 tokens\n",
      "****** warning only considering first 512 tokens, document is 1806 words long.  There are 879 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [00:06<00:15,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1806 words long.  There are 899 tokens\n",
      "****** warning only considering first 512 tokens, document is 1806 words long.  There are 742 tokens\n",
      "****** warning only considering first 512 tokens, document is 1806 words long.  There are 803 tokens\n",
      "****** warning only considering first 512 tokens, document is 1806 words long.  There are 669 tokens\n",
      "****** warning only considering first 512 tokens, document is 1806 words long.  There are 756 tokens\n",
      "****** warning only considering first 512 tokens, document is 7944 words long.  There are 2575 tokens\n",
      "****** warning only considering first 512 tokens, document is 7944 words long.  There are 2889 tokens\n",
      "****** warning only considering first 512 tokens, document is 7944 words long.  There are 2804 tokens\n",
      "****** warning only considering first 512 tokens, document is 7944 words long.  There are 2902 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [00:06<00:16,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 7944 words long.  There are 2879 tokens\n",
      "****** warning only considering first 512 tokens, document is 7944 words long.  There are 2818 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [00:07<00:15,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2398 words long.  There are 1210 tokens\n",
      "****** warning only considering first 512 tokens, document is 2398 words long.  There are 1158 tokens\n",
      "****** warning only considering first 512 tokens, document is 2398 words long.  There are 1249 tokens\n",
      "****** warning only considering first 512 tokens, document is 2398 words long.  There are 1117 tokens\n",
      "****** warning only considering first 512 tokens, document is 2398 words long.  There are 1003 tokens\n",
      "****** warning only considering first 512 tokens, document is 2398 words long.  There are 944 tokens\n",
      "****** warning only considering first 512 tokens, document is 5045 words long.  There are 1878 tokens\n",
      "****** warning only considering first 512 tokens, document is 5045 words long.  There are 2094 tokens\n",
      "****** warning only considering first 512 tokens, document is 5045 words long.  There are 1678 tokens\n",
      "****** warning only considering first 512 tokens, document is 5045 words long.  There are 1737 tokens\n",
      "****** warning only considering first 512 tokens, document is 5045 words long.  There are 2014 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [00:07<00:15,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 5045 words long.  There are 1670 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [00:08<00:14,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 5009 words long.  There are 1669 tokens\n",
      "****** warning only considering first 512 tokens, document is 5009 words long.  There are 1612 tokens\n",
      "****** warning only considering first 512 tokens, document is 5009 words long.  There are 1634 tokens\n",
      "****** warning only considering first 512 tokens, document is 5009 words long.  There are 1609 tokens\n",
      "****** warning only considering first 512 tokens, document is 5009 words long.  There are 2051 tokens\n",
      "****** warning only considering first 512 tokens, document is 5009 words long.  There are 2058 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [00:08<00:13,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 4010 words long.  There are 1395 tokens\n",
      "****** warning only considering first 512 tokens, document is 4010 words long.  There are 1500 tokens\n",
      "****** warning only considering first 512 tokens, document is 4010 words long.  There are 1332 tokens\n",
      "****** warning only considering first 512 tokens, document is 4010 words long.  There are 1584 tokens\n",
      "****** warning only considering first 512 tokens, document is 4010 words long.  There are 1178 tokens\n",
      "****** warning only considering first 512 tokens, document is 4010 words long.  There are 1207 tokens\n",
      "****** warning only considering first 512 tokens, document is 7149 words long.  There are 2153 tokens\n",
      "****** warning only considering first 512 tokens, document is 7149 words long.  There are 2382 tokens\n",
      "****** warning only considering first 512 tokens, document is 7149 words long.  There are 2585 tokens\n",
      "****** warning only considering first 512 tokens, document is 7149 words long.  There are 2051 tokens\n",
      "****** warning only considering first 512 tokens, document is 7149 words long.  There are 2075 tokens\n",
      "****** warning only considering first 512 tokens, document is 7149 words long.  There are 1993 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [00:08<00:12,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 8454 words long.  There are 2937 tokens\n",
      "****** warning only considering first 512 tokens, document is 8454 words long.  There are 3178 tokens\n",
      "****** warning only considering first 512 tokens, document is 8454 words long.  There are 3584 tokens\n",
      "****** warning only considering first 512 tokens, document is 8454 words long.  There are 3034 tokens\n",
      "****** warning only considering first 512 tokens, document is 8454 words long.  There are 3350 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:09<00:12,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 8454 words long.  There are 2866 tokens\n",
      "****** warning only considering first 512 tokens, document is 1964 words long.  There are 816 tokens\n",
      "****** warning only considering first 512 tokens, document is 1964 words long.  There are 875 tokens\n",
      "****** warning only considering first 512 tokens, document is 1964 words long.  There are 1026 tokens\n",
      "****** warning only considering first 512 tokens, document is 1964 words long.  There are 821 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [00:09<00:10,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1964 words long.  There are 959 tokens\n",
      "****** warning only considering first 512 tokens, document is 1964 words long.  There are 850 tokens\n",
      "****** warning only considering first 512 tokens, document is 4683 words long.  There are 1527 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [00:10<00:09,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 4683 words long.  There are 1311 tokens\n",
      "****** warning only considering first 512 tokens, document is 4683 words long.  There are 1492 tokens\n",
      "****** warning only considering first 512 tokens, document is 4683 words long.  There are 1400 tokens\n",
      "****** warning only considering first 512 tokens, document is 4683 words long.  There are 1522 tokens\n",
      "****** warning only considering first 512 tokens, document is 4683 words long.  There are 1903 tokens\n",
      "****** warning only considering first 512 tokens, document is 11065 words long.  There are 5532 tokens\n",
      "****** warning only considering first 512 tokens, document is 11065 words long.  There are 4599 tokens\n",
      "****** warning only considering first 512 tokens, document is 11065 words long.  There are 5160 tokens\n",
      "****** warning only considering first 512 tokens, document is 11065 words long.  There are 5243 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [00:10<00:11,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 11065 words long.  There are 4599 tokens\n",
      "****** warning only considering first 512 tokens, document is 11065 words long.  There are 4199 tokens\n",
      "****** warning only considering first 512 tokens, document is 7625 words long.  There are 2769 tokens\n",
      "****** warning only considering first 512 tokens, document is 7625 words long.  There are 2994 tokens\n",
      "****** warning only considering first 512 tokens, document is 7625 words long.  There are 3699 tokens\n",
      "****** warning only considering first 512 tokens, document is 7625 words long.  There are 3331 tokens\n",
      "****** warning only considering first 512 tokens, document is 7625 words long.  There are 3258 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [00:11<00:11,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 7625 words long.  There are 3262 tokens\n",
      "****** warning only considering first 512 tokens, document is 1821 words long.  There are 852 tokens\n",
      "****** warning only considering first 512 tokens, document is 1821 words long.  There are 628 tokens\n",
      "****** warning only considering first 512 tokens, document is 1821 words long.  There are 613 tokens\n",
      "****** warning only considering first 512 tokens, document is 1821 words long.  There are 644 tokens\n",
      "****** warning only considering first 512 tokens, document is 1821 words long.  There are 630 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [00:11<00:09,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1821 words long.  There are 634 tokens\n",
      "****** warning only considering first 512 tokens, document is 1828 words long.  There are 667 tokens\n",
      "****** warning only considering first 512 tokens, document is 1828 words long.  There are 674 tokens\n",
      "****** warning only considering first 512 tokens, document is 1828 words long.  There are 649 tokens\n",
      "****** warning only considering first 512 tokens, document is 1828 words long.  There are 641 tokens\n",
      "****** warning only considering first 512 tokens, document is 1828 words long.  There are 671 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [00:11<00:07,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1828 words long.  There are 776 tokens\n",
      "****** warning only considering first 512 tokens, document is 4491 words long.  There are 1495 tokens\n",
      "****** warning only considering first 512 tokens, document is 4491 words long.  There are 1640 tokens\n",
      "****** warning only considering first 512 tokens, document is 4491 words long.  There are 1549 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [00:11<00:07,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 4491 words long.  There are 1540 tokens\n",
      "****** warning only considering first 512 tokens, document is 4491 words long.  There are 1475 tokens\n",
      "****** warning only considering first 512 tokens, document is 4491 words long.  There are 1356 tokens\n",
      "****** warning only considering first 512 tokens, document is 1003 words long.  There are 589 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [00:12<00:05,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1003 words long.  There are 715 tokens\n",
      "****** warning only considering first 512 tokens, document is 1003 words long.  There are 719 tokens\n",
      "****** warning only considering first 512 tokens, document is 1003 words long.  There are 657 tokens\n",
      "****** warning only considering first 512 tokens, document is 7594 words long.  There are 2731 tokens\n",
      "****** warning only considering first 512 tokens, document is 7594 words long.  There are 3183 tokens\n",
      "****** warning only considering first 512 tokens, document is 7594 words long.  There are 2489 tokens\n",
      "****** warning only considering first 512 tokens, document is 7594 words long.  There are 2683 tokens\n",
      "****** warning only considering first 512 tokens, document is 7594 words long.  There are 2820 tokens\n",
      "****** warning only considering first 512 tokens, document is 7594 words long.  There are 2310 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:12<00:05,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2474 words long.  There are 699 tokens\n",
      "****** warning only considering first 512 tokens, document is 2474 words long.  There are 732 tokens\n",
      "****** warning only considering first 512 tokens, document is 2474 words long.  There are 802 tokens\n",
      "****** warning only considering first 512 tokens, document is 2474 words long.  There are 806 tokens\n",
      "****** warning only considering first 512 tokens, document is 2474 words long.  There are 940 tokens\n",
      "****** warning only considering first 512 tokens, document is 2474 words long.  There are 709 tokens\n",
      "****** warning only considering first 512 tokens, document is 6402 words long.  There are 1941 tokens\n",
      "****** warning only considering first 512 tokens, document is 6402 words long.  There are 1947 tokens\n",
      "****** warning only considering first 512 tokens, document is 6402 words long.  There are 1904 tokens\n",
      "****** warning only considering first 512 tokens, document is 6402 words long.  There are 2067 tokens\n",
      "****** warning only considering first 512 tokens, document is 6402 words long.  There are 2118 tokens\n",
      "****** warning only considering first 512 tokens, document is 6402 words long.  There are 1933 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [00:13<00:04,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1788 words long.  There are 547 tokens\n",
      "****** warning only considering first 512 tokens, document is 1788 words long.  There are 535 tokens\n",
      "****** warning only considering first 512 tokens, document is 1788 words long.  There are 562 tokens\n",
      "****** warning only considering first 512 tokens, document is 1788 words long.  There are 548 tokens\n",
      "****** warning only considering first 512 tokens, document is 1788 words long.  There are 575 tokens\n",
      "****** warning only considering first 512 tokens, document is 1736 words long.  There are 655 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [00:13<00:04,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1736 words long.  There are 693 tokens\n",
      "****** warning only considering first 512 tokens, document is 1736 words long.  There are 650 tokens\n",
      "****** warning only considering first 512 tokens, document is 1736 words long.  There are 685 tokens\n",
      "****** warning only considering first 512 tokens, document is 1736 words long.  There are 595 tokens\n",
      "****** warning only considering first 512 tokens, document is 1736 words long.  There are 534 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [00:13<00:03,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 3737 words long.  There are 1381 tokens\n",
      "****** warning only considering first 512 tokens, document is 3737 words long.  There are 1397 tokens\n",
      "****** warning only considering first 512 tokens, document is 3737 words long.  There are 1401 tokens\n",
      "****** warning only considering first 512 tokens, document is 3737 words long.  There are 1390 tokens\n",
      "****** warning only considering first 512 tokens, document is 3737 words long.  There are 1351 tokens\n",
      "****** warning only considering first 512 tokens, document is 3737 words long.  There are 1251 tokens\n",
      "****** warning only considering first 512 tokens, document is 4583 words long.  There are 1667 tokens\n",
      "****** warning only considering first 512 tokens, document is 4583 words long.  There are 1915 tokens\n",
      "****** warning only considering first 512 tokens, document is 4583 words long.  There are 1918 tokens\n",
      "****** warning only considering first 512 tokens, document is 4583 words long.  There are 2129 tokens\n",
      "****** warning only considering first 512 tokens, document is 4583 words long.  There are 2360 tokens\n",
      "****** warning only considering first 512 tokens, document is 4583 words long.  There are 1764 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [00:14<00:03,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 3595 words long.  There are 1338 tokens\n",
      "****** warning only considering first 512 tokens, document is 3595 words long.  There are 1256 tokens\n",
      "****** warning only considering first 512 tokens, document is 3595 words long.  There are 1454 tokens\n",
      "****** warning only considering first 512 tokens, document is 3595 words long.  There are 1412 tokens\n",
      "****** warning only considering first 512 tokens, document is 3595 words long.  There are 1452 tokens\n",
      "****** warning only considering first 512 tokens, document is 3595 words long.  There are 1711 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [00:14<00:03,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1988 words long.  There are 808 tokens\n",
      "****** warning only considering first 512 tokens, document is 1988 words long.  There are 853 tokens\n",
      "****** warning only considering first 512 tokens, document is 1988 words long.  There are 667 tokens\n",
      "****** warning only considering first 512 tokens, document is 1988 words long.  There are 873 tokens\n",
      "****** warning only considering first 512 tokens, document is 1988 words long.  There are 975 tokens\n",
      "****** warning only considering first 512 tokens, document is 1988 words long.  There are 813 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [00:14<00:02,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2204 words long.  There are 930 tokens\n",
      "****** warning only considering first 512 tokens, document is 2204 words long.  There are 1276 tokens\n",
      "****** warning only considering first 512 tokens, document is 2204 words long.  There are 1093 tokens\n",
      "****** warning only considering first 512 tokens, document is 2204 words long.  There are 1013 tokens\n",
      "****** warning only considering first 512 tokens, document is 2204 words long.  There are 1145 tokens\n",
      "****** warning only considering first 512 tokens, document is 2204 words long.  There are 992 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [00:15<00:02,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 3554 words long.  There are 1138 tokens\n",
      "****** warning only considering first 512 tokens, document is 3554 words long.  There are 1338 tokens\n",
      "****** warning only considering first 512 tokens, document is 3554 words long.  There are 1387 tokens\n",
      "****** warning only considering first 512 tokens, document is 3554 words long.  There are 1230 tokens\n",
      "****** warning only considering first 512 tokens, document is 3554 words long.  There are 1267 tokens\n",
      "****** warning only considering first 512 tokens, document is 3554 words long.  There are 1462 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:15<00:02,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1270 words long.  There are 529 tokens\n",
      "****** warning only considering first 512 tokens, document is 1270 words long.  There are 525 tokens\n",
      "****** warning only considering first 512 tokens, document is 1270 words long.  There are 656 tokens\n",
      "****** warning only considering first 512 tokens, document is 1270 words long.  There are 583 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [00:15<00:01,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1243 words long.  There are 524 tokens\n",
      "****** warning only considering first 512 tokens, document is 1243 words long.  There are 573 tokens\n",
      "****** warning only considering first 512 tokens, document is 2435 words long.  There are 1111 tokens\n",
      "****** warning only considering first 512 tokens, document is 2435 words long.  There are 1016 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [00:15<00:01,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2435 words long.  There are 870 tokens\n",
      "****** warning only considering first 512 tokens, document is 2435 words long.  There are 930 tokens\n",
      "****** warning only considering first 512 tokens, document is 2435 words long.  There are 902 tokens\n",
      "****** warning only considering first 512 tokens, document is 2435 words long.  There are 903 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [00:15<00:01,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 4040 words long.  There are 1839 tokens\n",
      "****** warning only considering first 512 tokens, document is 4040 words long.  There are 1708 tokens\n",
      "****** warning only considering first 512 tokens, document is 4040 words long.  There are 1528 tokens\n",
      "****** warning only considering first 512 tokens, document is 4040 words long.  There are 1173 tokens\n",
      "****** warning only considering first 512 tokens, document is 4040 words long.  There are 1491 tokens\n",
      "****** warning only considering first 512 tokens, document is 4040 words long.  There are 1390 tokens\n",
      "****** warning only considering first 512 tokens, document is 8423 words long.  There are 3119 tokens\n",
      "****** warning only considering first 512 tokens, document is 8423 words long.  There are 3034 tokens\n",
      "****** warning only considering first 512 tokens, document is 8423 words long.  There are 3003 tokens\n",
      "****** warning only considering first 512 tokens, document is 8423 words long.  There are 2737 tokens\n",
      "****** warning only considering first 512 tokens, document is 8423 words long.  There are 2848 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [00:16<00:01,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 8423 words long.  There are 3022 tokens\n",
      "****** warning only considering first 512 tokens, document is 1563 words long.  There are 687 tokens\n",
      "****** warning only considering first 512 tokens, document is 1563 words long.  There are 726 tokens\n",
      "****** warning only considering first 512 tokens, document is 1563 words long.  There are 649 tokens\n",
      "****** warning only considering first 512 tokens, document is 1563 words long.  There are 640 tokens\n",
      "****** warning only considering first 512 tokens, document is 1563 words long.  There are 697 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [00:16<00:00,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1563 words long.  There are 696 tokens\n",
      "****** warning only considering first 512 tokens, document is 6984 words long.  There are 2972 tokens\n",
      "****** warning only considering first 512 tokens, document is 6984 words long.  There are 3111 tokens\n",
      "****** warning only considering first 512 tokens, document is 6984 words long.  There are 2795 tokens\n",
      "****** warning only considering first 512 tokens, document is 6984 words long.  There are 2494 tokens\n",
      "****** warning only considering first 512 tokens, document is 6984 words long.  There are 2788 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [00:17<00:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 6984 words long.  There are 2726 tokens\n",
      "****** warning only considering first 512 tokens, document is 4758 words long.  There are 1572 tokens\n",
      "****** warning only considering first 512 tokens, document is 4758 words long.  There are 1780 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [00:17<00:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 4758 words long.  There are 1771 tokens\n",
      "****** warning only considering first 512 tokens, document is 4758 words long.  There are 1490 tokens\n",
      "****** warning only considering first 512 tokens, document is 4758 words long.  There are 1613 tokens\n",
      "****** warning only considering first 512 tokens, document is 4758 words long.  There are 1495 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 4507 words long.  There are 1286 tokens\n",
      "****** warning only considering first 512 tokens, document is 4507 words long.  There are 1331 tokens\n",
      "****** warning only considering first 512 tokens, document is 4507 words long.  There are 1371 tokens\n",
      "****** warning only considering first 512 tokens, document is 4507 words long.  There are 1404 tokens\n",
      "****** warning only considering first 512 tokens, document is 4507 words long.  There are 1466 tokens\n",
      "****** warning only considering first 512 tokens, document is 4507 words long.  There are 1450 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answers = searchAbstracts(hit_dictionary, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2m86xqzSOO8A"
   },
   "outputs": [],
   "source": [
    "FIND_PDFS=False \n",
    "SEARCH_MEDRXIV=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HV9gJtmqY9ic"
   },
   "outputs": [],
   "source": [
    "workingPath = '/root/kaggle/working'\n",
    "import pandas as pd\n",
    "import re\n",
    "if FIND_PDFS:\n",
    "    from metapub import UrlReverse\n",
    "    from metapub import FindIt\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "#from summarizer import Summarizer\n",
    "#summarizerModel = Summarizer()\n",
    "def displayResults(hit_dictionary, answers, question):\n",
    "    \n",
    "    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n",
    "    #all_HTML_txt = question_HTML\n",
    "    confidence = list(answers.keys())\n",
    "    confidence.sort(reverse=True)\n",
    "    \n",
    "    confidence = list(answers.keys())\n",
    "    confidence.sort(reverse=True)\n",
    "    \n",
    "\n",
    "    for c in confidence:\n",
    "        if c>0 and c <= 1 and len(answers[c]['answer']) != 0:\n",
    "            if 'idx' not in  answers[c]:\n",
    "                continue\n",
    "            rowData = []\n",
    "            idx = answers[c]['idx']\n",
    "            title = hit_dictionary[idx]['title']\n",
    "\n",
    "            \n",
    "            full_abs = answers[c]['abstract_bert']\n",
    "            bert_ans = answers[c]['answer']\n",
    "            \n",
    "            \n",
    "            #split_abs = full_abs.split(bert_ans)\n",
    "            #x=(split_abs[0][:split_abs[0].rfind('. ')+1])\n",
    "            #sentance_beginning = x[x.rfind('. ')+1:] + split_abs[0][split_abs[0].rfind('. ')+1:]\n",
    "            #sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n",
    "            x=''\n",
    "            y=''\n",
    "            z=''\n",
    "            t=''\n",
    "            regex = r\"(\\.\\s[^0-9])(?!.*(\\.\\s[^0-9]))\"\n",
    "            split_abs = full_abs.split(bert_ans)\n",
    "            matches = re.finditer(regex, split_abs[0], re.MULTILINE)\n",
    "            for matchNum1, match in enumerate(matches, start=1):\n",
    "              y=(split_abs[0][:match.start()+1])\n",
    "            matches = re.finditer(regex, y, re.MULTILINE)\n",
    "            for matchNum2, match in enumerate(matches, start=1):\n",
    "              x=(y[match.start()+1:])\n",
    "            if x=='':\n",
    "              x=split_abs[0]\n",
    "            matches = re.finditer(regex, split_abs[0], re.MULTILINE)\n",
    "            for matchNum3, match in enumerate(matches, start=1):\n",
    "              z=(split_abs[0][match.start()+1:])  \n",
    "            sentance_beginning = x+z\n",
    "            regex2=r\"(.*?(?<!\\b\\w)[.?!])\\s+[a-zA-Z0-9]\"\n",
    "            if len(split_abs) == 1:\n",
    "                sentance_end_pos = len(full_abs)\n",
    "                sentance_end =''\n",
    "            else:\n",
    "                matches = re.finditer(regex2, split_abs[1], re.MULTILINE)\n",
    "                for matchNum4, match in enumerate(matches, start=1):\n",
    "                    if matchNum4==1:\n",
    "                        t=(split_abs[1][:match.end()-2])\n",
    "                sentance_end_pos = split_abs[1].find('. ')+1\n",
    "                if sentance_end_pos == 0:\n",
    "                    sentance_end = split_abs[1]\n",
    "                else:\n",
    "                    sentance_end = t\n",
    "            #if len(split_abs) == 1:\n",
    "            #    sentance_end_pos = len(full_abs)\n",
    "            #    sentance_end =''\n",
    "            #else:\n",
    "            #    sentance_end_pos = split_abs[1].find('. ')+1\n",
    "            #    if sentance_end_pos == 0:\n",
    "            #      sentance_end = split_abs[1]\n",
    "            #    else:\n",
    "            #      sentance_end = split_abs[1][:sentance_end_pos]\n",
    "                \n",
    "            #sentance_full = sentance_beginning + bert_ans+ sentance_end\n",
    "            answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n",
    "            answers[c]['partial_answer'] = bert_ans+sentance_end\n",
    "            answers[c]['sentence_beginning'] = sentance_beginning\n",
    "            answers[c]['sentence_end'] = sentance_end\n",
    "            answers[c]['title'] = title\n",
    "        else:\n",
    "            answers.pop(c)\n",
    "    \n",
    "    \n",
    "    ## now rerank based on semantic similarity of the answers to the question\n",
    "    ## Universal sentence encoder\n",
    "    cList = list(answers.keys())\n",
    "    allAnswers = [answers[c]['full_answer'] for c in cList]\n",
    "    \n",
    "    messages = [question]+allAnswers\n",
    "    \n",
    "    encoding_matrix = embed_fn(messages)\n",
    "    similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n",
    "    rankings = similarity_matrix[1:,0]\n",
    "    \n",
    "    for i,c in enumerate(cList):\n",
    "        answers[rankings[i]] = answers.pop(c)\n",
    "\n",
    "    ## now form pandas dv\n",
    "    confidence = list(answers.keys())\n",
    "    confidence.sort(reverse=True)\n",
    "    pandasData = []\n",
    "    ranked_aswers = []\n",
    "    full_abs_list=[]\n",
    "    for c in confidence:\n",
    "        rowData=[]\n",
    "        title = answers[c]['title']\n",
    "        idx = answers[c]['idx']\n",
    "        rowData += [idx]            \n",
    "        sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='red'>\"+answers[c]['answer']+\"</font> \"+answers[c]['sentence_end']+'</div>'\n",
    "        \n",
    "        rowData += [sentance_html, c,title]\n",
    "        pandasData.append(rowData)\n",
    "        ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n",
    "        full_abs_list.append(' '.join([answers[c]['abstract_bert']]))\n",
    "    \n",
    "    if FIND_PDFS or SEARCH_MEDRXIV:\n",
    "        pdata2 = []\n",
    "        pm_ids = []\n",
    "        for rowData in pandasData:\n",
    "            rd = rowData\n",
    "            idx = rowData[0]\n",
    "    else:\n",
    "        pdata2 = pandasData\n",
    "        \n",
    "    \n",
    "    display(HTML(question_HTML))\n",
    "\n",
    "    if USE_SUMMARY:\n",
    "        ## try generating an exacutive summary with bart abstractive summarizer\n",
    "        allAnswersTxt = ' '.join(ranked_aswers[:10]).replace('\\n','')\n",
    "    #    exec_sum = summarizerModel(allAnswersTxt, min_length=1, max_length=500)    \n",
    "     #   execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>BERT Extractive Summary:</b>: '+exec_sum+'</div>'\n",
    "\n",
    "        answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=2048)['input_ids'].to(torch_device)\n",
    "        #answers_input_ids = SUMMARY_TOKENIZER.prepare_seq2seq_batch([allAnswersTxt], return_tensors='pt', truncation=True, padding='longest')['input_ids'].to(torch_device)\n",
    "        summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n",
    "                                               num_beams=10,\n",
    "                                               length_penalty=1.1,\n",
    "                                               max_length=2048,\n",
    "                                               min_length=64,\n",
    "                                               no_repeat_ngram_size=0,\n",
    "                                                do_sample=False )\n",
    "\n",
    "        exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "        execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>BART Abstractive Summary:</b>: '+exec_sum+'</div>'\n",
    "        display(HTML(execSum_HTML))\n",
    "        warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 12px; padding-bottom:12px; color:#CCCC00; margin-top:1pt\"> Warning this is an autogenerated summary based on semantic search of abstracts, always examine the sources before accepting this conclusion.  If the evidence only mentions topic in passing or the evidence is not clear, the summary will likely not clearly answer the question.</div>'\n",
    "        display(HTML(warning_HTML))\n",
    "\n",
    "#    display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>Body of Evidence:</b></div>'))\n",
    "    \n",
    "    if FIND_PDFS or SEARCH_MEDRXIV:\n",
    "        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])\n",
    "    else:\n",
    "        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])\n",
    "        \n",
    "    display(HTML(df.to_html(render_links=True, escape=False)))\n",
    "    return full_abs_list,ranked_aswers,pandasData\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y12l1rgYgAFF",
    "outputId": "ce8be8b9-7243-4b3b-8887-7f878c2d0e9d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lucene ID</th>\n",
       "      <th>BERT-SQuAD Answer with Highlights</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Title/Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dry eye syndrome</td>\n",
       "      <td><div> ~ sentence _ 1 other associated symptoms include irritation, redness, discharge, and easily fatigued eyes. ~ sentence _ 2 blurred vision may also occur.  <font color='red'>~ sentence _ 3 the symptoms can range from mild and occasional to severe and continuous</font> </div></td>\n",
       "      <td>0.580961</td>\n",
       "      <td>Dry eye syndrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lassa fever</td>\n",
       "      <td><div> ~ sentence _ 90 as of 2003,10-16 % of people in sierra leone and liberia admitted to hospital had the virus. ~ sentence _ 91 the case fatality rate for those who are hospitalized for the disease is  <font color='red'>about 15-20 %</font> . ~ sentence _ 92 research showed a twofold increase risk of infection for those living in close proximity to someone with infection symptoms within the last year. ~ sentence _ 93 the high risk areas cannot be well defined by any known biogeographical or environmental breaks except for the multimammate rat, particularly guinea (kindia, faranah and nzerekore regions), liberia (mostly in lofa, bong, and nimba counties), nigeria (in about 10 of 36 states) and sierra leone (typically from kenema and kailahun districts). ~ sentence _ 94 it is less common in the central african republic, mali, senegal and other nearby countries, and less common yet in ghana and the democratic republic of the congo. ~ sentence _ 95 benin had its first confirmed cases in 2014, and togo had its first confirmed cases in 2016. ~ sentence _ 96 as of 2013, the spread of lassa outside of west africa had been very limited. ~ sentence _ 97 twenty to thirty cases had been described in europe, as being caused by importation through infected individuals. ~ sentence _ 98 these cases found outside of west africa were found to have a high fatality risk because of the delay of diagnosis and treatment due to being unaware of the risk associated with the symptoms. ~ sentence _ 99 imported cases have not manifested in larger epidemics outside of africa due to a lack of human to human transmission in hospital settings. ~ sentence _ 100 an exception had occurred in 2003 when a healthcare worker became infected before the person showed clear symptoms. ~ section _ 13 nigeria ~ section _ 14 2018 outbreak ~ sentence _ 101 an outbreak of lassa fever occurred in nigeria during 2018 and spread to 18 of the country ' s states ; it was the largest outbreak of lassa recorded. ~ sentence _ 102 on 25 february 2018, there were 1081 suspected cases and 90 reported deaths ; 317 of the cases and 72 deaths were confirmed as lassa which increased to a total of 431 reported cases in 2018. ~ section _ 15 2019 outbreak ~ sentence _ 103 the total cases in nigeria in 2019 was 810 with 167 deaths, the largest case fatality rate (23.</div></td>\n",
       "      <td>0.574216</td>\n",
       "      <td>Lassa fever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transient global amnesia</td>\n",
       "      <td><div> ~ sentence _ 82 tga is most common in people between age 56 and 75, with the average age of a person experiencing tga being approximately 62. ~ section _ 12 see also ~ item _ 2 _ 14 amnesia ~ item _ 2 _ 15 [ [ dissociative _ amnesia | dissociative amnesia ] ] ~ <font color='red'>only mild symptoms with no need for hospitalization</font> </div></td>\n",
       "      <td>0.545259</td>\n",
       "      <td>Transient global amnesia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Funding Circle</td>\n",
       "      <td><div> ~ sentence _ 36 it further announced that during 2013 there would be consultation to decide how the new rules would work. ~ <font color='red'>only mild symptoms with no need for hospitalization</font> </div></td>\n",
       "      <td>0.499015</td>\n",
       "      <td>Funding Circle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bucillamine</td>\n",
       "      <td><div> food & drug administration (fda) has approved revive therapeutics ltd. to proceed with a randomized, double-blind, placebo-controlled confirmatory phase 3 clinical trial protocol to evaluate the safety and efficacy of bucillamine in patients  <font color='red'>with mild-moderate</font> </div></td>\n",
       "      <td>0.436881</td>\n",
       "      <td>Bucillamine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Autism</td>\n",
       "      <td><div> although the dsm-iv rules out concurrent diagnosis of many other conditions along with autism, the full criteria for [ [ attention _ deficit _ hyperactivity _ disorder | attention deficit hyperactivity disorder ] ] (adhd), [ [ tourette _ syndrome | tourette syndrome ] ], and other of these conditions are often present and these [ [ conditions _ comorbid _ to _ autism _ spectrum _ disorders | comorbid diagnoses ] ] are increasingly accepted. ~ item _ 2 _  <font color='red'>19 sleep problems</font>  affect about two-thirds of individuals with asd at some point in childhood.</div></td>\n",
       "      <td>0.436739</td>\n",
       "      <td>Autism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Arteriovenous malformation</td>\n",
       "      <td><div> miller ] ] was diagnosed with avm after filming [ [ yogi _ bear _ (film) | yogi bear ] ] in new zealand in 2010 ; miller described his experience with the disease on the pete holmes podcast you made it weird on october 28,2011, shedding his comedian side for a moment and becoming more philosophical, narrating his behaviors and inability to sleep during that time. he suffered a seizure upon return to los angeles and successfully underwent surgery that had a mortality rate  <font color='red'>of ten percent</font> . ~ item _ 4 _ 35 jazz guitarist [ [ pat _ martino | pat martino ] ] experienced an avm and subsequently developed amnesia and manic depression.</div></td>\n",
       "      <td>0.413802</td>\n",
       "      <td>Arteriovenous malformation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alcoholism</td>\n",
       "      <td><div> <font color='red'>sentence _ 0 \" alcoholic \" redirects here. ~ sentence _ 1 for alcoholic beverages, see [ [ alcoholic _ drink | alcoholic drink ] ]. ~ sentence _ 2 for the song by starsailor, see [ [ alcoholic _ (song) | alcoholic (song) ] ]. ~ sentence _ 3 alcoholism is, broadly, any drinking of [ [ alcohol _ (drug) | alcohol ] ] that results in significant mental or physical [ [ health | health ] ] problems. ~ sentence _ 4 alcoholism is not a recognized diagnostic entity. ~ sentence _ 5 predominant diagostic classifications are [ [ alcohol _ use _ disorder | alcohol use disorder ] ] ([ [ dsm-5 | dsm-5 ] ]) or [ [ alcohol _ dependence | alcohol dependence ] ] ([ [ icd-11 | icd-11 ] ]). ~ sentence _ 6 excessive alcohol use can damage all organ systems, but it particularly affects the brain, heart, liver, [ [ pancreas | pancreas ] ] and [ [ immune _ system | immune system ] ]. ~ sentence _ 7 alcoholism can result in [ [ mental _ illness | mental illness ] ], [ [ delirium _ tremens | delirium tremens ] ], [ [ wernicke – korsakoff _ syndrome | wernicke – korsakoff syndrome ] ], [ [ heart _ arrhythmia | irregular heartbeat ] ], an impaired immune response, liver [ [ cirrhosis | cirrhosis ] ] and [ [ alcohol _ and _ cancer | increased cancer risk ] ]. ~ sentence _ 8 drinking during [ [ pregnancy | pregnancy ] ] can result in [ [ fetal _ alcohol _ spectrum _ disorder | fetal alcohol spectrum disorders ] ]. ~ sentence _ 9 women are generally more sensitive than men to the harmful effects of alcohol, primarily due to their smaller body weight, lower capacity to metabolize alcohol, and higher proportion of body fat. ~ sentence _ 10 in a small number of individuals</font> , prolonged, severe alcohol abuse ultimately leads to frank [ [ alcohol-related _ dementia | dementia ] ]. ~ sentence _ 11 environmental factors and genetics are two factors affecting risk for alcoholism, with about half the risk attributed to each. ~ sentence _ 12 someone with a parent or sibling with alcoholism is three to four times more likely to become an alcoholic themselves, but only a minority of them do. ~ sentence _ 13 environmental factors include social, cultural and behavioral influences. ~ sentence _ 14 high [ [ stress _ (biological) | stress levels ] ] and anxiety, as well as alcohol ' s inexpensive cost and easy accessibility, increase the risk. ~ sentence _ 15 people may continue to drink partly to prevent or improve symptoms of withdrawal. ~ sentence _ 16 after a person stops drinking alcohol, they may experience a low level of withdrawal lasting for months. ~ sentence _ 17 medically, alcoholism is considered both a physical and mental illness. ~ sentence _ 18 questionnaires are usually used to detect possible alcoholism. ~ sentence _ 19 further information is then collected to confirm the diagnosis. ~ sentence _ 20 prevention of alcoholism may be attempted by regulating and limiting the sale of alcohol (particularly to minors), taxing alcohol to increase its cost, and providing education and inexpensive treatment. ~ sentence _ 21 prohibition did not work. ~ sentence _ 22 treatment of alcoholism may take several forms. ~ sentence _ 23 due to medical problems that can occur during withdrawal, [ [ alcohol _ detoxification | alcohol detoxification ] ] should be carefully controlled. ~ sentence _ 24 one common method involves the use of [ [ benzodiazepine | benzodiazepine ] ] medications, such as [ [ diazepam | diazepam ] ]. ~ sentence _ 25 these can be either given while admitted to a health care institution or occasionally while a person remains in the community with close supervision. ~ sentence _ 26 mental illness or other addictions may complicate treatment. ~ sentence _ 27 after detoxification, various forms of individual or group therapy or [ [ support _ group | support groups ] ] can help keep a person from returning to drinking. ~ sentence _ 28 one commonly used form of support is the group [ [ alcoholics _ anonymous | alcoholics anonymous ] ]. ~ sentence _ 29 the medications [ [ acamprosate | acamprosate ] ], [ [ disulfiram | disulfiram ] ] or [ [ naltrexone | naltrexone ] ] may also be used to help prevent further drinking. ~ sentence _ 30 the [ [ world _ health _ organization | world health organization ] ] has estimated that as of 2016, there were 380 million people with alcoholism worldwide (5. 1 % of the population over 15 years of age). ~ sentence _ 31 as of 2015 in the united states, about 17 million (7 %) of adults and 0. 7 million (2. 8 %) of those age 12 to 17 years of age are affected. ~ sentence _ 32 alcoholism is most common among males and young adults. ~ sentence _ 33 geographically, it is least common in africa (1. 1 % of the population) and has the highest rates in [ [ eastern _ europe | eastern europe ] ] (11 %). ~ sentence _ 34 alcoholism directly resulted in 139,000 deaths in 2013, up from 112,000 deaths in 1990. ~ sentence _ 35 a total of 3. 3 million deaths (5. 9 % of all deaths) are believed to be due to alcohol. ~ sentence _ 36 alcoholism reduces a person ' s [ [ life _ expectancy | life expectancy ] ] by approximately ten years. ~ sentence _ 37 many terms, some [ [ pejorative | insulting ] ] and others [ [ slang | informal ] ], have been used to refer to people affected by alcoholism ; the expressions include tippler, drunkard, [ [ dipsomaniac | dipsomaniac ] ] and souse. ~ sentence _ 38 in 1979, the world health organization discouraged the use of \" alcoholism \" due to its inexact meaning, preferring \" alcohol dependence syndrome \". ~ section _ 0 signs and symptoms ~ sentence _ 39 the risk of alcohol dependence begins at low levels of drinking and increases directly with both the volume of alcohol consumed and a pattern of [ [ binge _ drinking | drinking larger amounts on an occasion ] ], to the point of intoxication, which is sometimes called \" binge drinking \". ~ section _ 1 long-term misuse ~ sentence _ 40 alcoholism is characterised by an increased [ [ alcohol _ tolerance | tolerance ] ] to alcohol – which means that an individual can consume more alcohol – and [ [ physical _ dependence | physical dependence ] ] on alcohol, which makes it hard for an individual to control their consumption. ~ sentence _ 41 the physical dependency caused by alcohol can lead to an affected individual having a very strong urge to drink alcohol. ~ sentence _ 42 these characteristics play a role in decreasing an alcoholic ' s ability to stop drinking. ~ sentence _ 43 alcoholism can have adverse effects on mental health, contributing to psychiatric disorders and increasing the risk of suicide. ~ sentence _ 44 a [ [ depression _ (mood) | depressed mood ] ] is a common symptom of heavy alcohol drinkers. ~ section _ 2 warning signs ~ sentence _ 45 warning signs of alcoholism include the consumption of increasing amounts of alcohol and frequent intoxication, preoccupation with drinking to the exclusion of other activities, promises to quit drinking and failure to keep those promises, the inability to remember what was said or done while drinking (colloquially known as \" blackouts \"), personality changes associated with drinking, denial or the making of excuses for drinking, the refusal to admit excessive drinking, dysfunction or other problems at work or school, the loss of interest in personal appearance or hygiene, marital and economic problems, and the complaint of poor health, with loss of appetite, respiratory infections, or increased anxiety. ~ section _ 3 physical ~ section _ 4 short-term effects ~ sentence _ 46 main article : [ [ short-term _ effects _ of _ alcohol _ consumption | short-term effects of alcohol consumption ] ] ~ sentence _ 47 drinking enough to cause a [ [ blood _ alcohol _ concentration | blood alcohol concentration ] ] (bac) of 0. 03 – 0. 12 % typically causes an overall improvement in mood and possible [ [ euphoria | euphoria ] ] (a \" happy \" feeling), increased self-confidence and sociability, decreased anxiety, a [ [ alcohol _ flush _ reaction | flushed, red appearance in the face ] ] and impaired judgment and fine muscle coordination. ~ sentence _ 48 a bac of 0. 09 % to 0. 25 % causes [ [ lethargy | lethargy ] ], [ [ sedation | sedation ] ], balance problems and blurred vision. ~ sentence _ 49 a bac of 0. 18 % to 0. 30 % causes profound confusion, impaired speech (e. g. slurred speech), staggering, dizziness and vomiting. ~ sentence _ 50 a bac from 0. 25 % to 0. 40 % causes [ [ stupor | stupor ] ], unconsciousness, [ [ anterograde _ amnesia | anterograde amnesia ] ], vomiting (death may occur due to inhalation of vomit (pulmonary aspiration) while unconscious) and [ [ respiratory _ depression | respiratory depression ] ] (potentially life-threatening). ~ sentence _ 51 a bac from 0. 35 % to 0. 80 % causes a [ [ coma | coma ] ] (unconsciousness), life-threatening respiratory depression and possibly fatal [ [ alcohol _ poisoning | alcohol poisoning ] ]. ~ sentence _ 52 with all alcoholic beverages, [ [ drinking _ and _ driving | drinking while driving ] ], operating an aircraft or heavy machinery increases the risk of an accident ; many countries have penalties for drunk driving. ~ section _ 5 long-term effects ~ sentence _ 53 see also : [ [ long-term _ effects _ of _ alcohol _ consumption | long-term effects of alcohol consumption ] ] ~ sentence _ 54 having more than one drink a day for women or two drinks for men increases the risk of heart disease, [ [ high _ blood _ pressure | high blood pressure ] ], [ [ atrial _ fibrillation | atrial fibrillation ] ], and [ [ stroke | stroke ] ]. ~ sentence _ 55 risk is greater with [ [ binge _ drinking | binge drinking ] ], which may also result in violence or accidents. ~ sentence _ 56 about 3. 3 million deaths (5. 9 % of all deaths) are believed to be due to alcohol each year. ~ sentence _ 57 alcoholism reduces a person ' s life expectancy by around ten years and alcohol use is the third leading cause of early death in the united states. ~ sentence _ 58 no professional medical association recommends that people who are nondrinkers should start drinking. ~ sentence _ 59 long-term alcohol abuse can cause a number of physical symptoms, including [ [ cirrhosis | cirrhosis ] ] of the liver, [ [ pancreatitis | pancreatitis ] ], [ [ epilepsy | epilepsy ] ], [ [ polyneuropathy | polyneuropathy ] ], [ [ wernicke-korsakoff _ syndrome | alcoholic dementia ] ], heart disease, nutritional deficiencies, [ [ duodenal _ ulcer | peptic ulcers ] ] and [ [ sexual _ dysfunction | sexual dysfunction ] ], and can eventually be fatal. ~ sentence _ 60 other physical effects include an increased risk of developing [ [ cardiovascular _ disease | cardiovascular disease ] ], [ [ malabsorption | malabsorption ] ], [ [ alcoholic _ liver _ disease | alcoholic liver disease ] ], and several [ [ cancer | cancers ] ]. ~ sentence _ 61 damage to the [ [ central _ nervous _ system | central nervous system ] ] and peripheral nervous system can occur from sustained alcohol consumption. ~ sentence _ 62 a wide range of immunologic defects can result and there may be a generalized skeletal fragility, in addition to a recognized tendency to accidental injury, resulting a propensity to bone fractures. ~ sentence _ 63 women develop long-term complications of alcohol dependence more rapidly than do men. ~ sentence _ 64 additionally, women have a higher mortality rate from alcoholism than men. ~ sentence _ 65 examples of long-term complications include brain, heart, and liver damage and an [ [ alcohol _ and _ breast _ cancer | increased risk of breast cancer ] ]. ~ sentence _ 66 additionally, heavy drinking over time has been found to have a negative effect on reproductive functioning in women. ~ sentence _ 67 this results in reproductive dysfunction such as [ [ anovulation | anovulation ] ], decreased ovarian mass, problems or irregularity of the [ [ menstrual _ cycle | menstrual cycle ] ], and early [ [ menopause | menopause ] ]. ~ sentence _ 68 alcoholic [ [ ketoacidosis | ketoacidosis ] ] can occur in individuals who chronically abuse alcohol and have a recent history of [ [ binge _ drinking | binge drinking ] ]. ~ sentence _ 69 the amount of alcohol that can be biologically processed and its effects differ between sexes. ~ sentence _ 70 equal dosages of alcohol consumed by men and women generally result in women having higher [ [ blood _ alcohol _ concentration | blood alcohol concentrations ] ] (bacs), since women generally have a lower weight and higher percentage of body fat and therefore a lower volume of distribution for alcohol than men. ~ section _ 6 psychiatric ~ sentence _ 71 long-term misuse of alcohol can cause a wide range of mental health problems. ~ sentence _ 72 severe [ [ cognitive | cognitive ] ] problems are common ; approximately 10 percent of all dementia cases are related to alcohol consumption, making it the second leading cause of [ [ dementia | dementia ] ]. ~ sentence _ 73 excessive alcohol use causes damage to brain function, and psychological health can be increasingly affected over time. ~ sentence _ 74 [ [ social _ skills | social skills ] ] are significantly impaired in people suffering from alcoholism due to the neurotoxic effects of alcohol on the brain, especially the [ [ prefrontal _ cortex | prefrontal cortex ] ] area of the brain. ~ sentence _ 75 the social skills that are impaired by [ [ alcohol _ abuse | alcohol abuse ] ] include impairments in perceiving facial emotions, [ [ prosody _ (linguistics) | prosody ] ] perception problems and [ [ theory _ of _ mind | theory of mind ] ] deficits ; the ability to understand humour is also impaired in alcohol abusers. ~ sentence _ 76 psychiatric disorders are common in alcoholics, with as many as 25 percent suffering severe psychiatric disturbances. ~ sentence _ 77 the most prevalent psychiatric symptoms are [ [ anxiety _ disorder | anxiety ] ] and [ [ major _ depressive _ disorder | depression ] ] disorders. ~ sentence _ 78 psychiatric symptoms usually initially worsen during alcohol withdrawal, but typically improve or disappear with continued abstinence. ~ sentence _ 79 [ [ psychosis | psychosis ] ], [ [ confusion | confusion ] ], and organic brain syndrome may be caused by alcohol misuse, which can lead to a misdiagnosis such as [ [ schizophrenia | schizophrenia ] ]. ~ sentence _ 80 [ [ panic _ disorder | panic disorder ] ] can develop or worsen as a direct result of long-term alcohol misuse. ~ sentence _ 81 the co-occurrence of [ [ major _ depressive _ disorder | major depressive disorder ] ] and alcoholism is well documented. ~ sentence _ 82 among those with [ [ comorbid | comorbid ] ] occurrences, a distinction is commonly made between depressive episodes that remit with alcohol abstinence (\" substance-induced \"), and depressive episodes that are primary and do not remit with abstinence (\" independent \" episodes). ~ sentence _ 83 additional use of other drugs may increase the risk of depression. ~ sentence _ 84 psychiatric disorders differ depending on gender. ~ sentence _ 85 women who have alcohol-use disorders often have a co-occurring psychiatric diagnosis such as [ [ major _ depression | major depression ] ], [ [ anxiety | anxiety ] ], [ [ panic _ disorder | panic disorder ] ], [ [ bulimia | bulimia ] ], [ [ post-traumatic _ stress _ disorder | post-traumatic stress disorder ] ] (ptsd), or [ [ borderline _ personality _ disorder | borderline personality disorder ] ]. ~ sentence _ 86 men with alcohol-use disorders more often have a co-occurring diagnosis of [ [ narcissistic _ personality _ disorder | narcissistic ] ] or [ [ antisocial _ personality _ disorder | antisocial personality disorder ] ], [ [ bipolar _ disorder | bipolar disorder ] ], [ [ schizophrenia | schizophrenia ] ], [ [ impulse _ disorder | impulse disorders ] ] or attention deficit / hyperactivity disorder (adhd). ~ sentence _ 87 women with alcoholism are more likely to experience physical or [ [ sexual _ assault | sexual assault ] ], abuse and [ [ domestic _ violence | domestic violence ] ] than women in the general population, which can lead to higher instances of psychiatric disorders and greater dependence on alcohol. ~ section _ 7 social effects ~ sentence _ 88 see also : [ [ drug-related _ crime | drug-related crime ] ] ~ sentence _ 89 serious social problems arise from alcoholism ; these dilemmas are caused by the pathological changes in the brain and the intoxicating effects of alcohol. ~ sentence _ 90 alcohol abuse is associated with an increased risk of committing criminal offences, including [ [ child _ abuse | child abuse ] ], [ [ domestic _ violence | domestic violence ] ], [ [ rape | rape ] ], [ [ burglary | burglary ] ] and [ [ assault | assault ] ]. ~ sentence _ 91 alcoholism is associated with loss of employment, which can lead to financial problems. ~ sentence _ 92 drinking at inappropriate times and behavior caused by reduced judgment can lead to legal consequences, such as criminal charges for [ [ drunk _ driving | drunk driving ] ] or public disorder, or civil penalties for [ [ tort | tortious ] ] behavior. ~ sentence _ 93 an alcoholic ' s behavior and mental impairment while drunk can profoundly affect those surrounding him and lead to isolation from family and friends. ~ sentence _ 94 this isolation can lead to marital conflict and divorce, or contribute to [ [ domestic _ violence | domestic violence ] ]. ~ sentence _ 95 alcoholism can also lead to [ [ child _ neglect | child neglect ] ], with subsequent lasting damage to the emotional development of the alcoholic ' s children. ~ sentence _ 96 for this reason, children of alcoholic parents can develop a number of emotional problems. ~ sentence _ 97 for example, they can become afraid of their parents, because of their unstable mood behaviors. ~ sentence _ 98 in addition, they can develop considerable amount of shame over their inadequacy to liberate their parents from alcoholism. ~ sentence _ 99 as a result of this failure, they develop wretched self-images, which can lead to depression. ~ section _ 8 alcohol withdrawal ~ sentence _ 100 main article : [ [ alcohol _ withdrawal _ syndrome | alcohol withdrawal syndrome ] ] ~ sentence _ 101 see also : [ [ kindling _ (sedative-hypnotic _ withdrawal) | kindling (sedative-hypnotic withdrawal) ] ] ~ sentence _ 102 as with similar substances with a sedative-hypnotic mechanism, such as [ [ barbiturates | barbiturates ] ] and [ [ benzodiazepines | benzodiazepines ] ], withdrawal from alcohol dependence can be fatal if it is not properly managed. ~ sentence _ 103 alcohol ' s primary effect is the increase in stimulation of the [ [ gabaa _ receptor | gabaa receptor ] ], promoting [ [ central _ nervous _ system | central nervous system ] ] depression. ~ sentence _ 104 with repeated heavy consumption of alcohol, these receptors are desensitized and reduced in number, resulting in [ [ drug _ tolerance | tolerance ] ] and [ [ physical _ dependence | physical dependence ] ]. ~ sentence _ 105 when alcohol consumption is stopped too abruptly, the person ' s nervous system suffers from uncontrolled [ [ synapse | synapse ] ] firing. ~ sentence _ 106 this can result in symptoms that include [ [ anxiety _ (mood) | anxiety ] ], life-threatening [ [ seizure | seizures ] ], [ [ delirium _ tremens | delirium tremens ] ], hallucinations, shakes and possible [ [ heart _ failure | heart failure ] ]. ~ sentence _ 107 other neurotransmitter systems are also involved, especially [ [ dopamine | dopamine ] ], [ [ nmda | nmda ] ] and [ [ glutamate | glutamate ] ]. ~ sentence _ 108 severe acute withdrawal symptoms such as [ [ delirium _ tremens | delirium tremens ] ] and seizures rarely occur after 1-week post cessation of alcohol. ~ sentence _ 109 the acute withdrawal phase can be defined as lasting between one and three weeks. ~ sentence _ 110 in the period of 3 – 6 weeks following cessation, anxiety, depression, fatigue, and sleep disturbance are common. ~ sentence _ 111 similar post-acute withdrawal symptoms have also been observed in animal models of alcohol dependence and withdrawal. ~ sentence _ 112 a [ [ kindling _ (sedative-hypnotic _ withdrawal) | kindling effect ] ] also occurs in alcoholics whereby each subsequent withdrawal syndrome is more severe than the previous withdrawal episode ; this is due to neuroadaptations which occur as a result of periods of abstinence followed by re-exposure to alcohol. ~ sentence _ 113 individuals who have had multiple withdrawal episodes are more likely to develop seizures and experience more severe anxiety during withdrawal from alcohol than alcohol-dependent individuals without a history of past alcohol withdrawal episodes. ~ sentence _ 114 the kindling effect leads to persistent functional changes in brain neural circuits as well as to [ [ gene _ expression | gene expression ] ]. ~ sentence _ 115 kindling also results in the intensification of psychological symptoms of alcohol withdrawal. ~ sentence _ 116 there are decision tools and questionnaires that help guide physicians in evaluating alcohol withdrawal. ~ sentence _ 117 for example, the ciwa-ar objectifies alcohol withdrawal symptoms in order to guide therapy decisions which allows for an efficient interview while at the same time retaining clinical usefulness, validity, and reliability, ensuring proper care for withdrawal patients, who can be in danger of death. ~ section _ 9 causes ~ sentence _ 118 a complex combination of genetic and environmental factors influences the risk of the development of alcoholism. ~ sentence _ 119 genes that influence the metabolism of alcohol also influence the risk of alcoholism, as can a family history of alcoholism. ~ sentence _ 120 there is compelling evidence that alcohol use at an early age may influence the [ [ gene _ expression | expression of genes ] ] which increase the risk of alcohol dependence. ~ sentence _ 121 these genetic and [ [ epigenetics | epigenetic ] ] results are regarded as consistent with large longitudinal population studies finding that the younger the age of drinking onset, the greater the prevalence of lifetime alcohol dependence. ~ sentence _ 122 [ [ psychological _ trauma | severe childhood trauma ] ] is also associated with a general increase in the risk of drug dependency. ~ sentence _ 123 lack of peer and family support is associated with an increased risk of alcoholism developing. ~ sentence _ 124 genetics and adolescence are associated with an increased sensitivity to the neurotoxic effects of chronic alcohol abuse. ~ sentence _ 125 [ [ cerebral _ cortex | cortical ] ] degeneration due to the neurotoxic effects increases impulsive behaviour, which may contribute to the development, persistence and severity of alcohol use disorders. ~ sentence _ 126 there is evidence that with abstinence, there is a reversal of at least some of the alcohol induced central nervous system damage. ~ sentence _ 127 the use of cannabis was associated with later problems with alcohol use. ~ sentence _ 128 alcohol use was associated with an increased probability of later use of tobacco and illegal drugs such as cannabis. ~ section _ 10 availability ~ sentence _ 129 alcohol is the most available, widely consumed, and widely abused [ [ recreational _ drug | recreational drug ] ]. ~ sentence _ 130 [ [ beer | beer ] ] alone is the world ' s most widely consumed [ [ alcoholic _ beverage | alcoholic beverage ] ] ; it is the third-most popular drink overall, after [ [ drinking _ water | water ] ] and [ [ tea | tea ] ]. ~ sentence _ 131 it is thought by some to be the oldest fermented beverage. ~ section _ 11 gender difference ~ sentence _ 132 based on combined data in the us from [ [ substance _ abuse _ and _ mental _ health _ services _ administration | samhsa ] ] ' s 2004 – 2005 national surveys on drug use & health, the rate of past-year alcohol dependence or abuse among persons aged 12 or older varied by level of alcohol use : 44.</div></td>\n",
       "      <td>0.336868</td>\n",
       "      <td>Alcoholism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Economy of Lebanon</td>\n",
       "      <td><div> ~ sentence _ 168 there have been incessant power cuts and some residents have been calling the blackouts worse than those witnessed in the 1975 – 1990 civil war. ~ section _ 17 see also ~ item _ 4 _ 14 [ [ agriculture _ in _ lebanon | agriculture in lebanon ] ] ~ item _ 4 _ 15 [ [ beirut _ stock _ exchange | beirut stock exchange ] ] ~ item _ 4 _ 16 [ [ banque _ du _ liban | banque du liban ] ] (central bank) ~ item _ 4 _ 17 [ [ lebanese _ pound | lebanese pound ] ] ~ item _ 4 _ 18 [ [ solidere | solidere ] ] ~ item _ 4 _ 19 [ [ tourism _ in _ lebanon | tourism in lebanon ] ] ~ item _ 4 _ 20 [ [ list _ of _ banks _ in _ lebanon | list of banks in lebanon ] ] ~ <font color='red'>only mild symptoms</font> </div></td>\n",
       "      <td>0.315238</td>\n",
       "      <td>Economy of Lebanon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sampling bias</td>\n",
       "      <td><div> ~ sentence _ 69 this would adjust any estimates to achieve the same expected value as a sample that included exactly 50 men and 50 women, unless men and women differed in their likelihood of taking part in the survey. ~ section _ 8 see also ~ item _ 2 _ 12 [ [ censored _ regression _ model | censored regression model ] ] ~ item _ 2 _ 13 [ [ cherry _ picking _ (fallacy) | cherry picking (fallacy) ] ] ~ item _ 2 _ 14 ~ item _ 2 _ 15 [ [ friendship _ paradox | friendship paradox ] ] ~ item _ 2 _ 16 [ [ reporting _ bias | reporting bias ] ] ~ item _ 2 _ 17 [ [ sampling _ probability | sampling probability ] ] ~ item _ 2 _ 18 [ [ selection _ bias | selection bias ] ] ~ item _ 2 _ 19 [ [ spectrum _ bias | spectrum bias ] ] ~ item _ 2 _ 20 [ [ truncated _ regression _ model | truncated regression model ] ] ~ <font color='red'>only mild symptoms with no need for hospitalization</font> </div></td>\n",
       "      <td>0.178456</td>\n",
       "      <td>Sampling bias</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_abs_list,ranked_aswers,pm_ids=displayResults(hit_dictionary, answers, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bkOk2Go5MsaI",
    "outputId": "33bc2148-b686-4ee1-9183-3fbe8ebda0f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 1024)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (12): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (13): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (14): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (15): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (16): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (17): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (18): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (19): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (20): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (21): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (22): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (23): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (layer_2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "import sentencepiece\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "#from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "#SUMMARY_TOKENIZER = PegasusTokenizer.from_pretrained('mayu0007/pegasus_large_covid')\n",
    "#SUMMARY_MODEL = PegasusForConditionalGeneration.from_pretrained('mayu0007/pegasus_large_covid')\n",
    "\n",
    "SUMMARY_MODEL.to(torch_device)\n",
    "SUMMARY_MODEL.eval()\n",
    "def Summarizer(string_all):    \n",
    "    if 1==1:\n",
    "        ## try generating an exacutive summary with bart abstractive summarizer\n",
    "        allAnswersTxt = string_all.replace('\\n','')\n",
    "\n",
    "        answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=1024)['input_ids'].to(torch_device)\n",
    "        #answers_input_ids = SUMMARY_TOKENIZER.prepare_seq2seq_batch([allAnswersTxt], return_tensors='pt', truncation=True, padding='longest')['input_ids'].to(torch_device)\n",
    "        summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n",
    "                                               num_beams=4,\n",
    "                                               length_penalty=0.8,\n",
    "                                               repetition_penalty=2.0,\n",
    "                                               min_length=5,\n",
    "                                               no_repeat_ngram_size=0,\n",
    "                                                do_sample=False )\n",
    "\n",
    "        exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "        return exec_sum\n",
    "\n",
    "# Program to find most frequent  \n",
    "# element in a list \n",
    "  \n",
    "from collections import Counter \n",
    "  \n",
    "def most_frequent(List): \n",
    "    occurence_count = Counter(List) \n",
    "    return occurence_count.most_common(1)[0][0]\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,AutoModelForSequenceClassification\n",
    "#hg_model_hub_name = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "# hg_model_hub_name = \"ynie/electra-large-discriminator-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "hg_model_hub_name = \"ynie/xlnet-large-cased-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "\n",
    "tokenizer_nli = AutoTokenizer.from_pretrained(hg_model_hub_name, use_fast=False)\n",
    "model_nli = AutoModelForSequenceClassification.from_pretrained(hg_model_hub_name)\n",
    "model_nli.to(torch_device)\n",
    "model_nli.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y36I5UR4Xhpm"
   },
   "outputs": [],
   "source": [
    "def CheckEntailmentNeturalorContradict(ranked_aswers,string_all,pandasData):\n",
    "  if 1==1:\n",
    "    premise_list=[]\n",
    "    hypothesis_list=[]\n",
    "    entailment_prob=[]\n",
    "    neutral_prob=[]\n",
    "    contradict_prob=[]\n",
    "    final_result_list=[]\n",
    "    if len(ranked_aswers)>8:\n",
    "      v_range=8\n",
    "    else:\n",
    "      v_range=len(ranked_aswers)\n",
    "    for i in range(v_range):\n",
    "      if pandasData[i][2]>=0.4:\n",
    "        max_length = 512\n",
    "        candidate_answer=Summarizer(ranked_aswers[i])\n",
    "        #premise = \"symptoms of covid-19 range from none (asymptomatic) to severe pneumonia and it can be fatal. fever and cough are the most common symptoms in patients with 2019-ncov infection. Several people have muscle soreness or fatigue as well as ards. diarrhea, hemoptysis, headache, sore throat, shock, and other symptoms only occur in a small number of patients\"\n",
    "        if len(candidate_answer)>len(ranked_aswers[i]):\n",
    "          premise = ranked_aswers[i]\n",
    "        else:\n",
    "          premise=candidate_answer\n",
    "        #premise = \"covid-19 symptom is highly various in each patient, with fever, fatigue, shortness of breath, and cough as the main presenting symptoms. patient with covid-19 may shows severe symptom with severe pneumonia and ards, mild symptom resembling simple upper respiration tract infection, or even completely asymptomatic. approximately 80 % of cases is mild \"\n",
    "        hypothesis= string_all \n",
    "        tokenized_input_seq_pair = tokenizer_nli.encode_plus(premise, hypothesis,\n",
    "                                                      max_length=max_length,\n",
    "                                                      return_token_type_ids=True, truncation=True)\n",
    "\n",
    "        input_ids = torch.Tensor(tokenized_input_seq_pair['input_ids']).long().unsqueeze(0).to(torch_device)\n",
    "        # remember bart doesn't have 'token_type_ids', remove the line below if you are using bart.\n",
    "        token_type_ids = torch.Tensor(tokenized_input_seq_pair['token_type_ids']).long().unsqueeze(0).to(torch_device)\n",
    "        attention_mask = torch.Tensor(tokenized_input_seq_pair['attention_mask']).long().unsqueeze(0).to(torch_device)\n",
    "\n",
    "        outputs = model_nli(input_ids,\n",
    "                      attention_mask=attention_mask,\n",
    "                      token_type_ids=token_type_ids,\n",
    "                      labels=None)\n",
    "        # Note:\n",
    "        # \"id2label\": {\n",
    "        #     \"0\": \"entailment\",\n",
    "        #     \"1\": \"neutral\",\n",
    "        #     \"2\": \"contradiction\"\n",
    "        # },\n",
    "\n",
    "        predicted_probability = torch.softmax(outputs[0], dim=1)[0].tolist()  # batch_size only one\n",
    "\n",
    "        # print(\"Premise:\", premise)\n",
    "        premise_list.append(premise)\n",
    "        # print(\"Hypothesis:\", hypothesis)\n",
    "        hypothesis_list.append(hypothesis)\n",
    "        # print(\"input Ans:\",ranked_aswers[i])\n",
    "        # print(\"Entailment:\", predicted_probability[0])\n",
    "        entailment_prob.append(predicted_probability[0])\n",
    "        # print(\"Neutral:\", predicted_probability[1])\n",
    "        neutral_prob.append(predicted_probability[1])\n",
    "        # print(\"Contradiction:\", predicted_probability[2])\n",
    "        contradict_prob.append(predicted_probability[2])\n",
    "    if len(premise_list)>0:\n",
    "      avg_entailment=np.average(entailment_prob)\n",
    "      avg_neutral=np.average(neutral_prob)\n",
    "      avg_contradiction=np.average(contradict_prob) \n",
    "      zipped_list = zip(entailment_prob,neutral_prob,contradict_prob)\n",
    "      for key,val in enumerate(hypothesis_list):\n",
    "        if neutral_prob[key]>0.8:\n",
    "          final_result='Neutral'\n",
    "        elif entailment_prob[key]>contradict_prob[key]:\n",
    "          final_result='True'\n",
    "          final_result_list.append(final_result)\n",
    "        else:\n",
    "          final_result='False'\n",
    "          final_result_list.append(final_result)\n",
    "      if len(final_result_list)>0:\n",
    "          verdict=most_frequent(final_result_list)\n",
    "      else :\n",
    "          verdict='Neutral'\n",
    "    else:\n",
    "      zipped_list = zip(0,1,0)\n",
    "      verdict='Neutral'\n",
    "  return zipped_list,verdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yis0B0OVH2_q"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbvIllGHH2SW",
    "outputId": "efcb0af2-5cc9-4ee7-ce16-579b3ba7bbe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anglicize in /usr/local/lib/python3.7/dist-packages (0.0.3)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyserini==0.8.1.0\n",
    "from pyserini.search import pysearch\n",
    "\n",
    "import re\n",
    "import sys\n",
    "!pip install anglicize\n",
    "from anglicize import anglicize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "!python -m pip install -U spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "import spacy.cli # Colab fix\n",
    "spacy.cli.download(\"en_core_web_sm\") # Colab fix\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n",
    "# nlp.add_pipe(nlp.create_pipe('merge_noun_chunks')) # Colab fix?\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import statistics\n",
    "import pandas as pd\n",
    "\n",
    "DATA_TRAIN = \"/content/train.jsonl\" # Change this\n",
    "DATA_DEV = \"/content/dev.jsonl\" # Change this\n",
    "\n",
    "dataset_path = DATA_DEV\n",
    "#dataset_path = DATA_TRAIN\n",
    "\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-RP7PSiR3C3"
   },
   "outputs": [],
   "source": [
    "luceneDir = 'indexes/fever/lucene-index-fever-paragraph'\n",
    "searcher = pysearch.SimpleSearcher(luceneDir)\n",
    "\n",
    "def get_relevant_pages(claim, FULL_CLAIM, SPACY_ENTITIES, CASE_ENTITIES, ADD_SYNONYMS, ADD_SIMILAR, \n",
    "          LINKED_PAGES, LINKING_PAGES, CATCH_VERBS, TFIDF_TOP, FIND_ALL, N_RESULTS=70, SIMILAR_THRESHOLD=0.85, TFIDF_TOP_N=3):\n",
    "    \n",
    "    if LINKING_PAGES:\n",
    "        import sqlalchemy as sqla\n",
    "\n",
    "        db_fullpath = \"D:/FEVER/feverous_wikiv1.db\" # Change this\n",
    "        db = sqla.create_engine(\"sqlite:///{}\".format(db_fullpath))\n",
    "\n",
    "        from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "        Session = sessionmaker(bind=db)\n",
    "        session = Session()\n",
    "    \n",
    "    if not SPACY_ENTITIES and not CASE_ENTITIES and not LINKED_PAGES:\n",
    "        FULL_CLAIM = True\n",
    "    \n",
    "    start_timer = timer()\n",
    "\n",
    "    claim = nlp(claim.replace(\" \", \" \").replace(\"­\", \" \")) # Replaces the obnoxious space character with normal space\n",
    "\n",
    "    keywords = set()\n",
    "    entities = set()\n",
    "\n",
    "    if SPACY_ENTITIES:\n",
    "        spacy_entities = [entity.text for entity in claim.ents]\n",
    "        entities.update(spacy_entities)\n",
    "\n",
    "    if CASE_ENTITIES:\n",
    "        case_entities = set()\n",
    "        chunks = claim.noun_chunks\n",
    "        for chunk in chunks:\n",
    "            for token in tokenizer(chunk.text):\n",
    "                if token.text[0].isupper():\n",
    "                    case_entities.add(chunk.text)\n",
    "                    break\n",
    "\n",
    "        entities.update(case_entities)\n",
    "        #print(case_entities)\n",
    "        #print(entities)\n",
    "        #sys.exit(0)\n",
    "\n",
    "    keywords.update(entities)\n",
    "    #print(keywords)\n",
    "\n",
    "    if ADD_SYNONYMS:\n",
    "        for token in claim:\n",
    "            if token.is_stop:\n",
    "                continue\n",
    "            synonyms = wn.synsets(token.text)\n",
    "            if synonyms and token:\n",
    "                for synonym in synonyms:\n",
    "                    if synonym.pos() == token.pos_[0].lower() and synonym.pos() == \"n\":\n",
    "                        keywords.update([lemma.replace(\"_\", \" \") for lemma in synonym.lemma_names()])\n",
    "\n",
    "    if ADD_SIMILAR:\n",
    "        similar_check = [keyword for keyword in list(keywords) if keyword in vocab]\n",
    "\n",
    "        if similar_check:\n",
    "            similar_words = model.most_similar(positive=similar_check)\n",
    "            for i in range(0, len(similar_words)):\n",
    "                if i == 0 or similar_words[i][1] >= SIMILAR_THRESHOLD:\n",
    "                    keywords.update([similar_words[i][0]])\n",
    "    \n",
    "    if CATCH_VERBS:\n",
    "        for chunk in claim:\n",
    "            if chunk.pos_ == \"VERB\" and chunk.dep_ != \"case\" and chunk.dep_ != \"prep\":\n",
    "                keywords.update([chunk.lemma_])\n",
    "                \n",
    "    if TFIDF_TOP:\n",
    "        tfidf_sorting = np.argsort(tfidf.transform([claim.text]).toarray()).flatten()[::-1]\n",
    "        tfidf_len = len(tfidf_tokenize(claim.text))\n",
    "        keywords.update(tfidf_features[tfidf_sorting][0:min([TFIDF_TOP_N, tfidf_len])])\n",
    "\n",
    "    if not FULL_CLAIM:\n",
    "        search_query = (\", \".join(keywords) + '\"' + '\", \"'.join(entities) + '\"')\n",
    "#            search_query = (\", \".join(keywords) + \", \".join(entities))\n",
    "    else:\n",
    "        search_query = (claim.text + ' \"' + '\", \"'.join(keywords) + '\" \"' + '\", \"'.join(entities) + '\"')\n",
    "#            search_query = (claim.text + \" \" + \", \".join(keywords) + \", \".join(entities))\n",
    "\n",
    "    if FULL_CLAIM or keywords:\n",
    "        try:\n",
    "            lucene_hits = searcher.search(search_query.encode(\"utf-8\"), k=N_RESULTS)\n",
    "        except:\n",
    "            lucene_hits = None\n",
    "    else:\n",
    "        lucene_hits = None\n",
    "\n",
    "    hit_dictionary = {}\n",
    "\n",
    "    if lucene_hits:\n",
    "\n",
    "        for hit in lucene_hits:\n",
    "            hit_dict = {\"abstract\": None, \"real_abstract\": None, \"title\": None}\n",
    "            hit_dict['abstract']=hit.lucene_document.get(\"raw\")\n",
    "            hit_dict['real_abstract']=hit.lucene_document.get(\"raw\")\n",
    "            hit_dict['title'] = str(hit.docid)\n",
    "            hit_dictionary[str(hit.docid)] = hit_dict\n",
    "    \n",
    "        linked_pages = set()\n",
    "        if LINKED_PAGES:\n",
    "            for hit in lucene_hits:\n",
    "                links = re.findall(r\"(?:\\[\\[)(.*?)(?:\\|)\", hit.raw)\n",
    "                for link in links:\n",
    "                    linked_pages.update([link.replace(\"_\", \" \").encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\")])\n",
    "                    \n",
    "        linking_pages = set()\n",
    "        if LINKING_PAGES:\n",
    "            linking_results = session.execute('select target, sources from inlinks where target IN (\"{}\")'.format('\", \"'.join([hit.docid.replace('\"', '\"\"') for hit in lucene_hits])))\n",
    "            for row in linking_results:\n",
    "                linking_pages.update(row[\"sources\"].split(\";\"))\n",
    "            \"\"\"\n",
    "            for hit in lucene_hits:\n",
    "                linking_results = session.execute('select target, sources from inlinks where sources = \"{}\"'.format(hit.docid))\n",
    "                for row in linking_results:\n",
    "                    linking_pages.update(row[\"sources\"].split(\";\"))\"\"\"\n",
    "\n",
    "                    \n",
    "        #print(lucene_hits[0].docid.encode(\"latin-1\").decode(\"utf-8\"))\n",
    "        found_pages = [hit.docid for hit in lucene_hits]\n",
    "        #found_pages = set([hit.docid.encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\") for page in lucene_hits])\n",
    "        for i in range(0, len(found_pages)):\n",
    "\n",
    "            try:\n",
    "                found_pages[i] = found_pages[i].encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "                #print(\"Done:\",found_pages[i])\n",
    "            except:\n",
    "                import regex\n",
    "                print(found_pages[i], anglicize(found_pages[i]), regex.sub(r'[^\\p{Latin}]', '', found_pages[i]).encode(\"latin-1\").decode(\"utf-8\"))\n",
    "                \n",
    "        found_pages = set(found_pages)\n",
    "\n",
    "        hit_scores = [hit.score for hit in lucene_hits]\n",
    "        hit_score_min = min(hit_scores)\n",
    "        hit_score_25 = np.percentile(hit_scores, 25)\n",
    "        hit_score_mean = np.mean(hit_scores)\n",
    "        hit_score_median = statistics.median(hit_scores)\n",
    "        hit_score_75 = np.percentile(hit_scores, 75)\n",
    "        hit_score_max = max(hit_scores)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        is_found = False\n",
    "        found_pages = set()\n",
    "        entities = set()\n",
    "        keywords = set()\n",
    "        linked_pages = set()\n",
    "        lucene_hits = []\n",
    "        hit_scores = []\n",
    "        hit_score_min = None\n",
    "        hit_score_25 = None\n",
    "        hit_score_mean = None\n",
    "        hit_score_median = None\n",
    "        hit_score_75 = None\n",
    "        hit_score_max = None\n",
    "\n",
    "    end_timer = timer()\n",
    "    elapsed = int(end_timer - start_timer)\n",
    "    elapsed_formatted = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))\n",
    "        \n",
    "    result = [claim.text, elapsed_formatted, \n",
    "                found_pages, \n",
    "                keywords, \n",
    "                linked_pages, \n",
    "                linking_pages,\n",
    "                len(lucene_hits), \n",
    "                hit_score_min, hit_score_25, hit_score_mean, hit_score_median, \n",
    "                hit_score_75, hit_score_max, hit_scores]\n",
    "\n",
    "    #print(result)\n",
    "    \n",
    "    result = pd.DataFrame([result], columns=[\"CLAIM\", \"ELAPSED\", \n",
    "                                             \"PAGES_FOUND\",\n",
    "                                             \"KEYWORDS\", \n",
    "                                             \"LINKED_PAGES\",\n",
    "                                             \"LINKING_PAGES\",\n",
    "                                             \"N_LUCENE_HITS\", \n",
    "                                             \"HIT_SCORE_MIN\", \"HIT_SCORE_25\", \"HIT_SCORE_MEAN\", \"HIT_SCORE_MEDIAN\", \n",
    "                                             \"HIT_SCORE_75\", \"HIT_SCORE_MAX\", \"HIT_SCORES\"])\n",
    "    \n",
    "    return result, hit_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "p5eQYYTJT6Rn",
    "outputId": "5e2a6a6b-db0a-44cb-9cb1-0a2fb2bb4ab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ailanthus', 'Myrica', 'Spiraea', 'Izatha dulcior', 'Metacrias huttoni', 'Kon Ka Kinh National Park', 'Acacia sensu lato', 'Metacrias erichrysa', 'Metacrias strategica', 'William Trowbridge Merrifield Forbes', 'Alexander Julius Reichert', 'Julius Lederer (entomologist)', 'Museum of Natural History, University of Wrocław', 'Roscoea', 'Succession of the 14th Dalai Lama', 'Palace lantern', 'Alder', 'Lathys', 'Migration in China', 'Apoptosis', 'Agabus aequalis', 'Agilisaurus', 'Plumeria rubra', 'Xerochrysum bracteatum', 'Kuomintang', 'Jack Belden', 'Bambusiomyces', 'Elephantoceras', 'Amway', 'De-Sinicization', 'Antoninus Pius', 'Gim Jeong-hui', 'Battle of Pengcheng', 'Asiloidea', 'Khmer Rouge', 'Mary Jean Eisenhower', 'Yoshihiko Noda', 'Language and overseas Chinese communities', 'Internment of Chinese-Indians', 'Liugong Island', 'Ang (surname)', 'Sunken Plum', 'Liuli Gongfang', 'List of disputed territories of India', 'Li Qiang (activist)', 'Singaporemma', 'Qiongthela', 'Anna Chennault', 'History of Laos', \"Domino's Pizza\", 'Schima wallichii', 'Amleto Vespa', 'Odontopleura', 'Wu Mei-hung', 'Dimash Kudaibergen', 'Shouchangoceratinae', 'Rao Yi', 'Chinese Wand Exercise', 'Yin Zhuo', 'Marginellona gigas', 'Typhoon (2005 film)', 'Li Bai', 'Grandparent', 'Claude Corea', 'Agrarianism', 'Kunio Kishida', \"Hodgson's treecreeper\", 'Cherry Returns', 'Guanyin', 'Daughter of Emperor Xiaoming of Northern Wei']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLAIM</th>\n",
       "      <th>ELAPSED</th>\n",
       "      <th>PAGES_FOUND</th>\n",
       "      <th>KEYWORDS</th>\n",
       "      <th>LINKED_PAGES</th>\n",
       "      <th>LINKING_PAGES</th>\n",
       "      <th>N_LUCENE_HITS</th>\n",
       "      <th>HIT_SCORE_MIN</th>\n",
       "      <th>HIT_SCORE_25</th>\n",
       "      <th>HIT_SCORE_MEAN</th>\n",
       "      <th>HIT_SCORE_MEDIAN</th>\n",
       "      <th>HIT_SCORE_75</th>\n",
       "      <th>HIT_SCORE_MAX</th>\n",
       "      <th>HIT_SCORES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tridrepana hainana is a moth found in China th...</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>{Agabus aequalis, Sunken Plum, Myrica, Kunio K...</td>\n",
       "      <td>{Tridrepana hainana, the Drepanidae family, Ch...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>70</td>\n",
       "      <td>11.7252</td>\n",
       "      <td>12.029525</td>\n",
       "      <td>14.494609</td>\n",
       "      <td>12.4629</td>\n",
       "      <td>13.758575</td>\n",
       "      <td>33.459202</td>\n",
       "      <td>[33.45920181274414, 30.453800201416016, 25.158...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               CLAIM   ELAPSED                                        PAGES_FOUND                                           KEYWORDS LINKED_PAGES LINKING_PAGES  N_LUCENE_HITS  HIT_SCORE_MIN  HIT_SCORE_25  HIT_SCORE_MEAN  HIT_SCORE_MEDIAN  HIT_SCORE_75  HIT_SCORE_MAX                                         HIT_SCORES\n",
       "0  Tridrepana hainana is a moth found in China th...  00:00:00  {Agabus aequalis, Sunken Plum, Myrica, Kunio K...  {Tridrepana hainana, the Drepanidae family, Ch...           {}            {}             70        11.7252     12.029525       14.494609           12.4629     13.758575      33.459202  [33.45920181274414, 30.453800201416016, 25.158..."
      ]
     },
     "execution_count": 120,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_stats, hit_dictionary = get_relevant_pages(dataset[0][\"claim\"], FULL_CLAIM=True, SPACY_ENTITIES=True, CASE_ENTITIES=True, ADD_SYNONYMS=False, ADD_SIMILAR=False, \n",
    "            LINKED_PAGES=False, LINKING_PAGES=False, CATCH_VERBS=False, TFIDF_TOP=False, FIND_ALL=True, N_RESULTS=70)\n",
    "\n",
    "print([key for key in hit_dictionary])\n",
    "hit_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJFhuZZQeee6"
   },
   "outputs": [],
   "source": [
    "def get_answers(query, hit_dictionary):\n",
    "    for idx,v in hit_dictionary.items():\n",
    "        abs_dirty = v['abstract']\n",
    "        real_abs_dirty = v['real_abstract']\n",
    "        #abs_dirty = v['paragraph']\n",
    "        # looks like the abstract value can be an empty list\n",
    "        v['abstract_paragraphs'] = []\n",
    "        v['abstract_full'] = ''\n",
    "        v['real_abstract_full'] = ''\n",
    "        v['real_abstract_paragraphs']=[]\n",
    "\n",
    "        if abs_dirty:\n",
    "            # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
    "            # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
    "            # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
    "\n",
    "            if isinstance(abs_dirty, list):\n",
    "                for p in abs_dirty:\n",
    "                    v['abstract_paragraphs'].append(p['text'])\n",
    "                    v['abstract_full'] += p['text'] + ' \\n\\n'\n",
    "\n",
    "            # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
    "            if isinstance(abs_dirty, str):\n",
    "                v['abstract_paragraphs'].append(abs_dirty)\n",
    "                v['abstract_full'] += abs_dirty + ' \\n\\n'\n",
    "        if real_abs_dirty:\n",
    "            # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
    "            # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
    "            # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
    "\n",
    "\n",
    "            if isinstance(real_abs_dirty, list):\n",
    "                for p in real_abs_dirty:\n",
    "                    v['real_abstract_paragraphs'].append(p['text'])\n",
    "                    v['real_abstract_full'] += p['text'] + ' \\n\\n'\n",
    "\n",
    "            # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
    "            if isinstance(abs_dirty, str):\n",
    "                v['real_abstract_paragraphs'].append(real_abs_dirty)\n",
    "                v['real_abstract_full'] += real_abs_dirty + ' \\n\\n'\n",
    "        v[\"indexed_para\"]=v[\"abstract_full\"][len(v[\"real_abstract_full\"]):]\n",
    "\n",
    "    def embed_useT(module):\n",
    "        with tf.Graph().as_default():\n",
    "            sentences = tf.compat.v1.placeholder(tf.string)\n",
    "            embed = hub.Module(module)\n",
    "            embeddings = embed(sentences)\n",
    "            session = tf.compat.v1.train.MonitoredSession()\n",
    "        return lambda x: session.run(embeddings, {sentences: x})\n",
    "    embed_fn = embed_useT('/root/kaggle/working/sentence_wise_email/module/module_useT')\n",
    "\n",
    "    import numpy as np\n",
    "    #Reconstruct text for generating the answer text from the BERT result, do the necessary text preprocessing and construct the answer.\n",
    "    def reconstructText(tokens, start=0, stop=-1):\n",
    "        tokens = tokens[start: stop]\n",
    "        if '[SEP]' in tokens:\n",
    "            sepind = tokens.index('[SEP]')\n",
    "            tokens = tokens[sepind+1:]\n",
    "        txt = ' '.join(tokens)\n",
    "        txt = txt.replace(' ##', '')\n",
    "        txt = txt.replace('##', '')\n",
    "        txt = txt.strip()\n",
    "        txt = \" \".join(txt.split())\n",
    "        txt = txt.replace(' .', '.')\n",
    "        txt = txt.replace('( ', '(')\n",
    "        txt = txt.replace(' )', ')')\n",
    "        txt = txt.replace(' - ', '-')\n",
    "        txt_list = txt.split(' , ')\n",
    "        txt = ''\n",
    "        nTxtL = len(txt_list)\n",
    "        if nTxtL == 1:\n",
    "            return txt_list[0]\n",
    "        newList =[]\n",
    "        for i,t in enumerate(txt_list):\n",
    "            if i < nTxtL -1:\n",
    "                if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n",
    "                    newList += [t,',']\n",
    "                else:\n",
    "                    newList += [t, ', ']\n",
    "            else:\n",
    "                newList += [t]\n",
    "        return ''.join(newList)\n",
    "\n",
    "\n",
    "    def makeBERTSQuADPrediction(document, question):\n",
    "        ## we need to rewrite this function so that it chuncks the document into 250-300 word segments with\n",
    "        ## 50 word overlaps on either end so that it can understand and check longer abstracts\n",
    "        ## Get chuck of documents in order to overcome the BERT's max 512 token problem. First count the number of tokens. For example if it is\n",
    "        ##   larger than 2048 token determine the split size and overlapping token count. Split the document into pieces to fed into BERT encoding. Overlapping\n",
    "        ##      is necessary to maintain the coherence between the splits. If the token count is larger than some number 2560 token approximately this will fail.\n",
    "        nWords = len(document.split())\n",
    "        input_ids_all = QA_TOKENIZER.encode(question, document)\n",
    "        tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n",
    "        overlapFac = 1.1\n",
    "        if len(input_ids_all)*overlapFac > 2560:\n",
    "            nSearchWords = int(np.ceil(nWords/6))\n",
    "            fifth = int(np.ceil(nWords/5))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[fifth-int(nSearchWords*overlapFac/2):fifth+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[fifth*2-int(nSearchWords*overlapFac/2):fifth*2+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[fifth*3-int(nSearchWords*overlapFac/2):fifth*3+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[fifth*4-int(nSearchWords*overlapFac/2):fifth*4+int(fifth*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "\n",
    "        elif len(input_ids_all)*overlapFac > 2048:\n",
    "            nSearchWords = int(np.ceil(nWords/5))\n",
    "            quarter = int(np.ceil(nWords/4))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
    "            \n",
    "        elif len(input_ids_all)*overlapFac > 1536:\n",
    "            nSearchWords = int(np.ceil(nWords/4))\n",
    "            third = int(np.ceil(nWords/3))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
    "            \n",
    "        elif len(input_ids_all)*overlapFac > 1024:\n",
    "            nSearchWords = int(np.ceil(nWords/3))\n",
    "            middle = int(np.ceil(nWords/2))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                        ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),\n",
    "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "        elif len(input_ids_all)*overlapFac > 512:\n",
    "            nSearchWords = int(np.ceil(nWords/2))\n",
    "            docSplit = document.split()\n",
    "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "        else:\n",
    "            input_ids = [input_ids_all]\n",
    "        absTooLong = False    \n",
    "        \n",
    "        answers = []\n",
    "        cons = []\n",
    "        #print(input_ids)\n",
    "        for iptIds in input_ids:\n",
    "            tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n",
    "            #print(tokens)\n",
    "            sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)\n",
    "            num_seg_a = sep_index + 1\n",
    "            num_seg_b = len(iptIds) - num_seg_a\n",
    "            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "            assert len(segment_ids) == len(iptIds)\n",
    "            n_ids = len(segment_ids)\n",
    "            #print(n_ids)\n",
    "            if n_ids < 512:\n",
    "                outputs = QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
    "                                        token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n",
    "                #print(QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
    "                #                        token_type_ids=torch.tensor([segment_ids]).to(torch_device)))\n",
    "            else:\n",
    "                #this cuts off the text if its more than 512 words so it fits in model space\n",
    "                #need run multiple inferences for longer text. add to the todo. I will try to expand this to longer sequences if it is necessary. \n",
    "                print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n",
    "                absTooLong = True\n",
    "                outputs= QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device), \n",
    "                                        token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))\n",
    "            start_scores=outputs.start_logits\n",
    "            end_scores=outputs.end_logits\n",
    "            start_scores = start_scores[:,1:-1]\n",
    "            end_scores = end_scores[:,1:-1]\n",
    "            answer_start = torch.argmax(start_scores)\n",
    "            answer_end = torch.argmax(end_scores)\n",
    "            #print(answer_start, answer_end)\n",
    "            answer = reconstructText(tokens, answer_start, answer_end+2)\n",
    "        \n",
    "            if answer.startswith('. ') or answer.startswith(', '):\n",
    "                answer = answer[2:]\n",
    "                \n",
    "            c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n",
    "            answers.append(answer)\n",
    "            cons.append(c)\n",
    "        \n",
    "        maxC = max(cons)\n",
    "        iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n",
    "        confidence = cons[iMaxC]\n",
    "        answer = answers[iMaxC]\n",
    "        \n",
    "        sep_index = tokens_all.index('[SEP]')\n",
    "        full_txt_tokens = tokens_all[sep_index+1:]\n",
    "        \n",
    "        abs_returned = reconstructText(full_txt_tokens)\n",
    "\n",
    "        ans={}\n",
    "        ans['answer'] = answer\n",
    "        #print(answer)\n",
    "        if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n",
    "            ans['confidence'] = -1000000\n",
    "        else:\n",
    "            #confidence = torch.max(start_scores) + torch.max(end_scores)\n",
    "            #confidence = np.log(confidence.item())\n",
    "            ans['confidence'] = confidence\n",
    "        #ans['start'] = answer_start.item()\n",
    "        #ans['end'] = answer_end.item()\n",
    "        ans['abstract_bert'] = abs_returned\n",
    "        ans['abs_too_long'] = absTooLong\n",
    "        return ans\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    def searchAbstracts(hit_dictionary, question):\n",
    "        abstractResults = {}\n",
    "        for k,v in tqdm(hit_dictionary.items()):\n",
    "            abstract = v['abstract_full']\n",
    "            indexed_para=v['indexed_para']\n",
    "            if abstract:\n",
    "                ans = makeBERTSQuADPrediction(abstract, question)\n",
    "                if ans['answer']:\n",
    "                    confidence = ans['confidence']\n",
    "                    abstractResults[confidence]={}\n",
    "                    abstractResults[confidence]['answer'] = ans['answer']\n",
    "                    #abstractResults[confidence]['start'] = ans['start']\n",
    "                    #abstractResults[confidence]['end'] = ans['end']\n",
    "                    abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n",
    "                    abstractResults[confidence]['idx'] = k\n",
    "                    abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n",
    "\n",
    "                    \n",
    "        cList = list(abstractResults.keys())\n",
    "\n",
    "        if cList:\n",
    "            maxScore = max(cList)\n",
    "            total = 0.0\n",
    "            exp_scores = []\n",
    "            for c in cList:\n",
    "                s = np.exp(c-maxScore)\n",
    "                exp_scores.append(s)\n",
    "            total = sum(exp_scores)\n",
    "            for i,c in enumerate(cList):\n",
    "                abstractResults[exp_scores[i]/total] = abstractResults.pop(c)\n",
    "        return abstractResults\n",
    "\n",
    "    answers = searchAbstracts(hit_dictionary, query)\n",
    "    \n",
    "    FIND_PDFS=False \n",
    "    SEARCH_MEDRXIV=False\n",
    "\n",
    "    workingPath = '/root/kaggle/working'\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    if FIND_PDFS:\n",
    "        from metapub import UrlReverse\n",
    "        from metapub import FindIt\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "    #from summarizer import Summarizer\n",
    "    #summarizerModel = Summarizer()\n",
    "    def displayResults(hit_dictionary, answers, question):\n",
    "        \n",
    "        question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n",
    "        #all_HTML_txt = question_HTML\n",
    "        confidence = list(answers.keys())\n",
    "        confidence.sort(reverse=True)\n",
    "        \n",
    "        confidence = list(answers.keys())\n",
    "        confidence.sort(reverse=True)\n",
    "        \n",
    "\n",
    "        for c in confidence:\n",
    "            if c>0 and c <= 1 and len(answers[c]['answer']) != 0:\n",
    "                if 'idx' not in  answers[c]:\n",
    "                    continue\n",
    "                rowData = []\n",
    "                idx = answers[c]['idx']\n",
    "                title = hit_dictionary[idx]['title']\n",
    "\n",
    "                \n",
    "                full_abs = answers[c]['abstract_bert']\n",
    "                bert_ans = answers[c]['answer']\n",
    "                \n",
    "                \n",
    "                #split_abs = full_abs.split(bert_ans)\n",
    "                #x=(split_abs[0][:split_abs[0].rfind('. ')+1])\n",
    "                #sentance_beginning = x[x.rfind('. ')+1:] + split_abs[0][split_abs[0].rfind('. ')+1:]\n",
    "                #sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n",
    "                x=''\n",
    "                y=''\n",
    "                z=''\n",
    "                t=''\n",
    "\n",
    "                # print(bert_ans)\n",
    "                split_abs = full_abs.split(bert_ans)\n",
    "                sentance_beginning = split_abs[0].split(\" ~ ~ \")[-1]\n",
    "                sentance_end = split_abs[1].split(\" ~ ~ \")[0]\n",
    "                    \n",
    "                sentance_full = sentance_beginning + bert_ans+ sentance_end\n",
    "                print(sentance_full)\n",
    "\n",
    "                answer_ids = re.findall(r\"((sentence|table|cell|section|list|item)( _ [0-9]+)+ )\", sentance_full)\n",
    "                answer_ids_formatted = []\n",
    "\n",
    "                for ids in answer_ids:\n",
    "                    if isinstance(ids, str):\n",
    "                        candidate = ids.replace(\" \", \"\")\n",
    "                        if \"_\" in candidate and candidate[0] != \"_\" and candidate not in [\"sentence\", \"table\", \"cell\", \"section\", \"list\", \"item\"]:  # very hacky\n",
    "                            answer_ids_formatted.append(candidate)\n",
    "                    else:\n",
    "                        for id in ids:\n",
    "                            candidate = id.replace(\" \", \"\")\n",
    "                            if \"_\" in candidate and candidate[0] != \"_\" and candidate not in [\"sentence\", \"table\", \"cell\", \"section\", \"list\", \"item\"]:  # very hacky\n",
    "                                answer_ids_formatted.append(candidate)\n",
    "                print(answer_ids_formatted)\n",
    "\n",
    "                answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n",
    "                answers[c]['partial_answer'] = bert_ans+sentance_end\n",
    "                answers[c]['sentence_beginning'] = sentance_beginning\n",
    "                answers[c]['sentence_end'] = sentance_end\n",
    "                answers[c]['title'] = title\n",
    "            else:\n",
    "                answers.pop(c)\n",
    "        \n",
    "        \n",
    "        ## now rerank based on semantic similarity of the answers to the question\n",
    "        ## Universal sentence encoder\n",
    "        cList = list(answers.keys())\n",
    "        allAnswers = [answers[c]['full_answer'] for c in cList]\n",
    "        \n",
    "        messages = [question]+allAnswers\n",
    "        \n",
    "        encoding_matrix = embed_fn(messages)\n",
    "        similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n",
    "        rankings = similarity_matrix[1:,0]\n",
    "        \n",
    "        for i,c in enumerate(cList):\n",
    "            answers[rankings[i]] = answers.pop(c)\n",
    "\n",
    "        ## now form pandas dv\n",
    "        confidence = list(answers.keys())\n",
    "        confidence.sort(reverse=True)\n",
    "        pandasData = []\n",
    "        ranked_aswers = []\n",
    "        full_abs_list=[]\n",
    "        for c in confidence:\n",
    "            rowData=[]\n",
    "            title = answers[c]['title']\n",
    "            idx = answers[c]['idx']\n",
    "            rowData += [idx]            \n",
    "            sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='red'>\"+answers[c]['answer']+\"</font> \"+answers[c]['sentence_end']+'</div>'\n",
    "            answer_key = answers[c]['answer'].split(\"\\t\")[-1].strip() if \"\\t\" in answers[c]['answer'] else answers[c]['answer'].strip()\n",
    "\n",
    "            # rowData += [sentance_html, answer_key, c,title]\n",
    "            rowData += [sentance_html, c,title]\n",
    "            pandasData.append(rowData)\n",
    "            ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n",
    "            full_abs_list.append(' '.join([answers[c]['abstract_bert']]))\n",
    "        \n",
    "        if FIND_PDFS or SEARCH_MEDRXIV:\n",
    "            pdata2 = []\n",
    "            pm_ids = []\n",
    "            for rowData in pandasData:\n",
    "                rd = rowData\n",
    "                idx = rowData[0]\n",
    "        else:\n",
    "            pdata2 = pandasData\n",
    "            \n",
    "        \n",
    "        display(HTML(question_HTML))\n",
    "\n",
    "        if USE_SUMMARY:\n",
    "            ## try generating an exacutive summary with bart abstractive summarizer\n",
    "            allAnswersTxt = ' '.join(ranked_aswers[:10]).replace('\\n','')\n",
    "        #    exec_sum = summarizerModel(allAnswersTxt, min_length=1, max_length=500)    \n",
    "        #   execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>BERT Extractive Summary:</b>: '+exec_sum+'</div>'\n",
    "\n",
    "            answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=2048)['input_ids'].to(torch_device)\n",
    "            #answers_input_ids = SUMMARY_TOKENIZER.prepare_seq2seq_batch([allAnswersTxt], return_tensors='pt', truncation=True, padding='longest')['input_ids'].to(torch_device)\n",
    "            summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n",
    "                                                num_beams=10,\n",
    "                                                length_penalty=1.1,\n",
    "                                                max_length=2048,\n",
    "                                                min_length=64,\n",
    "                                                no_repeat_ngram_size=0,\n",
    "                                                    do_sample=False )\n",
    "\n",
    "            exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "            execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>BART Abstractive Summary:</b>: '+exec_sum+'</div>'\n",
    "            display(HTML(execSum_HTML))\n",
    "            warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 12px; padding-bottom:12px; color:#CCCC00; margin-top:1pt\"> Warning this is an autogenerated summary based on semantic search of abstracts, always examine the sources before accepting this conclusion.  If the evidence only mentions topic in passing or the evidence is not clear, the summary will likely not clearly answer the question.</div>'\n",
    "            display(HTML(warning_HTML))\n",
    "\n",
    "    #    display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>Body of Evidence:</b></div>'))\n",
    "        \n",
    "        if FIND_PDFS or SEARCH_MEDRXIV:\n",
    "            # df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Answer Key', 'Confidence', 'Title/Link'])\n",
    "            df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])\n",
    "        else:\n",
    "            # df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Answer Key', 'Confidence', 'Title/Link'])\n",
    "            df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])\n",
    "            \n",
    "        display(HTML(df.to_html(render_links=True, escape=False)))\n",
    "        return full_abs_list,ranked_aswers,pandasData\n",
    "\n",
    "    return displayResults(hit_dictionary, answers, query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QEp2LU2fbmgl",
    "outputId": "cacf0c44-1134-4f1f-dff6-e3bb4c99fd6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 379 words long.  There are 514 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 4/70 [00:00<00:06,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 537 words long.  There are 544 tokens\n",
      "****** warning only considering first 512 tokens, document is 537 words long.  There are 515 tokens\n",
      "****** warning only considering first 512 tokens, document is 542 words long.  There are 516 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 6/70 [00:00<00:06,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1026 words long.  There are 662 tokens\n",
      "****** warning only considering first 512 tokens, document is 3029 words long.  There are 1418 tokens\n",
      "****** warning only considering first 512 tokens, document is 3029 words long.  There are 1322 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 7/70 [00:00<00:09,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 3029 words long.  There are 1465 tokens\n",
      "****** warning only considering first 512 tokens, document is 3029 words long.  There are 1344 tokens\n",
      "****** warning only considering first 512 tokens, document is 3029 words long.  There are 1231 tokens\n",
      "****** warning only considering first 512 tokens, document is 3029 words long.  There are 1190 tokens\n",
      "****** warning only considering first 512 tokens, document is 455 words long.  There are 519 tokens\n",
      "****** warning only considering first 512 tokens, document is 552 words long.  There are 538 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 14/70 [00:01<00:06,  9.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1783 words long.  There are 742 tokens\n",
      "****** warning only considering first 512 tokens, document is 1783 words long.  There are 699 tokens\n",
      "****** warning only considering first 512 tokens, document is 1783 words long.  There are 762 tokens\n",
      "****** warning only considering first 512 tokens, document is 1783 words long.  There are 702 tokens\n",
      "****** warning only considering first 512 tokens, document is 1783 words long.  There are 670 tokens\n",
      "****** warning only considering first 512 tokens, document is 1783 words long.  There are 776 tokens\n",
      "****** warning only considering first 512 tokens, document is 919 words long.  There are 514 tokens\n",
      "****** warning only considering first 512 tokens, document is 919 words long.  There are 519 tokens\n",
      "****** warning only considering first 512 tokens, document is 919 words long.  There are 616 tokens\n",
      "****** warning only considering first 512 tokens, document is 1553 words long.  There are 578 tokens\n",
      "****** warning only considering first 512 tokens, document is 1553 words long.  There are 649 tokens\n",
      "****** warning only considering first 512 tokens, document is 1553 words long.  There are 599 tokens\n",
      "****** warning only considering first 512 tokens, document is 1553 words long.  There are 513 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 16/70 [00:01<00:06,  8.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1944 words long.  There are 837 tokens\n",
      "****** warning only considering first 512 tokens, document is 1944 words long.  There are 841 tokens\n",
      "****** warning only considering first 512 tokens, document is 1944 words long.  There are 691 tokens\n",
      "****** warning only considering first 512 tokens, document is 1944 words long.  There are 958 tokens\n",
      "****** warning only considering first 512 tokens, document is 1944 words long.  There are 1357 tokens\n",
      "****** warning only considering first 512 tokens, document is 1944 words long.  There are 1159 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 18/70 [00:02<00:06,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 8890 words long.  There are 2883 tokens\n",
      "****** warning only considering first 512 tokens, document is 8890 words long.  There are 3109 tokens\n",
      "****** warning only considering first 512 tokens, document is 8890 words long.  There are 2550 tokens\n",
      "****** warning only considering first 512 tokens, document is 8890 words long.  There are 2563 tokens\n",
      "****** warning only considering first 512 tokens, document is 8890 words long.  There are 2669 tokens\n",
      "****** warning only considering first 512 tokens, document is 8890 words long.  There are 2889 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 19/70 [00:02<00:11,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 6426 words long.  There are 2458 tokens\n",
      "****** warning only considering first 512 tokens, document is 6426 words long.  There are 2896 tokens\n",
      "****** warning only considering first 512 tokens, document is 6426 words long.  There are 2685 tokens\n",
      "****** warning only considering first 512 tokens, document is 6426 words long.  There are 2287 tokens\n",
      "****** warning only considering first 512 tokens, document is 6426 words long.  There are 2402 tokens\n",
      "****** warning only considering first 512 tokens, document is 6426 words long.  There are 2174 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 22/70 [00:03<00:10,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 794 words long.  There are 544 tokens\n",
      "****** warning only considering first 512 tokens, document is 794 words long.  There are 552 tokens\n",
      "****** warning only considering first 512 tokens, document is 1727 words long.  There are 688 tokens\n",
      "****** warning only considering first 512 tokens, document is 1727 words long.  There are 693 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 23/70 [00:03<00:10,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1727 words long.  There are 626 tokens\n",
      "****** warning only considering first 512 tokens, document is 1727 words long.  There are 565 tokens\n",
      "****** warning only considering first 512 tokens, document is 1727 words long.  There are 742 tokens\n",
      "****** warning only considering first 512 tokens, document is 1727 words long.  There are 615 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 24/70 [00:03<00:10,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 3344 words long.  There are 1470 tokens\n",
      "****** warning only considering first 512 tokens, document is 3344 words long.  There are 1441 tokens\n",
      "****** warning only considering first 512 tokens, document is 3344 words long.  There are 1543 tokens\n",
      "****** warning only considering first 512 tokens, document is 3344 words long.  There are 1245 tokens\n",
      "****** warning only considering first 512 tokens, document is 3344 words long.  There are 1128 tokens\n",
      "****** warning only considering first 512 tokens, document is 3344 words long.  There are 1047 tokens\n",
      "****** warning only considering first 512 tokens, document is 9205 words long.  There are 3772 tokens\n",
      "****** warning only considering first 512 tokens, document is 9205 words long.  There are 3506 tokens\n",
      "****** warning only considering first 512 tokens, document is 9205 words long.  There are 3322 tokens\n",
      "****** warning only considering first 512 tokens, document is 9205 words long.  There are 3865 tokens\n",
      "****** warning only considering first 512 tokens, document is 9205 words long.  There are 3689 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 26/70 [00:04<00:11,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 9205 words long.  There are 3617 tokens\n",
      "****** warning only considering first 512 tokens, document is 1177 words long.  There are 589 tokens\n",
      "****** warning only considering first 512 tokens, document is 1177 words long.  There are 563 tokens\n",
      "****** warning only considering first 512 tokens, document is 1177 words long.  There are 582 tokens\n",
      "****** warning only considering first 512 tokens, document is 6559 words long.  There are 2178 tokens\n",
      "****** warning only considering first 512 tokens, document is 6559 words long.  There are 2476 tokens\n",
      "****** warning only considering first 512 tokens, document is 6559 words long.  There are 2627 tokens\n",
      "****** warning only considering first 512 tokens, document is 6559 words long.  There are 2405 tokens\n",
      "****** warning only considering first 512 tokens, document is 6559 words long.  There are 2219 tokens\n",
      "****** warning only considering first 512 tokens, document is 6559 words long.  There are 2107 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 29/70 [00:04<00:09,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 6972 words long.  There are 2513 tokens\n",
      "****** warning only considering first 512 tokens, document is 6972 words long.  There are 2483 tokens\n",
      "****** warning only considering first 512 tokens, document is 6972 words long.  There are 2323 tokens\n",
      "****** warning only considering first 512 tokens, document is 6972 words long.  There are 2290 tokens\n",
      "****** warning only considering first 512 tokens, document is 6972 words long.  There are 3090 tokens\n",
      "****** warning only considering first 512 tokens, document is 6972 words long.  There are 2878 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 30/70 [00:05<00:11,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 5584 words long.  There are 2253 tokens\n",
      "****** warning only considering first 512 tokens, document is 5584 words long.  There are 2342 tokens\n",
      "****** warning only considering first 512 tokens, document is 5584 words long.  There are 1956 tokens\n",
      "****** warning only considering first 512 tokens, document is 5584 words long.  There are 1873 tokens\n",
      "****** warning only considering first 512 tokens, document is 5584 words long.  There are 2340 tokens\n",
      "****** warning only considering first 512 tokens, document is 5584 words long.  There are 2329 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 32/70 [00:05<00:10,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2239 words long.  There are 969 tokens\n",
      "****** warning only considering first 512 tokens, document is 2239 words long.  There are 917 tokens\n",
      "****** warning only considering first 512 tokens, document is 2239 words long.  There are 900 tokens\n",
      "****** warning only considering first 512 tokens, document is 2239 words long.  There are 837 tokens\n",
      "****** warning only considering first 512 tokens, document is 2239 words long.  There are 980 tokens\n",
      "****** warning only considering first 512 tokens, document is 2239 words long.  There are 906 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 34/70 [00:05<00:06,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 12518 words long.  There are 4555 tokens\n",
      "****** warning only considering first 512 tokens, document is 12518 words long.  There are 4311 tokens\n",
      "****** warning only considering first 512 tokens, document is 12518 words long.  There are 4454 tokens\n",
      "****** warning only considering first 512 tokens, document is 12518 words long.  There are 4015 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 36/70 [00:06<00:08,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 12518 words long.  There are 4224 tokens\n",
      "****** warning only considering first 512 tokens, document is 12518 words long.  There are 3760 tokens\n",
      "****** warning only considering first 512 tokens, document is 830 words long.  There are 648 tokens\n",
      "****** warning only considering first 512 tokens, document is 830 words long.  There are 536 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 37/70 [00:06<00:07,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1795 words long.  There are 788 tokens\n",
      "****** warning only considering first 512 tokens, document is 1795 words long.  There are 764 tokens\n",
      "****** warning only considering first 512 tokens, document is 1795 words long.  There are 722 tokens\n",
      "****** warning only considering first 512 tokens, document is 1795 words long.  There are 609 tokens\n",
      "****** warning only considering first 512 tokens, document is 1795 words long.  There are 652 tokens\n",
      "****** warning only considering first 512 tokens, document is 1795 words long.  There are 646 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 38/70 [00:07<00:07,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2697 words long.  There are 988 tokens\n",
      "****** warning only considering first 512 tokens, document is 2697 words long.  There are 1231 tokens\n",
      "****** warning only considering first 512 tokens, document is 2697 words long.  There are 1235 tokens\n",
      "****** warning only considering first 512 tokens, document is 2697 words long.  There are 1370 tokens\n",
      "****** warning only considering first 512 tokens, document is 2697 words long.  There are 1016 tokens\n",
      "****** warning only considering first 512 tokens, document is 2697 words long.  There are 949 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 39/70 [00:07<00:07,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1358 words long.  There are 557 tokens\n",
      "****** warning only considering first 512 tokens, document is 1358 words long.  There are 558 tokens\n",
      "****** warning only considering first 512 tokens, document is 1358 words long.  There are 536 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 40/70 [00:07<00:05,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 306 words long.  There are 513 tokens\n",
      "****** warning only considering first 512 tokens, document is 2015 words long.  There are 672 tokens\n",
      "****** warning only considering first 512 tokens, document is 2015 words long.  There are 820 tokens\n",
      "****** warning only considering first 512 tokens, document is 2015 words long.  There are 605 tokens\n",
      "****** warning only considering first 512 tokens, document is 2015 words long.  There are 663 tokens\n",
      "****** warning only considering first 512 tokens, document is 2015 words long.  There are 1041 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 43/70 [00:07<00:04,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2015 words long.  There are 980 tokens\n",
      "****** warning only considering first 512 tokens, document is 786 words long.  There are 526 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 44/70 [00:07<00:04,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2425 words long.  There are 894 tokens\n",
      "****** warning only considering first 512 tokens, document is 2425 words long.  There are 1131 tokens\n",
      "****** warning only considering first 512 tokens, document is 2425 words long.  There are 1267 tokens\n",
      "****** warning only considering first 512 tokens, document is 2425 words long.  There are 947 tokens\n",
      "****** warning only considering first 512 tokens, document is 2425 words long.  There are 960 tokens\n",
      "****** warning only considering first 512 tokens, document is 2425 words long.  There are 946 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 45/70 [00:08<00:04,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 5503 words long.  There are 1926 tokens\n",
      "****** warning only considering first 512 tokens, document is 5503 words long.  There are 2053 tokens\n",
      "****** warning only considering first 512 tokens, document is 5503 words long.  There are 1979 tokens\n",
      "****** warning only considering first 512 tokens, document is 5503 words long.  There are 1967 tokens\n",
      "****** warning only considering first 512 tokens, document is 5503 words long.  There are 1980 tokens\n",
      "****** warning only considering first 512 tokens, document is 5503 words long.  There are 1787 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 48/70 [00:08<00:03,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 8735 words long.  There are 3493 tokens\n",
      "****** warning only considering first 512 tokens, document is 8735 words long.  There are 4005 tokens\n",
      "****** warning only considering first 512 tokens, document is 8735 words long.  There are 4322 tokens\n",
      "****** warning only considering first 512 tokens, document is 8735 words long.  There are 4153 tokens\n",
      "****** warning only considering first 512 tokens, document is 8735 words long.  There are 4544 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 49/70 [00:09<00:05,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 8735 words long.  There are 3930 tokens\n",
      "****** warning only considering first 512 tokens, document is 3442 words long.  There are 1359 tokens\n",
      "****** warning only considering first 512 tokens, document is 3442 words long.  There are 1562 tokens\n",
      "****** warning only considering first 512 tokens, document is 3442 words long.  There are 1315 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 50/70 [00:09<00:05,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 3442 words long.  There are 1394 tokens\n",
      "****** warning only considering first 512 tokens, document is 3442 words long.  There are 1358 tokens\n",
      "****** warning only considering first 512 tokens, document is 3442 words long.  There are 1435 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 52/70 [00:09<00:04,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1170 words long.  There are 609 tokens\n",
      "****** warning only considering first 512 tokens, document is 1170 words long.  There are 585 tokens\n",
      "****** warning only considering first 512 tokens, document is 1170 words long.  There are 622 tokens\n",
      "****** warning only considering first 512 tokens, document is 1170 words long.  There are 547 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 55/70 [00:10<00:03,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 4923 words long.  There are 1849 tokens\n",
      "****** warning only considering first 512 tokens, document is 4923 words long.  There are 2152 tokens\n",
      "****** warning only considering first 512 tokens, document is 4923 words long.  There are 2237 tokens\n",
      "****** warning only considering first 512 tokens, document is 4923 words long.  There are 2330 tokens\n",
      "****** warning only considering first 512 tokens, document is 4923 words long.  There are 1846 tokens\n",
      "****** warning only considering first 512 tokens, document is 4923 words long.  There are 1397 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 57/70 [00:10<00:02,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1451 words long.  There are 625 tokens\n",
      "****** warning only considering first 512 tokens, document is 1451 words long.  There are 572 tokens\n",
      "****** warning only considering first 512 tokens, document is 1451 words long.  There are 513 tokens\n",
      "****** warning only considering first 512 tokens, document is 1451 words long.  There are 513 tokens\n",
      "****** warning only considering first 512 tokens, document is 1451 words long.  There are 666 tokens\n",
      "****** warning only considering first 512 tokens, document is 1451 words long.  There are 750 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 58/70 [00:10<00:02,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 2279 words long.  There are 718 tokens\n",
      "****** warning only considering first 512 tokens, document is 2279 words long.  There are 818 tokens\n",
      "****** warning only considering first 512 tokens, document is 2279 words long.  There are 876 tokens\n",
      "****** warning only considering first 512 tokens, document is 2279 words long.  There are 860 tokens\n",
      "****** warning only considering first 512 tokens, document is 2279 words long.  There are 741 tokens\n",
      "****** warning only considering first 512 tokens, document is 2279 words long.  There are 637 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 61/70 [00:10<00:01,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 385 words long.  There are 559 tokens\n",
      "****** warning only considering first 512 tokens, document is 1058 words long.  There are 631 tokens\n",
      "****** warning only considering first 512 tokens, document is 1058 words long.  There are 545 tokens\n",
      "****** warning only considering first 512 tokens, document is 5908 words long.  There are 2356 tokens\n",
      "****** warning only considering first 512 tokens, document is 5908 words long.  There are 2464 tokens\n",
      "****** warning only considering first 512 tokens, document is 5908 words long.  There are 2139 tokens\n",
      "****** warning only considering first 512 tokens, document is 5908 words long.  There are 2101 tokens\n",
      "****** warning only considering first 512 tokens, document is 5908 words long.  There are 2296 tokens\n",
      "****** warning only considering first 512 tokens, document is 5908 words long.  There are 2327 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 63/70 [00:11<00:01,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 3554 words long.  There are 1449 tokens\n",
      "****** warning only considering first 512 tokens, document is 3554 words long.  There are 1660 tokens\n",
      "****** warning only considering first 512 tokens, document is 3554 words long.  There are 1314 tokens\n",
      "****** warning only considering first 512 tokens, document is 3554 words long.  There are 1033 tokens\n",
      "****** warning only considering first 512 tokens, document is 3554 words long.  There are 1000 tokens\n",
      "****** warning only considering first 512 tokens, document is 3554 words long.  There are 988 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 64/70 [00:11<00:01,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 1753 words long.  There are 619 tokens\n",
      "****** warning only considering first 512 tokens, document is 1753 words long.  There are 591 tokens\n",
      "****** warning only considering first 512 tokens, document is 1753 words long.  There are 601 tokens\n",
      "****** warning only considering first 512 tokens, document is 1753 words long.  There are 628 tokens\n",
      "****** warning only considering first 512 tokens, document is 1753 words long.  There are 562 tokens\n",
      "****** warning only considering first 512 tokens, document is 1753 words long.  There are 619 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 65/70 [00:11<00:01,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 3883 words long.  There are 1365 tokens\n",
      "****** warning only considering first 512 tokens, document is 3883 words long.  There are 1613 tokens\n",
      "****** warning only considering first 512 tokens, document is 3883 words long.  There are 1262 tokens\n",
      "****** warning only considering first 512 tokens, document is 3883 words long.  There are 1578 tokens\n",
      "****** warning only considering first 512 tokens, document is 3883 words long.  There are 1668 tokens\n",
      "****** warning only considering first 512 tokens, document is 3883 words long.  There are 1323 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 67/70 [00:12<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 540 words long.  There are 557 tokens\n",
      "****** warning only considering first 512 tokens, document is 7887 words long.  There are 3666 tokens\n",
      "****** warning only considering first 512 tokens, document is 7887 words long.  There are 3431 tokens\n",
      "****** warning only considering first 512 tokens, document is 7887 words long.  There are 2518 tokens\n",
      "****** warning only considering first 512 tokens, document is 7887 words long.  There are 2692 tokens\n",
      "****** warning only considering first 512 tokens, document is 7887 words long.  There are 3154 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 69/70 [00:12<00:00,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** warning only considering first 512 tokens, document is 7887 words long.  There are 3065 tokens\n",
      "****** warning only considering first 512 tokens, document is 1229 words long.  There are 564 tokens\n",
      "****** warning only considering first 512 tokens, document is 1229 words long.  There are 532 tokens\n",
      "****** warning only considering first 512 tokens, document is 1229 words long.  There are 558 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:12<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence _ 5 a traumatized young girl is found alive in the basement, later identified as cherry, who was abducted 12 years ago. ~ ~ sentence _ 6 her family, who has since resided in hong kong, feels agitated upon the shocking news. ~ ~ sentence _ 7 in the wake of the trauma, cherry appears to lose all recollection of her family and childhood. ~ ~ sentence _ 8 to cope with the aftermath and get her life back on track, her family takes her back to hong kong to start anew. ~ ~ sentence _ 9 while cherry is making progress in recovery, strange events occur that threaten to harm those close to her. ~ ~ sentence _ 10 complicated by the appearance of a mysterious stranger, police investigator starts to look into cherry ' s past for lead. ~ ~ sentence _ 11 but what he is about to discover, no one is prepared for the fatal outcome that follows... ~ ~ section _ 1 cast ~ ~ section _ 2 reception ~ ~ sentence _ 12 the film has grossed cna¥33. 3 million in china. ~ ~\n",
      "['sentence_5', 'sentence_6', 'sentence_7', 'sentence_8', 'sentence_9', 'sentence_10', 'sentence_11', 'section_1', 'section_2', 'sentence_12']\n",
      "sentence _ 2 this species can be found in the lower half of the north island and western alpine areas of the south \n",
      "['sentence_2']\n",
      "sentence _ 0 bambusiomyces is a fungal [ [ genus | genus ] ] in the family [ [ ustilaginaceae | ustilaginaceae ] ]. ~ ~ sentence _ 1 it was circumscribed in 2011 to contain the [ [ smut _ fungus | smut fungus ] ] formerly known as ustilago shiraina, originally described by german mycologist [ [ paul _ christoph _ hennings | paul christoph hennings ] ] from japan in 1900. ~ ~ sentence _ 2 the fungus was originally found growing on wild japanese bamboo, bambusa veitchii, but has since been isolated from several plants belonging to tribe [ [ bambuseae | bambuseae ] ] in the grass family poaceae. ~ ~ sentence _ 3 in addition to japan, it is also found in china and other locales in southeast asia, and has been [ [ introduced _ species | introduced ] ] to the united states. ~ ~\n",
      "['sentence_0', 'sentence_1', 'sentence_2', 'sentence_3']\n",
      "sentence _ 0 agabus aequalis is a species of predatory diving [ [ beetle | beetle ] ] belonging to the family [ [ dytiscidae | dytiscidae ] ].\n",
      "['sentence_0']\n",
      "sentence _ 2 it is found in northern indian, [ [ bangladesh | bangladesh ] ], [ [ nepal | nepal ] ], [ [ bhutan | bhutan ] ], southwestern china ([ [ tibet | tibet ] ], [ [ yunnan | yunnan ] ], [ [ guizhou | guizhou ] ], and guangxi), [ [ myanmar | myanmar ] ], [ [ thailand | thailand ] ], [ [ laos | laos ] ], and [ [ vietnam | vietnam ] ].\n",
      "['sentence_2']\n",
      "sentence _ 0 shouchangoceratinae is one of three [ [ ammonoid | ammonoid ] ] [ [ subfamilies | subfamilies ] ] of the [ [ family _ (biology) | family ] ] [ [ pseudohaloritidae | pseudohaloritidae ] ], which in turn is one of two families in the [ [ goniatitida | goniatitid superfamily ] ] [ [ pseudohaloritoidea | pseudohaloritoidea ] ]. ~ ~ sentence _ 1 the shouchangoceratinid ammonoids were found in marine environments throughout the world during the [ [ permian | permian ] ], particularly in [ [ china | china ] ]. ~ ~\n",
      "['sentence_0', 'sentence_1']\n",
      "item _ 0 _ 2 singaporemma bifurcatum lin & li, 2010 – china ~ ~ item _ 0 _ 3 singaporemma halongense lehtinen, 1981 – vietnam\n",
      "['item_0_2', 'item_0_3']\n",
      "sentence _ 0 elephantoceras is genus of a [ [ middle _ permian | middle permian ] ] [ [ ammonoidea | ammonite ] ] belonging to the [ [ goniatitida | goniatitid ] ] family [ [ pseudohaloritidae | pseudohaloritidae ] ]. ~ ~ sentence _ 1 fossils belonging to this genera were found in china. ~ ~\n",
      "['sentence_0', 'sentence_1']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: Tridrepana hainana is a moth found in China that belongs to the Drepanidae family of the order Lepidoptera.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lucene ID</th>\n",
       "      <th>BERT-SQuAD Answer with Highlights</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Title/Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agabus aequalis</td>\n",
       "      <td><div>sentence _  <font color='red'>0 agabus aequalis</font>  is a species of predatory diving [ [ beetle | beetle ] ] belonging to the family [ [ dytiscidae | dytiscidae ] ].</div></td>\n",
       "      <td>0.796799</td>\n",
       "      <td>Agabus aequalis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Metacrias erichrysa</td>\n",
       "      <td><div>sentence _ 2 this species can be found in the lower half of the north  <font color='red'>island</font>  and western alpine areas of the south </div></td>\n",
       "      <td>0.691027</td>\n",
       "      <td>Metacrias erichrysa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bambusiomyces</td>\n",
       "      <td><div> <font color='red'>sentence _ 0 bambusiomyces is a fungal [ [ genus | genus ] ] in the family [ [ ustilaginaceae | ustilaginaceae ] ]. ~ ~ sentence _ 1 it was circumscribed in 2011 to contain the [ [ smut _ fungus | smut fungus ] ] formerly known as ustilago shiraina, originally described by german mycologist [ [ paul _ christoph _ hennings | paul christoph hennings ] ] from japan in 1900. ~ ~ sentence _ 2 the fungus was originally found growing on wild japanese bamboo, bambusa veitchii, but has since been isolated from several plants belonging to tribe [ [ bambuseae | bambuseae ] ] in the grass family poaceae. ~ ~ sentence _ 3 in addition to japan, it is also found in china and other locales in southeast asia</font> , and has been [ [ introduced _ species | introduced ] ] to the united states. ~ ~</div></td>\n",
       "      <td>0.685733</td>\n",
       "      <td>Bambusiomyces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Schima wallichii</td>\n",
       "      <td><div>sentence _ 2 it is found in northern indian, [ [ bangladesh | bangladesh ] ], [ [ nepal | nepal ] ], [ [ bhutan | bhutan ] ],  <font color='red'>southwestern china ([ [ tibet | tibet ] ], [ [ yunnan | yunnan ] ], [ [ guizhou | guizhou ] ], and guangxi</font> ), [ [ myanmar | myanmar ] ], [ [ thailand | thailand ] ], [ [ laos | laos ] ], and [ [ vietnam | vietnam ] ].</div></td>\n",
       "      <td>0.677549</td>\n",
       "      <td>Schima wallichii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shouchangoceratinae</td>\n",
       "      <td><div> <font color='red'>sentence _ 0 shouchangoceratinae is one of three [ [ ammonoid | ammonoid ] ] [ [ subfamilies | subfamilies ] ] of the [ [ family _ (biology) | family ] ] [ [ pseudohaloritidae | pseudohaloritidae ] ], which in turn is one of two families in the [ [ goniatitida | goniatitid superfamily ] ] [ [ pseudohaloritoidea | pseudohaloritoidea ] ]. ~ ~ sentence _ 1 the shouchangoceratinid ammonoids were found in marine environments throughout the world during the [ [ permian | permian ] ], particularly in [ [ china | china ]</font>  ]. ~ ~</div></td>\n",
       "      <td>0.575411</td>\n",
       "      <td>Shouchangoceratinae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Elephantoceras</td>\n",
       "      <td><div> <font color='red'>sentence _ 0 elephantoceras is genus of a [ [ middle _ permian | middle permian ] ] [ [ ammonoidea | ammonite ] ] belonging to the [ [ goniatitida | goniatitid ] ] family [ [ pseudohaloritidae | pseudohaloritidae ] ]. ~ ~ sentence _ 1 fossils</font>  belonging to this genera were found in china. ~ ~</div></td>\n",
       "      <td>0.555315</td>\n",
       "      <td>Elephantoceras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Singaporemma</td>\n",
       "      <td><div>item _ 0 _ 2 singaporemma bifurcatum lin & li, 2010  <font color='red'>– china ~ ~ item _ 0 _ 3 singaporemma halongense lehtinen</font> , 1981 – vietnam</div></td>\n",
       "      <td>0.221622</td>\n",
       "      <td>Singaporemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cherry Returns</td>\n",
       "      <td><div>sentence _ 5 a traumatized young girl is found alive in the basement, later identified as  <font color='red'>cherry, who was abducted 12 years ago. ~ ~ sentence _ 6 her family, who has since resided in hong kong, feels agitated upon the shocking news. ~ ~ sentence _ 7 in the wake of the trauma, cherry appears to lose all recollection of her family and childhood. ~ ~ sentence _ 8 to cope with the aftermath and get her life back on track, her family takes her back to hong kong to start anew. ~ ~ sentence _ 9 while cherry is making progress in recovery, strange events occur that threaten to harm those close to her. ~ ~ sentence _ 10 complicated by the appearance of a mysterious stranger, police investigator starts to look into cherry ' s past for lead. ~ ~ sentence _ 11 but what he is about to discover, no one is prepared for the fatal outcome that follows... ~ ~ section _ 1 cast ~ ~ section _ 2 reception ~ ~ sentence _ 12 the film has grossed cna¥33. 3 million</font>  in china. ~ ~</div></td>\n",
       "      <td>0.180775</td>\n",
       "      <td>Cherry Returns</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '', 'predicted_label': 'REFUTES', 'predicted_evidence': [['Agabus aequalis', \"<div>sentence _  <font color='red'>0 agabus aequalis</font>  is a species of predatory diving [ [ beetle | beetle ] ] belonging to the family [ [ dytiscidae | dytiscidae ] ].</div>\", ''], ['Metacrias erichrysa', \"<div>sentence _ 2 this species can be found in the lower half of the north  <font color='red'>island</font>  and western alpine areas of the south </div>\", ''], ['Bambusiomyces', \"<div> <font color='red'>sentence _ 0 bambusiomyces is a fungal [ [ genus | genus ] ] in the family [ [ ustilaginaceae | ustilaginaceae ] ]. ~ ~ sentence _ 1 it was circumscribed in 2011 to contain the [ [ smut _ fungus | smut fungus ] ] formerly known as ustilago shiraina, originally described by german mycologist [ [ paul _ christoph _ hennings | paul christoph hennings ] ] from japan in 1900. ~ ~ sentence _ 2 the fungus was originally found growing on wild japanese bamboo, bambusa veitchii, but has since been isolated from several plants belonging to tribe [ [ bambuseae | bambuseae ] ] in the grass family poaceae. ~ ~ sentence _ 3 in addition to japan, it is also found in china and other locales in southeast asia</font> , and has been [ [ introduced _ species | introduced ] ] to the united states. ~ ~</div>\", ''], ['Schima wallichii', \"<div>sentence _ 2 it is found in northern indian, [ [ bangladesh | bangladesh ] ], [ [ nepal | nepal ] ], [ [ bhutan | bhutan ] ],  <font color='red'>southwestern china ([ [ tibet | tibet ] ], [ [ yunnan | yunnan ] ], [ [ guizhou | guizhou ] ], and guangxi</font> ), [ [ myanmar | myanmar ] ], [ [ thailand | thailand ] ], [ [ laos | laos ] ], and [ [ vietnam | vietnam ] ].</div>\", ''], ['Shouchangoceratinae', \"<div> <font color='red'>sentence _ 0 shouchangoceratinae is one of three [ [ ammonoid | ammonoid ] ] [ [ subfamilies | subfamilies ] ] of the [ [ family _ (biology) | family ] ] [ [ pseudohaloritidae | pseudohaloritidae ] ], which in turn is one of two families in the [ [ goniatitida | goniatitid superfamily ] ] [ [ pseudohaloritoidea | pseudohaloritoidea ] ]. ~ ~ sentence _ 1 the shouchangoceratinid ammonoids were found in marine environments throughout the world during the [ [ permian | permian ] ], particularly in [ [ china | china ]</font>  ]. ~ ~</div>\", '']]}\n"
     ]
    }
   ],
   "source": [
    "for data in dataset[:1]:\n",
    "    claim = data[\"claim\"]\n",
    "    hit_stats, hit_dictionary = get_relevant_pages(claim, FULL_CLAIM=True, SPACY_ENTITIES=True, CASE_ENTITIES=True, ADD_SYNONYMS=False, ADD_SIMILAR=False, \n",
    "                            LINKED_PAGES=False, LINKING_PAGES=False, CATCH_VERBS=False, TFIDF_TOP=False, FIND_ALL=True, N_RESULTS=70)\n",
    "    \n",
    "    full_abs_list, ranked_aswers, pandasData = get_answers(claim, hit_dictionary)\n",
    "\n",
    "    # counter = 0\n",
    "    # for data in pandasData:\n",
    "    #     if data[2] > 0.4:\n",
    "    #         counter += 1\n",
    "    # print(counter)\n",
    "\n",
    "    evidence_probs, verdict = CheckEntailmentNeturalorContradict(ranked_aswers,claim,pandasData)\n",
    "\n",
    "    # print(len(list(evidence_probs)))\n",
    "    # print(verdict)\n",
    "\n",
    "    if verdict == \"False\":\n",
    "        predicted_label = \"REFUTES\"\n",
    "    elif verdict == \"True\":\n",
    "        predicted_label = \"SUPPORTS\"\n",
    "    else:\n",
    "        predicted_label = \"NOT ENOUGH INFO\"\n",
    "    \n",
    "    output = {\"id\": \"\", \"predicted_label\": predicted_label, \"predicted_evidence\": [[data[3], data[1], \"\"] for data in pandasData[:min([len(pandasData), 5])]]}\n",
    "\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FeverAnseriniQAandFormattedTest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
