{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeverAnseriniQAandFormattedTest.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7-FGu2I_yqT-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dbae76bfed2e480fa1aa509fd8e5a1d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c7779e97542b48df8179bd7418b4c7b9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_045576b7b0234868a9b24e42e0ffffd7",
              "IPY_MODEL_bfefd72888bb4a5085912a5c8d73e227"
            ]
          }
        },
        "c7779e97542b48df8179bd7418b4c7b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "045576b7b0234868a9b24e42e0ffffd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1b555339a47f4ebbac0e0d50f58ad278",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 477,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 477,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_57c4c91d97f94adf8820ee979fe1f6dd"
          }
        },
        "bfefd72888bb4a5085912a5c8d73e227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c7ee38880f6a49a48e4bb7796d497ec3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 477/477 [00:00&lt;00:00, 1.26kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9021e6e59422426d8077b5ab0564e9fa"
          }
        },
        "1b555339a47f4ebbac0e0d50f58ad278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "57c4c91d97f94adf8820ee979fe1f6dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7ee38880f6a49a48e4bb7796d497ec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9021e6e59422426d8077b5ab0564e9fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3287e8081b1849f6b70c87f69f921b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9309308fc6114033b3ebeefb7f7e4418",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_24b7a67fdda642c6afbb350b870635d8",
              "IPY_MODEL_2400583964f04a57ae49b2d9e9ad2c0c"
            ]
          }
        },
        "9309308fc6114033b3ebeefb7f7e4418": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24b7a67fdda642c6afbb350b870635d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b904c9d7e2974e7794a2955d022963b2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7c6cabc8deb482cb19e0679517951a0"
          }
        },
        "2400583964f04a57ae49b2d9e9ad2c0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0a1e62fb44f24095a5cde0ecd0a17c45",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:01&lt;00:00, 132kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5afd5b91a9c84a99ab4a37d929d78846"
          }
        },
        "b904c9d7e2974e7794a2955d022963b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7c6cabc8deb482cb19e0679517951a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a1e62fb44f24095a5cde0ecd0a17c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5afd5b91a9c84a99ab4a37d929d78846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cf21b95828c44d8c8186711679b6613d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_29cd0b999e7e4799b3b0a25fb5f71df8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3be1edc1cd9747bd96c007c2a8af8e7e",
              "IPY_MODEL_fe29744383b845e88ce9e46db448227c"
            ]
          }
        },
        "29cd0b999e7e4799b3b0a25fb5f71df8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3be1edc1cd9747bd96c007c2a8af8e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1af2f1b478aa4822880cc082c93d4070",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433297484,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433297484,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36978303dc5a4d94a5505125e5fb8ee2"
          }
        },
        "fe29744383b845e88ce9e46db448227c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9959ea5acc65429b9784dd9cef98c999",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433M/433M [00:17&lt;00:00, 24.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_82f90420c26d4066ac400ffd3f2913be"
          }
        },
        "1af2f1b478aa4822880cc082c93d4070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36978303dc5a4d94a5505125e5fb8ee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9959ea5acc65429b9784dd9cef98c999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "82f90420c26d4066ac400ffd3f2913be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-FGu2I_yqT-"
      },
      "source": [
        "# Base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQtJd7m0H_gC"
      },
      "source": [
        "%%capture\n",
        "!apt-get install maven -qq"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKkh78zferE8"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import sys"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUoccBGhIaiP"
      },
      "source": [
        "%%capture\n",
        "!git clone --recurse-submodules https://github.com/castorini/anserini.git\n",
        "%cd anserini\n",
        "!cd tools/eval && tar xvfz trec_eval.9.0.4.tar.gz && cd trec_eval.9.0.4 && make && cd ../../..\n",
        "!mvn clean package appassembler:assemble -DskipTests -Dmaven.javadoc.skip=true"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRWvO9ftIcdB",
        "outputId": "804d595d-a59e-43e6-9063-10cb948345e9"
      },
      "source": [
        "#import the necessary data (i am not sure this is the latest data)\n",
        "!mkdir collections/fever\n",
        "!mkdir indexes/fever\n",
        "\n",
        "!wget https://s3-eu-west-1.amazonaws.com/fever.public/feverous/feverous-wiki-pages.zip -P collections/fever\n",
        "!unzip collections/fever/feverous-wiki-pages.zip -d collections/fever\n",
        "\n",
        "!wget https://s3-eu-west-1.amazonaws.com/fever.public/feverous/train.jsonl -P collections/fever\n",
        "!wget https://s3-eu-west-1.amazonaws.com/fever.public/feverous/dev.jsonl -P collections/fever"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-13 18:09:22--  https://s3-eu-west-1.amazonaws.com/fever.public/feverous/feverous-wiki-pages.zip\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.104.66\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.104.66|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9906569155 (9.2G) [application/zip]\n",
            "Saving to: ‘collections/fever/feverous-wiki-pages.zip’\n",
            "\n",
            "feverous-wiki-pages 100%[===================>]   9.23G  41.2MB/s    in 2m 38s  \n",
            "\n",
            "2021-07-13 18:12:00 (59.8 MB/s) - ‘collections/fever/feverous-wiki-pages.zip’ saved [9906569155/9906569155]\n",
            "\n",
            "Archive:  collections/fever/feverous-wiki-pages.zip\n",
            "   creating: collections/fever/FeverousWikiv1/\n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_283.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_402.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_177.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_287.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_483.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_210.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_451.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_220.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_110.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_320.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_425.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_040.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_238.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_336.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_033.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_008.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_298.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_189.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_326.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_198.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_159.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_426.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_131.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_137.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_069.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_311.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_230.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_526.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_038.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_349.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_428.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_174.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_239.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_392.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_254.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_361.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_480.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_401.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_063.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_318.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_452.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_379.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_003.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_205.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_099.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_173.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_236.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_116.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_393.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_015.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_302.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_217.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_502.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_180.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_193.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_141.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_176.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_257.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_411.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_303.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_247.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_046.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_048.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_344.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_309.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_044.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_342.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_051.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_140.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_413.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_242.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_028.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_115.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_530.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_509.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_385.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_072.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_163.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_510.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_167.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_519.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_057.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_132.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_061.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_415.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_443.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_035.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_092.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_360.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_481.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_340.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_136.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_098.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_076.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_486.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_190.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_474.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_100.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_016.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_104.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_409.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_259.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_388.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_029.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_292.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_525.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_507.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_146.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_383.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_312.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_228.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_358.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_289.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_215.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_601.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_246.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_306.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_290.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_001.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_606.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_417.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_339.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_325.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_000.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_041.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_485.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_271.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_160.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_206.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_119.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_148.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_107.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_448.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_521.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_114.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_195.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_017.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_418.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_497.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_070.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_145.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_408.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_023.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_356.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_218.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_399.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_406.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_187.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_071.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_030.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_026.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_012.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_291.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_468.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_245.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_352.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_433.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_088.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_405.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_602.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_208.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_477.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_412.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_224.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_087.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_427.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_094.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_517.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_101.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_364.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_499.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_475.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_310.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_372.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_300.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_382.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_064.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_125.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_332.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_049.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_231.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_316.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_429.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_184.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_282.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_232.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_274.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_234.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_523.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_032.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_095.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_391.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_214.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_075.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_374.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_455.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_262.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_068.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_089.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_295.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_453.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_147.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_084.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_149.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_060.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_256.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_605.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_288.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_450.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_122.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_221.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_459.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_251.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_447.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_462.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_348.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_467.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_407.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_482.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_233.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_249.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_512.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_416.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_351.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_389.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_161.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_400.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_050.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_010.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_508.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_152.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_473.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_059.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_255.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_105.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_018.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_394.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_258.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_494.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_192.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_337.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_322.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_449.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_346.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_500.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_097.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_165.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_155.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_034.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_381.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_111.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_007.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_085.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_377.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_074.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_212.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_511.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_345.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_194.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_281.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_533.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_479.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_438.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_191.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_005.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_285.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_273.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_103.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_127.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_367.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_266.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_534.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_083.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_264.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_207.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_219.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_333.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_047.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_444.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_398.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_144.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_157.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_077.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_216.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_020.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_515.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_373.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_013.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_263.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_404.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_492.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_253.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_209.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_472.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_488.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_117.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_461.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_043.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_432.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_168.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_156.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_252.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_162.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_225.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_204.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_275.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_229.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_053.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_284.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_268.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_011.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_158.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_260.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_607.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_495.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_498.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_503.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_042.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_380.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_307.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_505.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_024.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_314.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_265.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_065.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_128.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_324.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_501.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_424.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_313.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_489.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_054.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_330.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_270.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_021.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_308.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_169.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_478.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_327.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_513.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_395.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_341.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_121.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_241.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_172.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_150.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_384.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_269.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_369.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_002.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_004.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_126.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_430.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_223.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_368.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_133.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_299.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_504.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_419.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_056.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_343.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_532.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_378.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_202.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_240.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_244.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_151.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_082.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_297.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_397.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_138.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_237.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_476.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_055.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_603.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_226.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_331.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_387.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_457.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_437.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_524.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_436.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_293.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_182.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_319.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_484.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_143.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_506.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_520.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_006.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_079.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_052.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_609.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_420.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_353.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_102.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_014.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_250.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_009.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_493.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_197.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_036.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_196.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_154.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_350.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_458.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_045.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_203.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_454.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_109.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_186.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_366.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_359.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_516.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_370.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_371.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_178.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_025.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_067.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_175.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_108.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_261.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_465.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_363.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_200.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_466.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_375.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_134.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_403.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_354.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_090.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_445.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_376.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_086.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_321.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_027.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_460.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_338.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_490.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_078.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_357.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_091.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_410.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_201.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_470.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_130.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_022.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_062.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_355.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_166.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_434.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_080.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_301.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_464.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_305.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_435.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_124.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_423.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_170.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_129.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_487.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_422.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_317.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_179.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_439.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_518.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_081.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_066.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_276.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_113.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_600.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_106.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_414.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_294.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_267.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_315.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_142.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_531.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_037.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_235.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_112.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_496.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_248.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_456.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_440.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_222.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_608.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_604.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_334.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_277.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_446.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_039.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_135.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_093.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_153.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_171.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_469.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_390.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_164.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_280.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_335.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_019.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_491.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_421.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_031.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_471.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_527.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_329.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_211.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_610.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_386.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_188.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_442.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_396.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_120.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_463.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_213.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_441.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_347.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_199.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_365.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_073.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_304.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_328.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_362.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_123.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_522.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_272.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_058.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_183.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_286.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_243.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_096.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_431.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_279.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_139.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_514.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_323.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_181.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_278.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_118.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_185.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_296.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_227.jsonl  \n",
            "--2021-07-13 18:22:02--  https://s3-eu-west-1.amazonaws.com/fever.public/feverous/train.jsonl\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.101.75\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.101.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 175493294 (167M) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘collections/fever/train.jsonl’\n",
            "\n",
            "train.jsonl         100%[===================>] 167.36M  87.5MB/s    in 1.9s    \n",
            "\n",
            "2021-07-13 18:22:04 (87.5 MB/s) - ‘collections/fever/train.jsonl’ saved [175493294/175493294]\n",
            "\n",
            "--2021-07-13 18:22:04--  https://s3-eu-west-1.amazonaws.com/fever.public/feverous/dev.jsonl\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.101.75\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.101.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17827949 (17M) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘collections/fever/dev.jsonl’\n",
            "\n",
            "dev.jsonl           100%[===================>]  17.00M  65.1MB/s    in 0.3s    \n",
            "\n",
            "2021-07-13 18:22:04 (65.1 MB/s) - ‘collections/fever/dev.jsonl’ saved [17827949/17827949]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmNJ0RIncWBy",
        "outputId": "14995c3c-4744-486b-aea6-d2863f9541d5"
      },
      "source": [
        "!ls collections/fever/FeverousWikiv1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wiki_000.jsonl\twiki_109.jsonl\twiki_218.jsonl\twiki_327.jsonl\twiki_436.jsonl\n",
            "wiki_001.jsonl\twiki_110.jsonl\twiki_219.jsonl\twiki_328.jsonl\twiki_437.jsonl\n",
            "wiki_002.jsonl\twiki_111.jsonl\twiki_220.jsonl\twiki_329.jsonl\twiki_438.jsonl\n",
            "wiki_003.jsonl\twiki_112.jsonl\twiki_221.jsonl\twiki_330.jsonl\twiki_439.jsonl\n",
            "wiki_004.jsonl\twiki_113.jsonl\twiki_222.jsonl\twiki_331.jsonl\twiki_440.jsonl\n",
            "wiki_005.jsonl\twiki_114.jsonl\twiki_223.jsonl\twiki_332.jsonl\twiki_441.jsonl\n",
            "wiki_006.jsonl\twiki_115.jsonl\twiki_224.jsonl\twiki_333.jsonl\twiki_442.jsonl\n",
            "wiki_007.jsonl\twiki_116.jsonl\twiki_225.jsonl\twiki_334.jsonl\twiki_443.jsonl\n",
            "wiki_008.jsonl\twiki_117.jsonl\twiki_226.jsonl\twiki_335.jsonl\twiki_444.jsonl\n",
            "wiki_009.jsonl\twiki_118.jsonl\twiki_227.jsonl\twiki_336.jsonl\twiki_445.jsonl\n",
            "wiki_010.jsonl\twiki_119.jsonl\twiki_228.jsonl\twiki_337.jsonl\twiki_446.jsonl\n",
            "wiki_011.jsonl\twiki_120.jsonl\twiki_229.jsonl\twiki_338.jsonl\twiki_447.jsonl\n",
            "wiki_012.jsonl\twiki_121.jsonl\twiki_230.jsonl\twiki_339.jsonl\twiki_448.jsonl\n",
            "wiki_013.jsonl\twiki_122.jsonl\twiki_231.jsonl\twiki_340.jsonl\twiki_449.jsonl\n",
            "wiki_014.jsonl\twiki_123.jsonl\twiki_232.jsonl\twiki_341.jsonl\twiki_450.jsonl\n",
            "wiki_015.jsonl\twiki_124.jsonl\twiki_233.jsonl\twiki_342.jsonl\twiki_451.jsonl\n",
            "wiki_016.jsonl\twiki_125.jsonl\twiki_234.jsonl\twiki_343.jsonl\twiki_452.jsonl\n",
            "wiki_017.jsonl\twiki_126.jsonl\twiki_235.jsonl\twiki_344.jsonl\twiki_453.jsonl\n",
            "wiki_018.jsonl\twiki_127.jsonl\twiki_236.jsonl\twiki_345.jsonl\twiki_454.jsonl\n",
            "wiki_019.jsonl\twiki_128.jsonl\twiki_237.jsonl\twiki_346.jsonl\twiki_455.jsonl\n",
            "wiki_020.jsonl\twiki_129.jsonl\twiki_238.jsonl\twiki_347.jsonl\twiki_456.jsonl\n",
            "wiki_021.jsonl\twiki_130.jsonl\twiki_239.jsonl\twiki_348.jsonl\twiki_457.jsonl\n",
            "wiki_022.jsonl\twiki_131.jsonl\twiki_240.jsonl\twiki_349.jsonl\twiki_458.jsonl\n",
            "wiki_023.jsonl\twiki_132.jsonl\twiki_241.jsonl\twiki_350.jsonl\twiki_459.jsonl\n",
            "wiki_024.jsonl\twiki_133.jsonl\twiki_242.jsonl\twiki_351.jsonl\twiki_460.jsonl\n",
            "wiki_025.jsonl\twiki_134.jsonl\twiki_243.jsonl\twiki_352.jsonl\twiki_461.jsonl\n",
            "wiki_026.jsonl\twiki_135.jsonl\twiki_244.jsonl\twiki_353.jsonl\twiki_462.jsonl\n",
            "wiki_027.jsonl\twiki_136.jsonl\twiki_245.jsonl\twiki_354.jsonl\twiki_463.jsonl\n",
            "wiki_028.jsonl\twiki_137.jsonl\twiki_246.jsonl\twiki_355.jsonl\twiki_464.jsonl\n",
            "wiki_029.jsonl\twiki_138.jsonl\twiki_247.jsonl\twiki_356.jsonl\twiki_465.jsonl\n",
            "wiki_030.jsonl\twiki_139.jsonl\twiki_248.jsonl\twiki_357.jsonl\twiki_466.jsonl\n",
            "wiki_031.jsonl\twiki_140.jsonl\twiki_249.jsonl\twiki_358.jsonl\twiki_467.jsonl\n",
            "wiki_032.jsonl\twiki_141.jsonl\twiki_250.jsonl\twiki_359.jsonl\twiki_468.jsonl\n",
            "wiki_033.jsonl\twiki_142.jsonl\twiki_251.jsonl\twiki_360.jsonl\twiki_469.jsonl\n",
            "wiki_034.jsonl\twiki_143.jsonl\twiki_252.jsonl\twiki_361.jsonl\twiki_470.jsonl\n",
            "wiki_035.jsonl\twiki_144.jsonl\twiki_253.jsonl\twiki_362.jsonl\twiki_471.jsonl\n",
            "wiki_036.jsonl\twiki_145.jsonl\twiki_254.jsonl\twiki_363.jsonl\twiki_472.jsonl\n",
            "wiki_037.jsonl\twiki_146.jsonl\twiki_255.jsonl\twiki_364.jsonl\twiki_473.jsonl\n",
            "wiki_038.jsonl\twiki_147.jsonl\twiki_256.jsonl\twiki_365.jsonl\twiki_474.jsonl\n",
            "wiki_039.jsonl\twiki_148.jsonl\twiki_257.jsonl\twiki_366.jsonl\twiki_475.jsonl\n",
            "wiki_040.jsonl\twiki_149.jsonl\twiki_258.jsonl\twiki_367.jsonl\twiki_476.jsonl\n",
            "wiki_041.jsonl\twiki_150.jsonl\twiki_259.jsonl\twiki_368.jsonl\twiki_477.jsonl\n",
            "wiki_042.jsonl\twiki_151.jsonl\twiki_260.jsonl\twiki_369.jsonl\twiki_478.jsonl\n",
            "wiki_043.jsonl\twiki_152.jsonl\twiki_261.jsonl\twiki_370.jsonl\twiki_479.jsonl\n",
            "wiki_044.jsonl\twiki_153.jsonl\twiki_262.jsonl\twiki_371.jsonl\twiki_480.jsonl\n",
            "wiki_045.jsonl\twiki_154.jsonl\twiki_263.jsonl\twiki_372.jsonl\twiki_481.jsonl\n",
            "wiki_046.jsonl\twiki_155.jsonl\twiki_264.jsonl\twiki_373.jsonl\twiki_482.jsonl\n",
            "wiki_047.jsonl\twiki_156.jsonl\twiki_265.jsonl\twiki_374.jsonl\twiki_483.jsonl\n",
            "wiki_048.jsonl\twiki_157.jsonl\twiki_266.jsonl\twiki_375.jsonl\twiki_484.jsonl\n",
            "wiki_049.jsonl\twiki_158.jsonl\twiki_267.jsonl\twiki_376.jsonl\twiki_485.jsonl\n",
            "wiki_050.jsonl\twiki_159.jsonl\twiki_268.jsonl\twiki_377.jsonl\twiki_486.jsonl\n",
            "wiki_051.jsonl\twiki_160.jsonl\twiki_269.jsonl\twiki_378.jsonl\twiki_487.jsonl\n",
            "wiki_052.jsonl\twiki_161.jsonl\twiki_270.jsonl\twiki_379.jsonl\twiki_488.jsonl\n",
            "wiki_053.jsonl\twiki_162.jsonl\twiki_271.jsonl\twiki_380.jsonl\twiki_489.jsonl\n",
            "wiki_054.jsonl\twiki_163.jsonl\twiki_272.jsonl\twiki_381.jsonl\twiki_490.jsonl\n",
            "wiki_055.jsonl\twiki_164.jsonl\twiki_273.jsonl\twiki_382.jsonl\twiki_491.jsonl\n",
            "wiki_056.jsonl\twiki_165.jsonl\twiki_274.jsonl\twiki_383.jsonl\twiki_492.jsonl\n",
            "wiki_057.jsonl\twiki_166.jsonl\twiki_275.jsonl\twiki_384.jsonl\twiki_493.jsonl\n",
            "wiki_058.jsonl\twiki_167.jsonl\twiki_276.jsonl\twiki_385.jsonl\twiki_494.jsonl\n",
            "wiki_059.jsonl\twiki_168.jsonl\twiki_277.jsonl\twiki_386.jsonl\twiki_495.jsonl\n",
            "wiki_060.jsonl\twiki_169.jsonl\twiki_278.jsonl\twiki_387.jsonl\twiki_496.jsonl\n",
            "wiki_061.jsonl\twiki_170.jsonl\twiki_279.jsonl\twiki_388.jsonl\twiki_497.jsonl\n",
            "wiki_062.jsonl\twiki_171.jsonl\twiki_280.jsonl\twiki_389.jsonl\twiki_498.jsonl\n",
            "wiki_063.jsonl\twiki_172.jsonl\twiki_281.jsonl\twiki_390.jsonl\twiki_499.jsonl\n",
            "wiki_064.jsonl\twiki_173.jsonl\twiki_282.jsonl\twiki_391.jsonl\twiki_500.jsonl\n",
            "wiki_065.jsonl\twiki_174.jsonl\twiki_283.jsonl\twiki_392.jsonl\twiki_501.jsonl\n",
            "wiki_066.jsonl\twiki_175.jsonl\twiki_284.jsonl\twiki_393.jsonl\twiki_502.jsonl\n",
            "wiki_067.jsonl\twiki_176.jsonl\twiki_285.jsonl\twiki_394.jsonl\twiki_503.jsonl\n",
            "wiki_068.jsonl\twiki_177.jsonl\twiki_286.jsonl\twiki_395.jsonl\twiki_504.jsonl\n",
            "wiki_069.jsonl\twiki_178.jsonl\twiki_287.jsonl\twiki_396.jsonl\twiki_505.jsonl\n",
            "wiki_070.jsonl\twiki_179.jsonl\twiki_288.jsonl\twiki_397.jsonl\twiki_506.jsonl\n",
            "wiki_071.jsonl\twiki_180.jsonl\twiki_289.jsonl\twiki_398.jsonl\twiki_507.jsonl\n",
            "wiki_072.jsonl\twiki_181.jsonl\twiki_290.jsonl\twiki_399.jsonl\twiki_508.jsonl\n",
            "wiki_073.jsonl\twiki_182.jsonl\twiki_291.jsonl\twiki_400.jsonl\twiki_509.jsonl\n",
            "wiki_074.jsonl\twiki_183.jsonl\twiki_292.jsonl\twiki_401.jsonl\twiki_510.jsonl\n",
            "wiki_075.jsonl\twiki_184.jsonl\twiki_293.jsonl\twiki_402.jsonl\twiki_511.jsonl\n",
            "wiki_076.jsonl\twiki_185.jsonl\twiki_294.jsonl\twiki_403.jsonl\twiki_512.jsonl\n",
            "wiki_077.jsonl\twiki_186.jsonl\twiki_295.jsonl\twiki_404.jsonl\twiki_513.jsonl\n",
            "wiki_078.jsonl\twiki_187.jsonl\twiki_296.jsonl\twiki_405.jsonl\twiki_514.jsonl\n",
            "wiki_079.jsonl\twiki_188.jsonl\twiki_297.jsonl\twiki_406.jsonl\twiki_515.jsonl\n",
            "wiki_080.jsonl\twiki_189.jsonl\twiki_298.jsonl\twiki_407.jsonl\twiki_516.jsonl\n",
            "wiki_081.jsonl\twiki_190.jsonl\twiki_299.jsonl\twiki_408.jsonl\twiki_517.jsonl\n",
            "wiki_082.jsonl\twiki_191.jsonl\twiki_300.jsonl\twiki_409.jsonl\twiki_518.jsonl\n",
            "wiki_083.jsonl\twiki_192.jsonl\twiki_301.jsonl\twiki_410.jsonl\twiki_519.jsonl\n",
            "wiki_084.jsonl\twiki_193.jsonl\twiki_302.jsonl\twiki_411.jsonl\twiki_520.jsonl\n",
            "wiki_085.jsonl\twiki_194.jsonl\twiki_303.jsonl\twiki_412.jsonl\twiki_521.jsonl\n",
            "wiki_086.jsonl\twiki_195.jsonl\twiki_304.jsonl\twiki_413.jsonl\twiki_522.jsonl\n",
            "wiki_087.jsonl\twiki_196.jsonl\twiki_305.jsonl\twiki_414.jsonl\twiki_523.jsonl\n",
            "wiki_088.jsonl\twiki_197.jsonl\twiki_306.jsonl\twiki_415.jsonl\twiki_524.jsonl\n",
            "wiki_089.jsonl\twiki_198.jsonl\twiki_307.jsonl\twiki_416.jsonl\twiki_525.jsonl\n",
            "wiki_090.jsonl\twiki_199.jsonl\twiki_308.jsonl\twiki_417.jsonl\twiki_526.jsonl\n",
            "wiki_091.jsonl\twiki_200.jsonl\twiki_309.jsonl\twiki_418.jsonl\twiki_527.jsonl\n",
            "wiki_092.jsonl\twiki_201.jsonl\twiki_310.jsonl\twiki_419.jsonl\twiki_530.jsonl\n",
            "wiki_093.jsonl\twiki_202.jsonl\twiki_311.jsonl\twiki_420.jsonl\twiki_531.jsonl\n",
            "wiki_094.jsonl\twiki_203.jsonl\twiki_312.jsonl\twiki_421.jsonl\twiki_532.jsonl\n",
            "wiki_095.jsonl\twiki_204.jsonl\twiki_313.jsonl\twiki_422.jsonl\twiki_533.jsonl\n",
            "wiki_096.jsonl\twiki_205.jsonl\twiki_314.jsonl\twiki_423.jsonl\twiki_534.jsonl\n",
            "wiki_097.jsonl\twiki_206.jsonl\twiki_315.jsonl\twiki_424.jsonl\twiki_600.jsonl\n",
            "wiki_098.jsonl\twiki_207.jsonl\twiki_316.jsonl\twiki_425.jsonl\twiki_601.jsonl\n",
            "wiki_099.jsonl\twiki_208.jsonl\twiki_317.jsonl\twiki_426.jsonl\twiki_602.jsonl\n",
            "wiki_100.jsonl\twiki_209.jsonl\twiki_318.jsonl\twiki_427.jsonl\twiki_603.jsonl\n",
            "wiki_101.jsonl\twiki_210.jsonl\twiki_319.jsonl\twiki_428.jsonl\twiki_604.jsonl\n",
            "wiki_102.jsonl\twiki_211.jsonl\twiki_320.jsonl\twiki_429.jsonl\twiki_605.jsonl\n",
            "wiki_103.jsonl\twiki_212.jsonl\twiki_321.jsonl\twiki_430.jsonl\twiki_606.jsonl\n",
            "wiki_104.jsonl\twiki_213.jsonl\twiki_322.jsonl\twiki_431.jsonl\twiki_607.jsonl\n",
            "wiki_105.jsonl\twiki_214.jsonl\twiki_323.jsonl\twiki_432.jsonl\twiki_608.jsonl\n",
            "wiki_106.jsonl\twiki_215.jsonl\twiki_324.jsonl\twiki_433.jsonl\twiki_609.jsonl\n",
            "wiki_107.jsonl\twiki_216.jsonl\twiki_325.jsonl\twiki_434.jsonl\twiki_610.jsonl\n",
            "wiki_108.jsonl\twiki_217.jsonl\twiki_326.jsonl\twiki_435.jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCY3dd7lYnuQ",
        "outputId": "dff11b07-eb86-4ff5-eb80-73328a23f92b"
      },
      "source": [
        "# A folder path, a file path, or a list of file paths is needed\n",
        "\n",
        "# DATA_LOCATION = \"wiki_pages\"\n",
        "# DATA_LOCATION = \"wiki_pages/wiki_000.jsonl\"\n",
        "DATA_LOCATION = [\n",
        "    \"collections/fever/FeverousWikiv1/wiki_000.jsonl\",\n",
        "    \"collections/fever/FeverousWikiv1/wiki_001.jsonl\",\n",
        "    \"collections/fever/FeverousWikiv1/wiki_002.jsonl\",\n",
        "    \"collections/fever/FeverousWikiv1/wiki_003.jsonl\",\n",
        "    \"collections/fever/FeverousWikiv1/wiki_004.jsonl\",\n",
        "]\n",
        "OUTPUT_FOLDER = \"wiki_pages_anserini\"\n",
        "\n",
        "SIMPLIFY_PAGE_LINKS = False\n",
        "PREFER_URL_FOR_LINKS = False\n",
        "\n",
        "# If SIMPLIFY_PAGE_LINKS and not PREFER_URL_FOR_LINKS: [[Page_(disambiguation)|Page Title]] -> Page Title\n",
        "# If SIMPLIFY_PAGE_LINKS and PREFER_URL_FOR_LINKS: [[Page_(disambiguation)|Page Title]] -> Page (disambiguation)\n",
        "# Note that the URL basename can sometimes contain extra information, but it can also redirect to a page that is different than the visible label.\n",
        "\n",
        "SUBSECTION_SEP = \"|\"\n",
        "SUBSUBSECTION_SEP = \" \"\n",
        "import glob\n",
        "import os\n",
        "\n",
        "to_process = None\n",
        "\n",
        "if type(DATA_LOCATION) == list:  # A list of file paths\n",
        "    to_process = [file for file in DATA_LOCATION]\n",
        "elif os.path.isfile(DATA_LOCATION):  # A file path\n",
        "    to_process = [DATA_LOCATION]\n",
        "elif os.path.isdir(DATA_LOCATION):  # A folder\n",
        "    to_process = glob.glob(\"{}/*.jsonl\".format(DATA_LOCATION))\n",
        "else:\n",
        "    raise ValueError(\"Data location is not a valid file or folder.\")\n",
        "\n",
        "print(\"{} file(s) will be processed.\".format(len(to_process)))\n",
        "\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "def clean_value(value):\n",
        "    \"\"\"Replaces unnecessary space (including \\t and such) with a single space\n",
        "    character.\n",
        "\n",
        "    Arg:\n",
        "        value: A string.\n",
        "\n",
        "    Returns:\n",
        "        The simplified version of the provided string.\n",
        "    \"\"\"\n",
        "    return \" \".join(value.split())\n",
        "\n",
        "\n",
        "def final_clean(\n",
        "    field,\n",
        "    simplify_page_links=SIMPLIFY_PAGE_LINKS,\n",
        "    prefer_url_for_links=PREFER_URL_FOR_LINKS,\n",
        "):\n",
        "    \"\"\"Removes multiple space characters with a single space character. Removes\n",
        "    leading and trailing spaces. Simplifies the page links according to the\n",
        "    parameters.\n",
        "\n",
        "    Args:\n",
        "        field (str): A field of the processed page data.\n",
        "        simplify_page_links (Boolean): A Boolean indicating whether the page\n",
        "            links must be simplified. By default, it is set to\n",
        "            SIMPLIFY_PAGE_LINKS.\n",
        "        prefer_url_for_links (Boolean): A Boolean indicating whether the page\n",
        "            links must be simplified to the URL basename instead of the link's\n",
        "            label. By default, it is set to SIMPLIFY_PAGE_LINKS.\n",
        "\n",
        "    Returns:\n",
        "        str: The simplified version of the provided string.\n",
        "    \"\"\"\n",
        "    field = re.sub(\" +\", \" \", field.strip(), flags=re.MULTILINE)\n",
        "    if SIMPLIFY_PAGE_LINKS and PREFER_URL_FOR_LINKS:\n",
        "        # Simplifies links by replacing them with the URL basename (replaces underscores)\n",
        "        # Underscores are replaced with space\n",
        "        field = re.sub(r\"(?:\\[\\[)(.*)?(?:\\|)\", field.replace(\"_\", \" \"), field)\n",
        "        field = re.sub(r\"(\\|).*?(\\]\\])|\\[\\[\", \"\", field, flags=re.MULTILINE)\n",
        "    elif SIMPLIFY_PAGE_LINKS:\n",
        "        # Simplifies links by replacing them with the titles\n",
        "        field = re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", field, flags=re.MULTILINE)\n",
        "    return field\n",
        "\n",
        "\n",
        "if not os.path.exists(OUTPUT_FOLDER):  # Creates the output folder if it does not exist\n",
        "    os.mkdir(OUTPUT_FOLDER)\n",
        "\n",
        "counter = 0\n",
        "for file_path in to_process:\n",
        "    file_basename = os.path.basename(file_path)\n",
        "    output_path = os.path.join(OUTPUT_FOLDER, file_basename)\n",
        "\n",
        "    open(output_path, mode=\"w\").close()  # Creating an empty output file\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        file_data = [json.loads(line) for line in f]\n",
        "\n",
        "    for page in file_data:\n",
        "\n",
        "        page_data_processed = {\"id\": page[\"title\"], \"text\": \"\", \"lines\": \"\"}\n",
        "\n",
        "        # The order list of the page will be used to parse the items\n",
        "        for item_id, item in enumerate(page[\"order\"]):\n",
        "            if item.startswith(\"sentence_\"):  # Sentence\n",
        "                clean_element = clean_value(page[item])\n",
        "                page_data_processed[\"text\"] += \" {}\".format(clean_element)\n",
        "                page_data_processed[\"lines\"] += \"{}\\t{}\\n\".format(item, clean_element)\n",
        "            elif item.startswith(\"table_\"):  # Table\n",
        "                page_data_processed[\"text\"] += \" \"\n",
        "                page_data_processed[\"lines\"] += \"{}\\t\".format(item)\n",
        "                table_rows = []\n",
        "                for row in page[item][\"table\"]:\n",
        "                    row_items = []\n",
        "                    for cell in row:\n",
        "                        row_items.append(clean_value(cell[\"value\"]))\n",
        "\n",
        "                    row_text = \"{}\".format(SUBSUBSECTION_SEP).join(row_items)\n",
        "                    table_rows.append(row_text)\n",
        "\n",
        "                if \"caption\" in page[item]:\n",
        "                    table_rows.append(clean_value(page[item][\"caption\"]))\n",
        "\n",
        "                table_text = \"{}\".format(SUBSECTION_SEP).join(table_rows)\n",
        "                page_data_processed[\"text\"] += table_text\n",
        "                page_data_processed[\"lines\"] += table_text\n",
        "                page_data_processed[\"lines\"] += \"\\n\"\n",
        "            elif item.startswith(\"section_\"):  # Section\n",
        "                clean_element = clean_value(page[item][\"value\"])\n",
        "                page_data_processed[\"text\"] += \" {}\".format(clean_element)\n",
        "                page_data_processed[\"lines\"] += \"{}\\t{}\\n\".format(item, clean_element)\n",
        "            elif item.startswith(\"list_\"):  # List\n",
        "                list_items = []\n",
        "                for list_item in page[item][\"list\"]:\n",
        "                    list_items.append(clean_value(list_item[\"value\"]))\n",
        "\n",
        "                list_text = \"{}\".format(SUBSECTION_SEP).join(list_items)\n",
        "                page_data_processed[\"text\"] += list_text\n",
        "                page_data_processed[\"lines\"] += \"{}\\t{}\\n\".format(item, list_text)\n",
        "            else:  # All alternatives must be handled and the code must not reach here\n",
        "                raise ValueError(\"Unidentified page element found.\")\n",
        "\n",
        "        page_data_processed[\"text\"] = final_clean(page_data_processed[\"text\"])\n",
        "        page_data_processed[\"lines\"] = final_clean(page_data_processed[\"lines\"])\n",
        "\n",
        "        # Appending the processed page data to the file\n",
        "        with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
        "            json.dump(page_data_processed, f, ensure_ascii=False)\n",
        "            f.write(\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 file(s) will be processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkZAGTDasS3m"
      },
      "source": [
        "!sh target/appassembler/bin/IndexCollection -collection FeverParagraphCollection \\\n",
        " -input wiki_pages_anserini \\\n",
        " -index indexes/fever/lucene-index-fever-paragraph \\\n",
        " -generator DefaultLuceneDocumentGenerator \\\n",
        " -threads 1 -storePositions -storeDocvectors -storeRaw \\\n",
        "  >& logs/log.fever &"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3jcK9thJgKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a630fb78-7f7d-41fb-c3f5-9c70d28eaf11"
      },
      "source": [
        "!tar -cvf \"/content/anserini/indexes/fever/lucene-index-fever-paragraph.tar\" \"/content/anserini/indexes/fever/lucene-index-fever-paragraph\" #Tar operation for exporting the index and make it reusable\n",
        "# Only one indexing will be enough we will use the tar file after that use the code below to untar it and equate to the LucuneDir\n",
        "#!tar xvfz lucene-index-fever-paragraph.tar"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "tar: /content/anserini/indexes/fever/lucene-index-fever-paragraph: Cannot stat: No such file or directory\n",
            "tar: Exiting with failure status due to previous errors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzHeiKMIWjLk"
      },
      "source": [
        "luceneDir = 'indexes/fever/lucene-index-fever-paragraph'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8zhAZVwfAmm"
      },
      "source": [
        "USE_SUMMARY = False"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM5KN40Peyit",
        "outputId": "6f0d5ae5-521b-4d8d-bf95-36b5fdbdeee0"
      },
      "source": [
        "import os\n",
        "#%%capture\n",
        "!curl -O https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz\n",
        "!mv openjdk-11.0.2_linux-x64_bin.tar.gz /usr/lib/jvm/; cd /usr/lib/jvm/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n",
        "!update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-11.0.2/bin/java 1\n",
        "!update-alternatives --set java /usr/lib/jvm/jdk-11.0.2/bin/java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/jdk-11.0.2\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  178M  100  178M    0     0   108M      0  0:00:01  0:00:01 --:--:--  108M\n",
            "jdk-11.0.2/bin/jaotc\n",
            "jdk-11.0.2/bin/jar\n",
            "jdk-11.0.2/bin/jarsigner\n",
            "jdk-11.0.2/bin/java\n",
            "jdk-11.0.2/bin/javac\n",
            "jdk-11.0.2/bin/javadoc\n",
            "jdk-11.0.2/bin/javap\n",
            "jdk-11.0.2/bin/jcmd\n",
            "jdk-11.0.2/bin/jconsole\n",
            "jdk-11.0.2/bin/jdb\n",
            "jdk-11.0.2/bin/jdeprscan\n",
            "jdk-11.0.2/bin/jdeps\n",
            "jdk-11.0.2/bin/jhsdb\n",
            "jdk-11.0.2/bin/jimage\n",
            "jdk-11.0.2/bin/jinfo\n",
            "jdk-11.0.2/bin/jjs\n",
            "jdk-11.0.2/bin/jlink\n",
            "jdk-11.0.2/bin/jmap\n",
            "jdk-11.0.2/bin/jmod\n",
            "jdk-11.0.2/bin/jps\n",
            "jdk-11.0.2/bin/jrunscript\n",
            "jdk-11.0.2/bin/jshell\n",
            "jdk-11.0.2/bin/jstack\n",
            "jdk-11.0.2/bin/jstat\n",
            "jdk-11.0.2/bin/jstatd\n",
            "jdk-11.0.2/bin/keytool\n",
            "jdk-11.0.2/bin/pack200\n",
            "jdk-11.0.2/bin/rmic\n",
            "jdk-11.0.2/bin/rmid\n",
            "jdk-11.0.2/bin/rmiregistry\n",
            "jdk-11.0.2/bin/serialver\n",
            "jdk-11.0.2/bin/unpack200\n",
            "jdk-11.0.2/conf/logging.properties\n",
            "jdk-11.0.2/conf/management/jmxremote.access\n",
            "jdk-11.0.2/conf/management/jmxremote.password.template\n",
            "jdk-11.0.2/conf/management/management.properties\n",
            "jdk-11.0.2/conf/net.properties\n",
            "jdk-11.0.2/conf/security/java.policy\n",
            "jdk-11.0.2/conf/security/java.security\n",
            "jdk-11.0.2/conf/security/policy/README.txt\n",
            "jdk-11.0.2/conf/security/policy/limited/default_US_export.policy\n",
            "jdk-11.0.2/conf/security/policy/limited/default_local.policy\n",
            "jdk-11.0.2/conf/security/policy/limited/exempt_local.policy\n",
            "jdk-11.0.2/conf/security/policy/unlimited/default_US_export.policy\n",
            "jdk-11.0.2/conf/security/policy/unlimited/default_local.policy\n",
            "jdk-11.0.2/conf/sound.properties\n",
            "jdk-11.0.2/include/classfile_constants.h\n",
            "jdk-11.0.2/include/jawt.h\n",
            "jdk-11.0.2/include/jdwpTransport.h\n",
            "jdk-11.0.2/include/jni.h\n",
            "jdk-11.0.2/include/jvmti.h\n",
            "jdk-11.0.2/include/jvmticmlr.h\n",
            "jdk-11.0.2/include/linux/jawt_md.h\n",
            "jdk-11.0.2/include/linux/jni_md.h\n",
            "jdk-11.0.2/jmods/java.base.jmod\n",
            "jdk-11.0.2/jmods/java.compiler.jmod\n",
            "jdk-11.0.2/jmods/java.datatransfer.jmod\n",
            "jdk-11.0.2/jmods/java.desktop.jmod\n",
            "jdk-11.0.2/jmods/java.instrument.jmod\n",
            "jdk-11.0.2/jmods/java.logging.jmod\n",
            "jdk-11.0.2/jmods/java.management.jmod\n",
            "jdk-11.0.2/jmods/java.management.rmi.jmod\n",
            "jdk-11.0.2/jmods/java.naming.jmod\n",
            "jdk-11.0.2/jmods/java.net.http.jmod\n",
            "jdk-11.0.2/jmods/java.prefs.jmod\n",
            "jdk-11.0.2/jmods/java.rmi.jmod\n",
            "jdk-11.0.2/jmods/java.scripting.jmod\n",
            "jdk-11.0.2/jmods/java.se.jmod\n",
            "jdk-11.0.2/jmods/java.security.jgss.jmod\n",
            "jdk-11.0.2/jmods/java.security.sasl.jmod\n",
            "jdk-11.0.2/jmods/java.smartcardio.jmod\n",
            "jdk-11.0.2/jmods/java.sql.jmod\n",
            "jdk-11.0.2/jmods/java.sql.rowset.jmod\n",
            "jdk-11.0.2/jmods/java.transaction.xa.jmod\n",
            "jdk-11.0.2/jmods/java.xml.crypto.jmod\n",
            "jdk-11.0.2/jmods/java.xml.jmod\n",
            "jdk-11.0.2/jmods/jdk.accessibility.jmod\n",
            "jdk-11.0.2/jmods/jdk.aot.jmod\n",
            "jdk-11.0.2/jmods/jdk.attach.jmod\n",
            "jdk-11.0.2/jmods/jdk.charsets.jmod\n",
            "jdk-11.0.2/jmods/jdk.compiler.jmod\n",
            "jdk-11.0.2/jmods/jdk.crypto.cryptoki.jmod\n",
            "jdk-11.0.2/jmods/jdk.crypto.ec.jmod\n",
            "jdk-11.0.2/jmods/jdk.dynalink.jmod\n",
            "jdk-11.0.2/jmods/jdk.editpad.jmod\n",
            "jdk-11.0.2/jmods/jdk.hotspot.agent.jmod\n",
            "jdk-11.0.2/jmods/jdk.httpserver.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.ed.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.jvmstat.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.le.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.opt.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.vm.ci.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.vm.compiler.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.vm.compiler.management.jmod\n",
            "jdk-11.0.2/jmods/jdk.jartool.jmod\n",
            "jdk-11.0.2/jmods/jdk.javadoc.jmod\n",
            "jdk-11.0.2/jmods/jdk.jcmd.jmod\n",
            "jdk-11.0.2/jmods/jdk.jconsole.jmod\n",
            "jdk-11.0.2/jmods/jdk.jdeps.jmod\n",
            "jdk-11.0.2/jmods/jdk.jdi.jmod\n",
            "jdk-11.0.2/jmods/jdk.jdwp.agent.jmod\n",
            "jdk-11.0.2/jmods/jdk.jfr.jmod\n",
            "jdk-11.0.2/jmods/jdk.jlink.jmod\n",
            "jdk-11.0.2/jmods/jdk.jshell.jmod\n",
            "jdk-11.0.2/jmods/jdk.jsobject.jmod\n",
            "jdk-11.0.2/jmods/jdk.jstatd.jmod\n",
            "jdk-11.0.2/jmods/jdk.localedata.jmod\n",
            "jdk-11.0.2/jmods/jdk.management.agent.jmod\n",
            "jdk-11.0.2/jmods/jdk.management.jfr.jmod\n",
            "jdk-11.0.2/jmods/jdk.management.jmod\n",
            "jdk-11.0.2/jmods/jdk.naming.dns.jmod\n",
            "jdk-11.0.2/jmods/jdk.naming.rmi.jmod\n",
            "jdk-11.0.2/jmods/jdk.net.jmod\n",
            "jdk-11.0.2/jmods/jdk.pack.jmod\n",
            "jdk-11.0.2/jmods/jdk.rmic.jmod\n",
            "jdk-11.0.2/jmods/jdk.scripting.nashorn.jmod\n",
            "jdk-11.0.2/jmods/jdk.scripting.nashorn.shell.jmod\n",
            "jdk-11.0.2/jmods/jdk.sctp.jmod\n",
            "jdk-11.0.2/jmods/jdk.security.auth.jmod\n",
            "jdk-11.0.2/jmods/jdk.security.jgss.jmod\n",
            "jdk-11.0.2/jmods/jdk.unsupported.desktop.jmod\n",
            "jdk-11.0.2/jmods/jdk.unsupported.jmod\n",
            "jdk-11.0.2/jmods/jdk.xml.dom.jmod\n",
            "jdk-11.0.2/jmods/jdk.zipfs.jmod\n",
            "jdk-11.0.2/legal/java.base/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.base/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.base/LICENSE\n",
            "jdk-11.0.2/legal/java.base/aes.md\n",
            "jdk-11.0.2/legal/java.base/asm.md\n",
            "jdk-11.0.2/legal/java.base/c-libutl.md\n",
            "jdk-11.0.2/legal/java.base/cldr.md\n",
            "jdk-11.0.2/legal/java.base/icu.md\n",
            "jdk-11.0.2/legal/java.base/public_suffix.md\n",
            "jdk-11.0.2/legal/java.base/unicode.md\n",
            "jdk-11.0.2/legal/java.compiler/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.compiler/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.compiler/LICENSE\n",
            "jdk-11.0.2/legal/java.datatransfer/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.datatransfer/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.datatransfer/LICENSE\n",
            "jdk-11.0.2/legal/java.desktop/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.desktop/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.desktop/LICENSE\n",
            "jdk-11.0.2/legal/java.desktop/colorimaging.md\n",
            "jdk-11.0.2/legal/java.desktop/giflib.md\n",
            "jdk-11.0.2/legal/java.desktop/harfbuzz.md\n",
            "jdk-11.0.2/legal/java.desktop/jpeg.md\n",
            "jdk-11.0.2/legal/java.desktop/lcms.md\n",
            "jdk-11.0.2/legal/java.desktop/libpng.md\n",
            "jdk-11.0.2/legal/java.desktop/mesa3d.md\n",
            "jdk-11.0.2/legal/java.desktop/opengl.md\n",
            "jdk-11.0.2/legal/java.desktop/xwindows.md\n",
            "jdk-11.0.2/legal/java.instrument/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.instrument/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.instrument/LICENSE\n",
            "jdk-11.0.2/legal/java.logging/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.logging/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.logging/LICENSE\n",
            "jdk-11.0.2/legal/java.management.rmi/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.management.rmi/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.management.rmi/LICENSE\n",
            "jdk-11.0.2/legal/java.management/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.management/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.management/LICENSE\n",
            "jdk-11.0.2/legal/java.naming/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.naming/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.naming/LICENSE\n",
            "jdk-11.0.2/legal/java.net.http/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.net.http/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.net.http/LICENSE\n",
            "jdk-11.0.2/legal/java.prefs/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.prefs/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.prefs/LICENSE\n",
            "jdk-11.0.2/legal/java.rmi/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.rmi/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.rmi/LICENSE\n",
            "jdk-11.0.2/legal/java.scripting/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.scripting/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.scripting/LICENSE\n",
            "jdk-11.0.2/legal/java.se/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.se/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.se/LICENSE\n",
            "jdk-11.0.2/legal/java.security.jgss/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.security.jgss/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.security.jgss/LICENSE\n",
            "jdk-11.0.2/legal/java.security.sasl/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.security.sasl/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.security.sasl/LICENSE\n",
            "jdk-11.0.2/legal/java.smartcardio/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.smartcardio/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.smartcardio/LICENSE\n",
            "jdk-11.0.2/legal/java.smartcardio/pcsclite.md\n",
            "jdk-11.0.2/legal/java.sql.rowset/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.sql.rowset/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.sql.rowset/LICENSE\n",
            "jdk-11.0.2/legal/java.sql/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.sql/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.sql/LICENSE\n",
            "jdk-11.0.2/legal/java.transaction.xa/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.transaction.xa/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.transaction.xa/LICENSE\n",
            "jdk-11.0.2/legal/java.xml.crypto/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.xml.crypto/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.xml.crypto/LICENSE\n",
            "jdk-11.0.2/legal/java.xml.crypto/santuario.md\n",
            "jdk-11.0.2/legal/java.xml/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.xml/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.xml/LICENSE\n",
            "jdk-11.0.2/legal/java.xml/bcel.md\n",
            "jdk-11.0.2/legal/java.xml/dom.md\n",
            "jdk-11.0.2/legal/java.xml/jcup.md\n",
            "jdk-11.0.2/legal/java.xml/xalan.md\n",
            "jdk-11.0.2/legal/java.xml/xerces.md\n",
            "jdk-11.0.2/legal/jdk.accessibility/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.accessibility/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.accessibility/LICENSE\n",
            "jdk-11.0.2/legal/jdk.aot/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.aot/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.aot/LICENSE\n",
            "jdk-11.0.2/legal/jdk.attach/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.attach/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.attach/LICENSE\n",
            "jdk-11.0.2/legal/jdk.charsets/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.charsets/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.charsets/LICENSE\n",
            "jdk-11.0.2/legal/jdk.compiler/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.compiler/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.compiler/LICENSE\n",
            "jdk-11.0.2/legal/jdk.crypto.cryptoki/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.crypto.cryptoki/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.crypto.cryptoki/LICENSE\n",
            "jdk-11.0.2/legal/jdk.crypto.cryptoki/pkcs11cryptotoken.md\n",
            "jdk-11.0.2/legal/jdk.crypto.cryptoki/pkcs11wrapper.md\n",
            "jdk-11.0.2/legal/jdk.crypto.ec/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.crypto.ec/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.crypto.ec/LICENSE\n",
            "jdk-11.0.2/legal/jdk.crypto.ec/ecc.md\n",
            "jdk-11.0.2/legal/jdk.dynalink/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.dynalink/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.dynalink/LICENSE\n",
            "jdk-11.0.2/legal/jdk.dynalink/dynalink.md\n",
            "jdk-11.0.2/legal/jdk.editpad/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.editpad/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.editpad/LICENSE\n",
            "jdk-11.0.2/legal/jdk.hotspot.agent/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.hotspot.agent/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.hotspot.agent/LICENSE\n",
            "jdk-11.0.2/legal/jdk.httpserver/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.httpserver/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.httpserver/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.ed/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.ed/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.ed/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.jvmstat/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.jvmstat/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.jvmstat/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.le/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.le/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.le/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.le/jline.md\n",
            "jdk-11.0.2/legal/jdk.internal.opt/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.opt/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.opt/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.opt/jopt-simple.md\n",
            "jdk-11.0.2/legal/jdk.internal.vm.ci/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.vm.ci/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.vm.ci/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.vm.compiler.management/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.vm.compiler.management/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.vm.compiler.management/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.vm.compiler/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.vm.compiler/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.vm.compiler/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jartool/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jartool/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jartool/LICENSE\n",
            "jdk-11.0.2/legal/jdk.javadoc/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.javadoc/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.javadoc/LICENSE\n",
            "jdk-11.0.2/legal/jdk.javadoc/jquery-migrate.md\n",
            "jdk-11.0.2/legal/jdk.javadoc/jquery.md\n",
            "jdk-11.0.2/legal/jdk.javadoc/jqueryUI.md\n",
            "jdk-11.0.2/legal/jdk.javadoc/jszip.md\n",
            "jdk-11.0.2/legal/jdk.javadoc/pako.md\n",
            "jdk-11.0.2/legal/jdk.jcmd/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jcmd/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jcmd/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jconsole/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jconsole/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jconsole/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jdeps/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jdeps/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jdeps/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jdi/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jdi/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jdi/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jdwp.agent/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jdwp.agent/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jdwp.agent/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jfr/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jfr/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jfr/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jlink/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jlink/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jlink/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jshell/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jshell/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jshell/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jsobject/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jsobject/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jsobject/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jstatd/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jstatd/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jstatd/LICENSE\n",
            "jdk-11.0.2/legal/jdk.localedata/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.localedata/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.localedata/LICENSE\n",
            "jdk-11.0.2/legal/jdk.localedata/cldr.md\n",
            "jdk-11.0.2/legal/jdk.localedata/thaidict.md\n",
            "jdk-11.0.2/legal/jdk.management.agent/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.management.agent/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.management.agent/LICENSE\n",
            "jdk-11.0.2/legal/jdk.management.jfr/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.management.jfr/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.management.jfr/LICENSE\n",
            "jdk-11.0.2/legal/jdk.management/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.management/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.management/LICENSE\n",
            "jdk-11.0.2/legal/jdk.naming.dns/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.naming.dns/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.naming.dns/LICENSE\n",
            "jdk-11.0.2/legal/jdk.naming.rmi/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.naming.rmi/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.naming.rmi/LICENSE\n",
            "jdk-11.0.2/legal/jdk.net/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.net/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.net/LICENSE\n",
            "jdk-11.0.2/legal/jdk.pack/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.pack/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.pack/LICENSE\n",
            "jdk-11.0.2/legal/jdk.rmic/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.rmic/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.rmic/LICENSE\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn.shell/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn.shell/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn.shell/LICENSE\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn/LICENSE\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn/double-conversion.md\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn/joni.md\n",
            "jdk-11.0.2/legal/jdk.sctp/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.sctp/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.sctp/LICENSE\n",
            "jdk-11.0.2/legal/jdk.security.auth/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.security.auth/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.security.auth/LICENSE\n",
            "jdk-11.0.2/legal/jdk.security.jgss/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.security.jgss/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.security.jgss/LICENSE\n",
            "jdk-11.0.2/legal/jdk.unsupported.desktop/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.unsupported.desktop/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.unsupported.desktop/LICENSE\n",
            "jdk-11.0.2/legal/jdk.unsupported/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.unsupported/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.unsupported/LICENSE\n",
            "jdk-11.0.2/legal/jdk.xml.dom/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.xml.dom/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.xml.dom/LICENSE\n",
            "jdk-11.0.2/legal/jdk.zipfs/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.zipfs/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.zipfs/LICENSE\n",
            "jdk-11.0.2/lib/classlist\n",
            "jdk-11.0.2/lib/ct.sym\n",
            "jdk-11.0.2/lib/jexec\n",
            "jdk-11.0.2/lib/jfr/default.jfc\n",
            "jdk-11.0.2/lib/jfr/profile.jfc\n",
            "jdk-11.0.2/lib/jli/libjli.so\n",
            "jdk-11.0.2/lib/jrt-fs.jar\n",
            "jdk-11.0.2/lib/jvm.cfg\n",
            "jdk-11.0.2/lib/libattach.so\n",
            "jdk-11.0.2/lib/libawt.so\n",
            "jdk-11.0.2/lib/libawt_headless.so\n",
            "jdk-11.0.2/lib/libawt_xawt.so\n",
            "jdk-11.0.2/lib/libdt_socket.so\n",
            "jdk-11.0.2/lib/libextnet.so\n",
            "jdk-11.0.2/lib/libfontmanager.so\n",
            "jdk-11.0.2/lib/libinstrument.so\n",
            "jdk-11.0.2/lib/libj2gss.so\n",
            "jdk-11.0.2/lib/libj2pcsc.so\n",
            "jdk-11.0.2/lib/libj2pkcs11.so\n",
            "jdk-11.0.2/lib/libjaas.so\n",
            "jdk-11.0.2/lib/libjava.so\n",
            "jdk-11.0.2/lib/libjavajpeg.so\n",
            "jdk-11.0.2/lib/libjawt.so\n",
            "jdk-11.0.2/lib/libjdwp.so\n",
            "jdk-11.0.2/lib/libjimage.so\n",
            "jdk-11.0.2/lib/libjsig.so\n",
            "jdk-11.0.2/lib/libjsound.so\n",
            "jdk-11.0.2/lib/liblcms.so\n",
            "jdk-11.0.2/lib/libmanagement.so\n",
            "jdk-11.0.2/lib/libmanagement_agent.so\n",
            "jdk-11.0.2/lib/libmanagement_ext.so\n",
            "jdk-11.0.2/lib/libmlib_image.so\n",
            "jdk-11.0.2/lib/libnet.so\n",
            "jdk-11.0.2/lib/libnio.so\n",
            "jdk-11.0.2/lib/libprefs.so\n",
            "jdk-11.0.2/lib/librmi.so\n",
            "jdk-11.0.2/lib/libsaproc.so\n",
            "jdk-11.0.2/lib/libsctp.so\n",
            "jdk-11.0.2/lib/libsplashscreen.so\n",
            "jdk-11.0.2/lib/libsunec.so\n",
            "jdk-11.0.2/lib/libunpack.so\n",
            "jdk-11.0.2/lib/libverify.so\n",
            "jdk-11.0.2/lib/libzip.so\n",
            "jdk-11.0.2/lib/modules\n",
            "jdk-11.0.2/lib/psfont.properties.ja\n",
            "jdk-11.0.2/lib/psfontj2d.properties\n",
            "jdk-11.0.2/lib/security/blacklisted.certs\n",
            "jdk-11.0.2/lib/security/cacerts\n",
            "jdk-11.0.2/lib/security/default.policy\n",
            "jdk-11.0.2/lib/security/public_suffix_list.dat\n",
            "jdk-11.0.2/lib/server/Xusage.txt\n",
            "jdk-11.0.2/lib/server/libjsig.so\n",
            "jdk-11.0.2/lib/server/libjvm.so\n",
            "jdk-11.0.2/lib/src.zip\n",
            "jdk-11.0.2/lib/tzdb.dat\n",
            "jdk-11.0.2/release\n",
            "update-alternatives: using /usr/lib/jvm/jdk-11.0.2/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-U3oN2Ge3Tk",
        "outputId": "20a09df5-85fa-4f87-81e4-396f4173f813"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "!mkdir ~/kaggle\n",
        "!mkdir ~/kaggle/working/\n",
        "!mkdir ~/kaggle/working/sentence_wise_email/\n",
        "!mkdir ~/kaggle/working/sentence_wise_email/module/\n",
        "!mkdir ~/kaggle/working/sentence_wise_email/module/module_useT\n",
        "# Download the module, and uncompress it to the destination folder. \n",
        "!curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed\" | tar -zxvC ~/kaggle/working/sentence_wise_email/module/module_useT"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0  745M    0 47943    0     0  71343      0  3:02:41 --:--:--  3:02:41 71343./\n",
            "./tfhub_module.pb\n",
            "./variables/\n",
            "./variables/variables.data-00000-of-00001\n",
            " 99  745M   99  741M    0     0  39.7M      0  0:00:18  0:00:18 --:--:-- 44.2M./variables/variables.index\n",
            "./assets/\n",
            "./saved_model.pb\n",
            "100  745M  100  745M    0     0  39.5M      0  0:00:18  0:00:18 --:--:-- 45.0M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dbae76bfed2e480fa1aa509fd8e5a1d7",
            "c7779e97542b48df8179bd7418b4c7b9",
            "045576b7b0234868a9b24e42e0ffffd7",
            "bfefd72888bb4a5085912a5c8d73e227",
            "1b555339a47f4ebbac0e0d50f58ad278",
            "57c4c91d97f94adf8820ee979fe1f6dd",
            "c7ee38880f6a49a48e4bb7796d497ec3",
            "9021e6e59422426d8077b5ab0564e9fa",
            "3287e8081b1849f6b70c87f69f921b2b",
            "9309308fc6114033b3ebeefb7f7e4418",
            "24b7a67fdda642c6afbb350b870635d8",
            "2400583964f04a57ae49b2d9e9ad2c0c",
            "b904c9d7e2974e7794a2955d022963b2",
            "d7c6cabc8deb482cb19e0679517951a0",
            "0a1e62fb44f24095a5cde0ecd0a17c45",
            "5afd5b91a9c84a99ab4a37d929d78846",
            "cf21b95828c44d8c8186711679b6613d",
            "29cd0b999e7e4799b3b0a25fb5f71df8",
            "3be1edc1cd9747bd96c007c2a8af8e7e",
            "fe29744383b845e88ce9e46db448227c",
            "1af2f1b478aa4822880cc082c93d4070",
            "36978303dc5a4d94a5505125e5fb8ee2",
            "9959ea5acc65429b9784dd9cef98c999",
            "82f90420c26d4066ac400ffd3f2913be"
          ]
        },
        "id": "RSVu4N7Pe6OK",
        "outputId": "97750f43-a3a6-4e20-aeca-5c4830ffc241"
      },
      "source": [
        "import torch\n",
        "!pip install transformers\n",
        "!pip install --no-cache-dir transformers sentencepiece\n",
        "#from transformers import BertForQuestionAnswering\n",
        "#from transformers import BertTokenizer\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "torch_device = 'cuda'\n",
        "\n",
        "#dmis-lab/biobert-base-cased-v1.1-squad\n",
        "#abhijithneilabraham/longformer_covid_qa\n",
        "#graviraja/covidbert_squad \n",
        "\n",
        "QA_TOKENIZER = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1-squad\")\n",
        "QA_MODEL = AutoModelForQuestionAnswering.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1-squad\")\n",
        "\n",
        "#QA_MODEL = AutoModelForQuestionAnswering.from_pretrained(\"NeuML/bert-small-cord19qa\")\n",
        "#QA_TOKENIZER = AutoTokenizer.from_pretrained(\"NeuML/bert-small-cord19qa\")\n",
        "\n",
        "#QA_MODEL = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "#QA_TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "QA_MODEL.to(torch_device)\n",
        "QA_MODEL.eval()\n",
        "#Auto Summarization with BART\n",
        "if USE_SUMMARY:\n",
        "    SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "    SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "    SUMMARY_MODEL.to(torch_device)\n",
        "    SUMMARY_MODEL.eval()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 17.4MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 20.8MB/s eta 0:00:01\r\u001b[K     |▍                               | 30kB 18.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 40kB 15.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 51kB 11.9MB/s eta 0:00:01\r\u001b[K     |▉                               | 61kB 12.4MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 12.0MB/s eta 0:00:01\r\u001b[K     |█                               | 81kB 12.0MB/s eta 0:00:01\r\u001b[K     |█▏                              | 92kB 11.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 102kB 11.8MB/s eta 0:00:01\r\u001b[K     |█▍                              | 112kB 11.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 122kB 11.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 133kB 11.8MB/s eta 0:00:01\r\u001b[K     |█▉                              | 143kB 11.8MB/s eta 0:00:01\r\u001b[K     |██                              | 153kB 11.8MB/s eta 0:00:01\r\u001b[K     |██                              | 163kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 174kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 184kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 194kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 204kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▊                             | 215kB 11.8MB/s eta 0:00:01\r\u001b[K     |██▉                             | 225kB 11.8MB/s eta 0:00:01\r\u001b[K     |███                             | 235kB 11.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 245kB 11.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 256kB 11.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 266kB 11.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 276kB 11.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 286kB 11.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 296kB 11.8MB/s eta 0:00:01\r\u001b[K     |████                            | 307kB 11.8MB/s eta 0:00:01\r\u001b[K     |████                            | 317kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 327kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 337kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▌                           | 348kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 358kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 368kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 378kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 389kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 399kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 409kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 419kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 430kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 440kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 450kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 460kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 471kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 481kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 491kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 501kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 512kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 522kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 532kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 542kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 552kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 563kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 573kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 583kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 593kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 604kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 614kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 624kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 634kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 645kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 655kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 665kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 675kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 686kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 696kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 706kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 716kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 727kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 737kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 747kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 757kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 768kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 778kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 788kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 798kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 808kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 819kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 829kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 839kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 849kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 860kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 870kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 880kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 890kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 901kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 911kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 921kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 931kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 942kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 952kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 962kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 972kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 983kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 993kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.6MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.7MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.8MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.9MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 2.0MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 2.1MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.2MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.3MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 2.4MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5MB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.5MB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbae76bfed2e480fa1aa509fd8e5a1d7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=477.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3287e8081b1849f6b70c87f69f921b2b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf21b95828c44d8c8186711679b6613d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433297484.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BAh4RL0RecR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273e7fe9-3cda-4d83-947d-3aa143b44044"
      },
      "source": [
        "#%%capture\n",
        "!pip install pyserini==0.8.1.0\n",
        "from pyserini.search import pysearch"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyserini==0.8.1.0 in /usr/local/lib/python3.7/dist-packages (0.8.1.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from pyserini==0.8.1.0) (0.29.23)\n",
            "Requirement already satisfied: pyjnius in /usr/local/lib/python3.7/dist-packages (from pyserini==0.8.1.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pyjnius->pyserini==0.8.1.0) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjZfCGUbXYDI"
      },
      "source": [
        "query_processed = 'As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?'\n",
        "query = 'As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?'\n",
        "\n",
        "keywords = '2019-nCoV, SARS-CoV-2, COVID-19, symptoms, hospitalization'"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TElPXtbMXKaI"
      },
      "source": [
        "import json\n",
        "\n",
        "searcher = pysearch.SimpleSearcher(luceneDir)\n",
        "hits = searcher.search(query_processed + '. ' + keywords,k=50) # We can retrieve the documents according to concatanated string of query and keywords we can change this setup.\n",
        "n_hits = len(hits)\n",
        "## collect the relevant data in a hit dictionary\n",
        "hit_dictionary = {}"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0lPvHURepbj"
      },
      "source": [
        "import json\n",
        "#'As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?'\n",
        "\n",
        "searcher = pysearch.SimpleSearcher(luceneDir)\n",
        "hits = searcher.search(query_processed + '. ' + keywords,k=50)\n",
        "n_hits = len(hits)\n",
        "## collect the relevant data in a hit dictionary\n",
        "hit_dictionary = {}\n",
        "for i in range(0, n_hits):\n",
        "  if 1==1:\n",
        "    doc_json=dict()\n",
        "    idx = str(hits[i].docid)\n",
        "    hit_dictionary[idx] = doc_json\n",
        "    hit_dictionary[idx]['abstract']=hits[i].lucene_document.get(\"raw\")\n",
        "    hit_dictionary[idx]['real_abstract']=hits[i].lucene_document.get(\"raw\")\n",
        "    hit_dictionary[idx]['title'] = str(hits[i].docid)\n",
        "\n",
        "## scrub the abstracts in prep for BERT-SQuAD\n",
        "for idx,v in hit_dictionary.items():\n",
        "    abs_dirty = v['abstract']\n",
        "    real_abs_dirty = v['real_abstract']\n",
        "    #abs_dirty = v['paragraph']\n",
        "    # looks like the abstract value can be an empty list\n",
        "    v['abstract_paragraphs'] = []\n",
        "    v['abstract_full'] = ''\n",
        "    v['real_abstract_full'] = ''\n",
        "    v['real_abstract_paragraphs']=[]\n",
        "\n",
        "    if abs_dirty:\n",
        "        # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
        "        # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
        "        # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
        "\n",
        "\n",
        "        if isinstance(abs_dirty, list):\n",
        "            for p in abs_dirty:\n",
        "                v['abstract_paragraphs'].append(p['text'])\n",
        "                v['abstract_full'] += p['text'] + ' \\n\\n'\n",
        "\n",
        "        # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
        "        if isinstance(abs_dirty, str):\n",
        "            v['abstract_paragraphs'].append(abs_dirty)\n",
        "            v['abstract_full'] += abs_dirty + ' \\n\\n'\n",
        "    if real_abs_dirty:\n",
        "        # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
        "        # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
        "        # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
        "\n",
        "\n",
        "        if isinstance(real_abs_dirty, list):\n",
        "            for p in real_abs_dirty:\n",
        "                v['real_abstract_paragraphs'].append(p['text'])\n",
        "                v['real_abstract_full'] += p['text'] + ' \\n\\n'\n",
        "\n",
        "        # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
        "        if isinstance(abs_dirty, str):\n",
        "            v['real_abstract_paragraphs'].append(real_abs_dirty)\n",
        "            v['real_abstract_full'] += real_abs_dirty + ' \\n\\n'\n",
        "    v[\"indexed_para\"]=v[\"abstract_full\"][len(v[\"real_abstract_full\"]):]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TZVSOX4XiFw",
        "outputId": "3bf8e422-1c08-4c7b-f230-2d49f3eee56f"
      },
      "source": [
        "def embed_useT(module):\n",
        "    with tf.Graph().as_default():\n",
        "        sentences = tf.compat.v1.placeholder(tf.string)\n",
        "        embed = hub.Module(module)\n",
        "        embeddings = embed(sentences)\n",
        "        session = tf.compat.v1.train.MonitoredSession()\n",
        "    return lambda x: session.run(embeddings, {sentences: x})\n",
        "embed_fn = embed_useT('/root/kaggle/working/sentence_wise_email/module/module_useT')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ22nEB1rYdw"
      },
      "source": [
        "import numpy as np\n",
        "#Reconstruct text for generating the answer text from the BERT result, do the necessary text preprocessing and construct the answer.\n",
        "def reconstructText(tokens, start=0, stop=-1):\n",
        "    tokens = tokens[start: stop]\n",
        "    if '[SEP]' in tokens:\n",
        "        sepind = tokens.index('[SEP]')\n",
        "        tokens = tokens[sepind+1:]\n",
        "    txt = ' '.join(tokens)\n",
        "    txt = txt.replace(' ##', '')\n",
        "    txt = txt.replace('##', '')\n",
        "    txt = txt.strip()\n",
        "    txt = \" \".join(txt.split())\n",
        "    txt = txt.replace(' .', '.')\n",
        "    txt = txt.replace('( ', '(')\n",
        "    txt = txt.replace(' )', ')')\n",
        "    txt = txt.replace(' - ', '-')\n",
        "    txt_list = txt.split(' , ')\n",
        "    txt = ''\n",
        "    nTxtL = len(txt_list)\n",
        "    if nTxtL == 1:\n",
        "        return txt_list[0]\n",
        "    newList =[]\n",
        "    for i,t in enumerate(txt_list):\n",
        "        if i < nTxtL -1:\n",
        "            if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n",
        "                newList += [t,',']\n",
        "            else:\n",
        "                newList += [t, ', ']\n",
        "        else:\n",
        "            newList += [t]\n",
        "    return ''.join(newList)\n",
        "\n",
        "\n",
        "def makeBERTSQuADPrediction(document, question):\n",
        "    ## we need to rewrite this function so that it chuncks the document into 250-300 word segments with\n",
        "    ## 50 word overlaps on either end so that it can understand and check longer abstracts\n",
        "    ## Get chuck of documents in order to overcome the BERT's max 512 token problem. First count the number of tokens. For example if it is\n",
        "    ##   larger than 2048 token determine the split size and overlapping token count. Split the document into pieces to fed into BERT encoding. Overlapping\n",
        "    ##      is necessary to maintain the coherence between the splits. If the token count is larger than some number 2560 token approximately this will fail.\n",
        "    nWords = len(document.split())\n",
        "    input_ids_all = QA_TOKENIZER.encode(question, document)\n",
        "    tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n",
        "    overlapFac = 1.1\n",
        "    if len(input_ids_all)*overlapFac > 2560:\n",
        "        nSearchWords = int(np.ceil(nWords/6))\n",
        "        fifth = int(np.ceil(nWords/5))\n",
        "        docSplit = document.split()\n",
        "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
        "                     ' '.join(docSplit[fifth-int(nSearchWords*overlapFac/2):fifth+int(fifth*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[fifth*2-int(nSearchWords*overlapFac/2):fifth*2+int(fifth*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[fifth*3-int(nSearchWords*overlapFac/2):fifth*3+int(fifth*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[fifth*4-int(nSearchWords*overlapFac/2):fifth*4+int(fifth*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
        "\n",
        "    elif len(input_ids_all)*overlapFac > 2048:\n",
        "        nSearchWords = int(np.ceil(nWords/5))\n",
        "        quarter = int(np.ceil(nWords/4))\n",
        "        docSplit = document.split()\n",
        "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
        "                     ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
        "        \n",
        "    elif len(input_ids_all)*overlapFac > 1536:\n",
        "        nSearchWords = int(np.ceil(nWords/4))\n",
        "        third = int(np.ceil(nWords/3))\n",
        "        docSplit = document.split()\n",
        "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
        "                     ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
        "        \n",
        "    elif len(input_ids_all)*overlapFac > 1024:\n",
        "        nSearchWords = int(np.ceil(nWords/3))\n",
        "        middle = int(np.ceil(nWords/2))\n",
        "        docSplit = document.split()\n",
        "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
        "                     ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
        "    elif len(input_ids_all)*overlapFac > 512:\n",
        "        nSearchWords = int(np.ceil(nWords/2))\n",
        "        docSplit = document.split()\n",
        "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
        "    else:\n",
        "        input_ids = [input_ids_all]\n",
        "    absTooLong = False    \n",
        "    \n",
        "    answers = []\n",
        "    cons = []\n",
        "    #print(input_ids)\n",
        "    for iptIds in input_ids:\n",
        "        tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n",
        "        #print(tokens)\n",
        "        sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)\n",
        "        num_seg_a = sep_index + 1\n",
        "        num_seg_b = len(iptIds) - num_seg_a\n",
        "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "        assert len(segment_ids) == len(iptIds)\n",
        "        n_ids = len(segment_ids)\n",
        "        #print(n_ids)\n",
        "        if n_ids < 512:\n",
        "            outputs = QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
        "                                     token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n",
        "            #print(QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
        "             #                        token_type_ids=torch.tensor([segment_ids]).to(torch_device)))\n",
        "        else:\n",
        "            #this cuts off the text if its more than 512 words so it fits in model space\n",
        "            #need run multiple inferences for longer text. add to the todo. I will try to expand this to longer sequences if it is necessary. \n",
        "            print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n",
        "            absTooLong = True\n",
        "            outputs= QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device), \n",
        "                                     token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))\n",
        "        start_scores=outputs.start_logits\n",
        "        end_scores=outputs.end_logits\n",
        "        start_scores = start_scores[:,1:-1]\n",
        "        end_scores = end_scores[:,1:-1]\n",
        "        answer_start = torch.argmax(start_scores)\n",
        "        answer_end = torch.argmax(end_scores)\n",
        "        #print(answer_start, answer_end)\n",
        "        answer = reconstructText(tokens, answer_start, answer_end+2)\n",
        "    \n",
        "        if answer.startswith('. ') or answer.startswith(', '):\n",
        "            answer = answer[2:]\n",
        "            \n",
        "        c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n",
        "        answers.append(answer)\n",
        "        cons.append(c)\n",
        "    \n",
        "    maxC = max(cons)\n",
        "    iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n",
        "    confidence = cons[iMaxC]\n",
        "    answer = answers[iMaxC]\n",
        "    \n",
        "    sep_index = tokens_all.index('[SEP]')\n",
        "    full_txt_tokens = tokens_all[sep_index+1:]\n",
        "    \n",
        "    abs_returned = reconstructText(full_txt_tokens)\n",
        "\n",
        "    ans={}\n",
        "    ans['answer'] = answer\n",
        "    #print(answer)\n",
        "    if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n",
        "        ans['confidence'] = -1000000\n",
        "    else:\n",
        "        #confidence = torch.max(start_scores) + torch.max(end_scores)\n",
        "        #confidence = np.log(confidence.item())\n",
        "        ans['confidence'] = confidence\n",
        "    #ans['start'] = answer_start.item()\n",
        "    #ans['end'] = answer_end.item()\n",
        "    ans['abstract_bert'] = abs_returned\n",
        "    ans['abs_too_long'] = absTooLong\n",
        "    return ans"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKNQBMURrbv9"
      },
      "source": [
        "from tqdm import tqdm\n",
        "def searchAbstracts(hit_dictionary, question):\n",
        "    abstractResults = {}\n",
        "    for k,v in tqdm(hit_dictionary.items()):\n",
        "        abstract = v['abstract_full']\n",
        "        indexed_para=v['indexed_para']\n",
        "        if abstract:\n",
        "            ans = makeBERTSQuADPrediction(abstract, question)\n",
        "            if ans['answer']:\n",
        "                confidence = ans['confidence']\n",
        "                abstractResults[confidence]={}\n",
        "                abstractResults[confidence]['answer'] = ans['answer']\n",
        "                #abstractResults[confidence]['start'] = ans['start']\n",
        "                #abstractResults[confidence]['end'] = ans['end']\n",
        "                abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n",
        "                abstractResults[confidence]['idx'] = k\n",
        "                abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n",
        "\n",
        "                \n",
        "    cList = list(abstractResults.keys())\n",
        "\n",
        "    if cList:\n",
        "        maxScore = max(cList)\n",
        "        total = 0.0\n",
        "        exp_scores = []\n",
        "        for c in cList:\n",
        "            s = np.exp(c-maxScore)\n",
        "            exp_scores.append(s)\n",
        "        total = sum(exp_scores)\n",
        "        for i,c in enumerate(cList):\n",
        "            abstractResults[exp_scores[i]/total] = abstractResults.pop(c)\n",
        "    return abstractResults"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZsPI-2PreQC",
        "outputId": "6065c0b0-80a3-47ea-ea0c-74bf68c83786"
      },
      "source": [
        "answers = searchAbstracts(hit_dictionary, query)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 3699 words long.  There are 1526 tokens\n",
            "****** warning only considering first 512 tokens, document is 3699 words long.  There are 1706 tokens\n",
            "****** warning only considering first 512 tokens, document is 3699 words long.  There are 1361 tokens\n",
            "****** warning only considering first 512 tokens, document is 3699 words long.  There are 1641 tokens\n",
            "****** warning only considering first 512 tokens, document is 3699 words long.  There are 1750 tokens\n",
            "****** warning only considering first 512 tokens, document is 3699 words long.  There are 1904 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 2/50 [00:00<00:13,  3.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2309 words long.  There are 685 tokens\n",
            "****** warning only considering first 512 tokens, document is 2309 words long.  There are 743 tokens\n",
            "****** warning only considering first 512 tokens, document is 2309 words long.  There are 783 tokens\n",
            "****** warning only considering first 512 tokens, document is 2309 words long.  There are 691 tokens\n",
            "****** warning only considering first 512 tokens, document is 2309 words long.  There are 858 tokens\n",
            "****** warning only considering first 512 tokens, document is 2309 words long.  There are 856 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 3/50 [00:00<00:12,  3.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2317 words long.  There are 884 tokens\n",
            "****** warning only considering first 512 tokens, document is 2317 words long.  There are 1263 tokens\n",
            "****** warning only considering first 512 tokens, document is 2317 words long.  There are 905 tokens\n",
            "****** warning only considering first 512 tokens, document is 2317 words long.  There are 1077 tokens\n",
            "****** warning only considering first 512 tokens, document is 2317 words long.  There are 822 tokens\n",
            "****** warning only considering first 512 tokens, document is 2317 words long.  There are 818 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 4/50 [00:00<00:11,  3.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2282 words long.  There are 1073 tokens\n",
            "****** warning only considering first 512 tokens, document is 2282 words long.  There are 945 tokens\n",
            "****** warning only considering first 512 tokens, document is 2282 words long.  There are 904 tokens\n",
            "****** warning only considering first 512 tokens, document is 2282 words long.  There are 795 tokens\n",
            "****** warning only considering first 512 tokens, document is 2282 words long.  There are 712 tokens\n",
            "****** warning only considering first 512 tokens, document is 2282 words long.  There are 678 tokens\n",
            "****** warning only considering first 512 tokens, document is 7277 words long.  There are 2637 tokens\n",
            "****** warning only considering first 512 tokens, document is 7277 words long.  There are 2754 tokens\n",
            "****** warning only considering first 512 tokens, document is 7277 words long.  There are 2920 tokens\n",
            "****** warning only considering first 512 tokens, document is 7277 words long.  There are 4321 tokens\n",
            "****** warning only considering first 512 tokens, document is 7277 words long.  There are 3482 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 5/50 [00:01<00:14,  3.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 7277 words long.  There are 3506 tokens\n",
            "****** warning only considering first 512 tokens, document is 4016 words long.  There are 1140 tokens\n",
            "****** warning only considering first 512 tokens, document is 4016 words long.  There are 1063 tokens\n",
            "****** warning only considering first 512 tokens, document is 4016 words long.  There are 1143 tokens\n",
            "****** warning only considering first 512 tokens, document is 4016 words long.  There are 1259 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 6/50 [00:01<00:13,  3.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4016 words long.  There are 1253 tokens\n",
            "****** warning only considering first 512 tokens, document is 4016 words long.  There are 1433 tokens\n",
            "****** warning only considering first 512 tokens, document is 2229 words long.  There are 790 tokens\n",
            "****** warning only considering first 512 tokens, document is 2229 words long.  There are 809 tokens\n",
            "****** warning only considering first 512 tokens, document is 2229 words long.  There are 847 tokens\n",
            "****** warning only considering first 512 tokens, document is 2229 words long.  There are 750 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 7/50 [00:01<00:11,  3.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2229 words long.  There are 805 tokens\n",
            "****** warning only considering first 512 tokens, document is 2229 words long.  There are 898 tokens\n",
            "****** warning only considering first 512 tokens, document is 1688 words long.  There are 648 tokens\n",
            "****** warning only considering first 512 tokens, document is 1688 words long.  There are 684 tokens\n",
            "****** warning only considering first 512 tokens, document is 1688 words long.  There are 556 tokens\n",
            "****** warning only considering first 512 tokens, document is 1688 words long.  There are 600 tokens\n",
            "****** warning only considering first 512 tokens, document is 1688 words long.  There are 572 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 8/50 [00:02<00:10,  3.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1688 words long.  There are 559 tokens\n",
            "****** warning only considering first 512 tokens, document is 7092 words long.  There are 2540 tokens\n",
            "****** warning only considering first 512 tokens, document is 7092 words long.  There are 2569 tokens\n",
            "****** warning only considering first 512 tokens, document is 7092 words long.  There are 3152 tokens\n",
            "****** warning only considering first 512 tokens, document is 7092 words long.  There are 4053 tokens\n",
            "****** warning only considering first 512 tokens, document is 7092 words long.  There are 3828 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 9/50 [00:02<00:13,  3.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 7092 words long.  There are 2590 tokens\n",
            "****** warning only considering first 512 tokens, document is 2119 words long.  There are 915 tokens\n",
            "****** warning only considering first 512 tokens, document is 2119 words long.  There are 769 tokens\n",
            "****** warning only considering first 512 tokens, document is 2119 words long.  There are 867 tokens\n",
            "****** warning only considering first 512 tokens, document is 2119 words long.  There are 861 tokens\n",
            "****** warning only considering first 512 tokens, document is 2119 words long.  There are 785 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 10/50 [00:02<00:11,  3.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2119 words long.  There are 689 tokens\n",
            "****** warning only considering first 512 tokens, document is 3254 words long.  There are 2080 tokens\n",
            "****** warning only considering first 512 tokens, document is 3254 words long.  There are 1921 tokens\n",
            "****** warning only considering first 512 tokens, document is 3254 words long.  There are 1393 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 11/50 [00:03<00:11,  3.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 3254 words long.  There are 1370 tokens\n",
            "****** warning only considering first 512 tokens, document is 3254 words long.  There are 1440 tokens\n",
            "****** warning only considering first 512 tokens, document is 3254 words long.  There are 1425 tokens\n",
            "****** warning only considering first 512 tokens, document is 1718 words long.  There are 555 tokens\n",
            "****** warning only considering first 512 tokens, document is 1718 words long.  There are 681 tokens\n",
            "****** warning only considering first 512 tokens, document is 1718 words long.  There are 554 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 24%|██▍       | 12/50 [00:03<00:09,  3.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1718 words long.  There are 639 tokens\n",
            "****** warning only considering first 512 tokens, document is 1718 words long.  There are 574 tokens\n",
            "****** warning only considering first 512 tokens, document is 1718 words long.  There are 515 tokens\n",
            "****** warning only considering first 512 tokens, document is 2044 words long.  There are 894 tokens\n",
            "****** warning only considering first 512 tokens, document is 2044 words long.  There are 932 tokens\n",
            "****** warning only considering first 512 tokens, document is 2044 words long.  There are 832 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 26%|██▌       | 13/50 [00:03<00:09,  4.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2044 words long.  There are 697 tokens\n",
            "****** warning only considering first 512 tokens, document is 2044 words long.  There are 733 tokens\n",
            "****** warning only considering first 512 tokens, document is 2044 words long.  There are 823 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1694 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 28%|██▊       | 14/50 [00:03<00:09,  3.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1554 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1572 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1561 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1982 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 2028 tokens\n",
            "****** warning only considering first 512 tokens, document is 7632 words long.  There are 2715 tokens\n",
            "****** warning only considering first 512 tokens, document is 7632 words long.  There are 2771 tokens\n",
            "****** warning only considering first 512 tokens, document is 7632 words long.  There are 2781 tokens\n",
            "****** warning only considering first 512 tokens, document is 7632 words long.  There are 2825 tokens\n",
            "****** warning only considering first 512 tokens, document is 7632 words long.  There are 2818 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 15/50 [00:04<00:11,  3.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 7632 words long.  There are 2772 tokens\n",
            "****** warning only considering first 512 tokens, document is 271 words long.  There are 585 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 34%|███▍      | 17/50 [00:04<00:09,  3.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1813 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 2020 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1644 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1672 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1943 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1610 tokens\n",
            "****** warning only considering first 512 tokens, document is 6892 words long.  There are 2160 tokens\n",
            "****** warning only considering first 512 tokens, document is 6892 words long.  There are 2487 tokens\n",
            "****** warning only considering first 512 tokens, document is 6892 words long.  There are 2526 tokens\n",
            "****** warning only considering first 512 tokens, document is 6892 words long.  There are 1984 tokens\n",
            "****** warning only considering first 512 tokens, document is 6892 words long.  There are 2018 tokens\n",
            "****** warning only considering first 512 tokens, document is 6892 words long.  There are 1950 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 19/50 [00:05<00:08,  3.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2233 words long.  There are 1164 tokens\n",
            "****** warning only considering first 512 tokens, document is 2233 words long.  There are 1138 tokens\n",
            "****** warning only considering first 512 tokens, document is 2233 words long.  There are 1197 tokens\n",
            "****** warning only considering first 512 tokens, document is 2233 words long.  There are 1082 tokens\n",
            "****** warning only considering first 512 tokens, document is 2233 words long.  There are 959 tokens\n",
            "****** warning only considering first 512 tokens, document is 2233 words long.  There are 906 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 20/50 [00:05<00:08,  3.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1746 words long.  There are 1043 tokens\n",
            "****** warning only considering first 512 tokens, document is 1746 words long.  There are 900 tokens\n",
            "****** warning only considering first 512 tokens, document is 1746 words long.  There are 760 tokens\n",
            "****** warning only considering first 512 tokens, document is 1746 words long.  There are 791 tokens\n",
            "****** warning only considering first 512 tokens, document is 1746 words long.  There are 645 tokens\n",
            "****** warning only considering first 512 tokens, document is 1746 words long.  There are 746 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 42%|████▏     | 21/50 [00:05<00:07,  3.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 3817 words long.  There are 1402 tokens\n",
            "****** warning only considering first 512 tokens, document is 3817 words long.  There are 1471 tokens\n",
            "****** warning only considering first 512 tokens, document is 3817 words long.  There are 1300 tokens\n",
            "****** warning only considering first 512 tokens, document is 3817 words long.  There are 1532 tokens\n",
            "****** warning only considering first 512 tokens, document is 3817 words long.  There are 1207 tokens\n",
            "****** warning only considering first 512 tokens, document is 3817 words long.  There are 1223 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 22/50 [00:06<00:07,  3.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4484 words long.  There are 1621 tokens\n",
            "****** warning only considering first 512 tokens, document is 4484 words long.  There are 1286 tokens\n",
            "****** warning only considering first 512 tokens, document is 4484 words long.  There are 1447 tokens\n",
            "****** warning only considering first 512 tokens, document is 4484 words long.  There are 1356 tokens\n",
            "****** warning only considering first 512 tokens, document is 4484 words long.  There are 1463 tokens\n",
            "****** warning only considering first 512 tokens, document is 4484 words long.  There are 1868 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 46%|████▌     | 23/50 [00:06<00:07,  3.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1900 words long.  There are 1288 tokens\n",
            "****** warning only considering first 512 tokens, document is 1900 words long.  There are 849 tokens\n",
            "****** warning only considering first 512 tokens, document is 1900 words long.  There are 1061 tokens\n",
            "****** warning only considering first 512 tokens, document is 1900 words long.  There are 725 tokens\n",
            "****** warning only considering first 512 tokens, document is 1900 words long.  There are 1038 tokens\n",
            "****** warning only considering first 512 tokens, document is 1900 words long.  There are 884 tokens\n",
            "****** warning only considering first 512 tokens, document is 6072 words long.  There are 1881 tokens\n",
            "****** warning only considering first 512 tokens, document is 6072 words long.  There are 1881 tokens\n",
            "****** warning only considering first 512 tokens, document is 6072 words long.  There are 1837 tokens\n",
            "****** warning only considering first 512 tokens, document is 6072 words long.  There are 1997 tokens\n",
            "****** warning only considering first 512 tokens, document is 6072 words long.  There are 2044 tokens\n",
            "****** warning only considering first 512 tokens, document is 6072 words long.  There are 1874 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 50%|█████     | 25/50 [00:06<00:06,  3.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1596 words long.  There are 837 tokens\n",
            "****** warning only considering first 512 tokens, document is 1596 words long.  There are 698 tokens\n",
            "****** warning only considering first 512 tokens, document is 1596 words long.  There are 679 tokens\n",
            "****** warning only considering first 512 tokens, document is 1596 words long.  There are 660 tokens\n",
            "****** warning only considering first 512 tokens, document is 1596 words long.  There are 587 tokens\n",
            "****** warning only considering first 512 tokens, document is 8559 words long.  There are 3007 tokens\n",
            "****** warning only considering first 512 tokens, document is 8559 words long.  There are 3785 tokens\n",
            "****** warning only considering first 512 tokens, document is 8559 words long.  There are 3673 tokens\n",
            "****** warning only considering first 512 tokens, document is 8559 words long.  There are 3385 tokens\n",
            "****** warning only considering first 512 tokens, document is 8559 words long.  There are 3429 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 52%|█████▏    | 26/50 [00:07<00:08,  2.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 8559 words long.  There are 2992 tokens\n",
            "****** warning only considering first 512 tokens, document is 7443 words long.  There are 2979 tokens\n",
            "****** warning only considering first 512 tokens, document is 7443 words long.  There are 3149 tokens\n",
            "****** warning only considering first 512 tokens, document is 7443 words long.  There are 2470 tokens\n",
            "****** warning only considering first 512 tokens, document is 7443 words long.  There are 2749 tokens\n",
            "****** warning only considering first 512 tokens, document is 7443 words long.  There are 2816 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 54%|█████▍    | 27/50 [00:07<00:08,  2.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 7443 words long.  There are 2303 tokens\n",
            "****** warning only considering first 512 tokens, document is 2341 words long.  There are 683 tokens\n",
            "****** warning only considering first 512 tokens, document is 2341 words long.  There are 700 tokens\n",
            "****** warning only considering first 512 tokens, document is 2341 words long.  There are 766 tokens\n",
            "****** warning only considering first 512 tokens, document is 2341 words long.  There are 782 tokens\n",
            "****** warning only considering first 512 tokens, document is 2341 words long.  There are 904 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 28/50 [00:08<00:07,  3.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2341 words long.  There are 679 tokens\n",
            "****** warning only considering first 512 tokens, document is 4715 words long.  There are 2100 tokens\n",
            "****** warning only considering first 512 tokens, document is 4715 words long.  There are 1723 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 58%|█████▊    | 29/50 [00:08<00:06,  3.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4715 words long.  There are 1667 tokens\n",
            "****** warning only considering first 512 tokens, document is 4715 words long.  There are 1649 tokens\n",
            "****** warning only considering first 512 tokens, document is 4715 words long.  There are 1837 tokens\n",
            "****** warning only considering first 512 tokens, document is 4715 words long.  There are 1444 tokens\n",
            "****** warning only considering first 512 tokens, document is 6808 words long.  There are 3422 tokens\n",
            "****** warning only considering first 512 tokens, document is 6808 words long.  There are 2952 tokens\n",
            "****** warning only considering first 512 tokens, document is 6808 words long.  There are 2981 tokens\n",
            "****** warning only considering first 512 tokens, document is 6808 words long.  There are 2969 tokens\n",
            "****** warning only considering first 512 tokens, document is 6808 words long.  There are 3147 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 30/50 [00:08<00:07,  2.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 6808 words long.  There are 2876 tokens\n",
            "****** warning only considering first 512 tokens, document is 16712 words long.  There are 5322 tokens\n",
            "****** warning only considering first 512 tokens, document is 16712 words long.  There are 5306 tokens\n",
            "****** warning only considering first 512 tokens, document is 16712 words long.  There are 5607 tokens\n",
            "****** warning only considering first 512 tokens, document is 16712 words long.  There are 5895 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 62%|██████▏   | 31/50 [00:09<00:09,  2.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 16712 words long.  There are 5958 tokens\n",
            "****** warning only considering first 512 tokens, document is 16712 words long.  There are 5352 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 32/50 [00:09<00:07,  2.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 3975 words long.  There are 1980 tokens\n",
            "****** warning only considering first 512 tokens, document is 3975 words long.  There are 1702 tokens\n",
            "****** warning only considering first 512 tokens, document is 3975 words long.  There are 1528 tokens\n",
            "****** warning only considering first 512 tokens, document is 3975 words long.  There are 1296 tokens\n",
            "****** warning only considering first 512 tokens, document is 3975 words long.  There are 1488 tokens\n",
            "****** warning only considering first 512 tokens, document is 3975 words long.  There are 1639 tokens\n",
            "****** warning only considering first 512 tokens, document is 8075 words long.  There are 3062 tokens\n",
            "****** warning only considering first 512 tokens, document is 8075 words long.  There are 2963 tokens\n",
            "****** warning only considering first 512 tokens, document is 8075 words long.  There are 2945 tokens\n",
            "****** warning only considering first 512 tokens, document is 8075 words long.  There are 2661 tokens\n",
            "****** warning only considering first 512 tokens, document is 8075 words long.  There are 2794 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 66%|██████▌   | 33/50 [00:10<00:07,  2.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 8075 words long.  There are 2963 tokens\n",
            "****** warning only considering first 512 tokens, document is 8597 words long.  There are 2918 tokens\n",
            "****** warning only considering first 512 tokens, document is 8597 words long.  There are 3140 tokens\n",
            "****** warning only considering first 512 tokens, document is 8597 words long.  There are 3045 tokens\n",
            "****** warning only considering first 512 tokens, document is 8597 words long.  There are 3163 tokens\n",
            "****** warning only considering first 512 tokens, document is 8597 words long.  There are 2827 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 34/50 [00:10<00:07,  2.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 8597 words long.  There are 2790 tokens\n",
            "****** warning only considering first 512 tokens, document is 6804 words long.  There are 3530 tokens\n",
            "****** warning only considering first 512 tokens, document is 6804 words long.  There are 3086 tokens\n",
            "****** warning only considering first 512 tokens, document is 6804 words long.  There are 2883 tokens\n",
            "****** warning only considering first 512 tokens, document is 6804 words long.  There are 2447 tokens\n",
            "****** warning only considering first 512 tokens, document is 6804 words long.  There are 2751 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 36/50 [00:11<00:05,  2.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 6804 words long.  There are 2702 tokens\n",
            "****** warning only considering first 512 tokens, document is 1844 words long.  There are 1059 tokens\n",
            "****** warning only considering first 512 tokens, document is 1844 words long.  There are 845 tokens\n",
            "****** warning only considering first 512 tokens, document is 1844 words long.  There are 623 tokens\n",
            "****** warning only considering first 512 tokens, document is 1844 words long.  There are 668 tokens\n",
            "****** warning only considering first 512 tokens, document is 1844 words long.  There are 632 tokens\n",
            "****** warning only considering first 512 tokens, document is 1844 words long.  There are 644 tokens\n",
            "****** warning only considering first 512 tokens, document is 7465 words long.  There are 2934 tokens\n",
            "****** warning only considering first 512 tokens, document is 7465 words long.  There are 2978 tokens\n",
            "****** warning only considering first 512 tokens, document is 7465 words long.  There are 3758 tokens\n",
            "****** warning only considering first 512 tokens, document is 7465 words long.  There are 3275 tokens\n",
            "****** warning only considering first 512 tokens, document is 7465 words long.  There are 3355 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 74%|███████▍  | 37/50 [00:11<00:05,  2.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 7465 words long.  There are 3792 tokens\n",
            "****** warning only considering first 512 tokens, document is 7884 words long.  There are 3512 tokens\n",
            "****** warning only considering first 512 tokens, document is 7884 words long.  There are 4040 tokens\n",
            "****** warning only considering first 512 tokens, document is 7884 words long.  There are 3444 tokens\n",
            "****** warning only considering first 512 tokens, document is 7884 words long.  There are 3236 tokens\n",
            "****** warning only considering first 512 tokens, document is 7884 words long.  There are 2893 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 76%|███████▌  | 38/50 [00:12<00:05,  2.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 7884 words long.  There are 3084 tokens\n",
            "****** warning only considering first 512 tokens, document is 9431 words long.  There are 3711 tokens\n",
            "****** warning only considering first 512 tokens, document is 9431 words long.  There are 3757 tokens\n",
            "****** warning only considering first 512 tokens, document is 9431 words long.  There are 3824 tokens\n",
            "****** warning only considering first 512 tokens, document is 9431 words long.  There are 3349 tokens\n",
            "****** warning only considering first 512 tokens, document is 9431 words long.  There are 4245 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 78%|███████▊  | 39/50 [00:13<00:05,  2.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 9431 words long.  There are 3716 tokens\n",
            "****** warning only considering first 512 tokens, document is 4539 words long.  There are 1851 tokens\n",
            "****** warning only considering first 512 tokens, document is 4539 words long.  There are 1926 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 40/50 [00:13<00:04,  2.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4539 words long.  There are 2031 tokens\n",
            "****** warning only considering first 512 tokens, document is 4539 words long.  There are 2676 tokens\n",
            "****** warning only considering first 512 tokens, document is 4539 words long.  There are 2824 tokens\n",
            "****** warning only considering first 512 tokens, document is 4539 words long.  There are 1797 tokens\n",
            "****** warning only considering first 512 tokens, document is 13673 words long.  There are 7708 tokens\n",
            "****** warning only considering first 512 tokens, document is 13673 words long.  There are 7043 tokens\n",
            "****** warning only considering first 512 tokens, document is 13673 words long.  There are 6717 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 41/50 [00:14<00:05,  1.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 13673 words long.  There are 8590 tokens\n",
            "****** warning only considering first 512 tokens, document is 13673 words long.  There are 5197 tokens\n",
            "****** warning only considering first 512 tokens, document is 13673 words long.  There are 5579 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 84%|████████▍ | 42/50 [00:14<00:03,  2.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 3536 words long.  There are 1344 tokens\n",
            "****** warning only considering first 512 tokens, document is 3536 words long.  There are 1346 tokens\n",
            "****** warning only considering first 512 tokens, document is 3536 words long.  There are 1357 tokens\n",
            "****** warning only considering first 512 tokens, document is 3536 words long.  There are 1367 tokens\n",
            "****** warning only considering first 512 tokens, document is 3536 words long.  There are 1304 tokens\n",
            "****** warning only considering first 512 tokens, document is 3536 words long.  There are 1208 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 86%|████████▌ | 43/50 [00:14<00:02,  2.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1809 words long.  There are 797 tokens\n",
            "****** warning only considering first 512 tokens, document is 1809 words long.  There are 808 tokens\n",
            "****** warning only considering first 512 tokens, document is 1809 words long.  There are 625 tokens\n",
            "****** warning only considering first 512 tokens, document is 1809 words long.  There are 824 tokens\n",
            "****** warning only considering first 512 tokens, document is 1809 words long.  There are 907 tokens\n",
            "****** warning only considering first 512 tokens, document is 1809 words long.  There are 760 tokens\n",
            "****** warning only considering first 512 tokens, document is 1711 words long.  There are 534 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 44/50 [00:14<00:01,  3.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1711 words long.  There are 521 tokens\n",
            "****** warning only considering first 512 tokens, document is 1711 words long.  There are 548 tokens\n",
            "****** warning only considering first 512 tokens, document is 1711 words long.  There are 538 tokens\n",
            "****** warning only considering first 512 tokens, document is 1711 words long.  There are 556 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 45/50 [00:15<00:01,  3.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4291 words long.  There are 1247 tokens\n",
            "****** warning only considering first 512 tokens, document is 4291 words long.  There are 1288 tokens\n",
            "****** warning only considering first 512 tokens, document is 4291 words long.  There are 1312 tokens\n",
            "****** warning only considering first 512 tokens, document is 4291 words long.  There are 1359 tokens\n",
            "****** warning only considering first 512 tokens, document is 4291 words long.  There are 1419 tokens\n",
            "****** warning only considering first 512 tokens, document is 4291 words long.  There are 1423 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 92%|█████████▏| 46/50 [00:15<00:01,  3.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2046 words long.  There are 938 tokens\n",
            "****** warning only considering first 512 tokens, document is 2046 words long.  There are 1234 tokens\n",
            "****** warning only considering first 512 tokens, document is 2046 words long.  There are 1052 tokens\n",
            "****** warning only considering first 512 tokens, document is 2046 words long.  There are 1051 tokens\n",
            "****** warning only considering first 512 tokens, document is 2046 words long.  There are 1044 tokens\n",
            "****** warning only considering first 512 tokens, document is 2046 words long.  There are 959 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 94%|█████████▍| 47/50 [00:15<00:00,  3.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2122 words long.  There are 956 tokens\n",
            "****** warning only considering first 512 tokens, document is 2122 words long.  There are 944 tokens\n",
            "****** warning only considering first 512 tokens, document is 2122 words long.  There are 891 tokens\n",
            "****** warning only considering first 512 tokens, document is 2122 words long.  There are 1072 tokens\n",
            "****** warning only considering first 512 tokens, document is 2122 words long.  There are 762 tokens\n",
            "****** warning only considering first 512 tokens, document is 2122 words long.  There are 645 tokens\n",
            "****** warning only considering first 512 tokens, document is 1209 words long.  There are 516 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 96%|█████████▌| 48/50 [00:15<00:00,  4.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1209 words long.  There are 644 tokens\n",
            "****** warning only considering first 512 tokens, document is 1209 words long.  There are 569 tokens\n",
            "****** warning only considering first 512 tokens, document is 2474 words long.  There are 941 tokens\n",
            "****** warning only considering first 512 tokens, document is 2474 words long.  There are 1169 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 98%|█████████▊| 49/50 [00:16<00:00,  4.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2474 words long.  There are 1183 tokens\n",
            "****** warning only considering first 512 tokens, document is 2474 words long.  There are 901 tokens\n",
            "****** warning only considering first 512 tokens, document is 2474 words long.  There are 888 tokens\n",
            "****** warning only considering first 512 tokens, document is 2474 words long.  There are 1235 tokens\n",
            "****** warning only considering first 512 tokens, document is 6000 words long.  There are 2560 tokens\n",
            "****** warning only considering first 512 tokens, document is 6000 words long.  There are 2210 tokens\n",
            "****** warning only considering first 512 tokens, document is 6000 words long.  There are 2449 tokens\n",
            "****** warning only considering first 512 tokens, document is 6000 words long.  There are 2243 tokens\n",
            "****** warning only considering first 512 tokens, document is 6000 words long.  There are 2718 tokens\n",
            "****** warning only considering first 512 tokens, document is 6000 words long.  There are 2359 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:16<00:00,  3.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m86xqzSOO8A"
      },
      "source": [
        "FIND_PDFS=False \n",
        "SEARCH_MEDRXIV=False"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV9gJtmqY9ic"
      },
      "source": [
        "workingPath = '/root/kaggle/working'\n",
        "import pandas as pd\n",
        "import re\n",
        "if FIND_PDFS:\n",
        "    from metapub import UrlReverse\n",
        "    from metapub import FindIt\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "#from summarizer import Summarizer\n",
        "#summarizerModel = Summarizer()\n",
        "def displayResults(hit_dictionary, answers, question):\n",
        "    \n",
        "    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n",
        "    #all_HTML_txt = question_HTML\n",
        "    confidence = list(answers.keys())\n",
        "    confidence.sort(reverse=True)\n",
        "    \n",
        "    confidence = list(answers.keys())\n",
        "    confidence.sort(reverse=True)\n",
        "    \n",
        "\n",
        "    for c in confidence:\n",
        "        if c>0 and c <= 1 and len(answers[c]['answer']) != 0:\n",
        "            if 'idx' not in  answers[c]:\n",
        "                continue\n",
        "            rowData = []\n",
        "            idx = answers[c]['idx']\n",
        "            title = hit_dictionary[idx]['title']\n",
        "\n",
        "            \n",
        "            full_abs = answers[c]['abstract_bert']\n",
        "            bert_ans = answers[c]['answer']\n",
        "            \n",
        "            \n",
        "            #split_abs = full_abs.split(bert_ans)\n",
        "            #x=(split_abs[0][:split_abs[0].rfind('. ')+1])\n",
        "            #sentance_beginning = x[x.rfind('. ')+1:] + split_abs[0][split_abs[0].rfind('. ')+1:]\n",
        "            #sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n",
        "            x=''\n",
        "            y=''\n",
        "            z=''\n",
        "            t=''\n",
        "            regex = r\"(\\.\\s[^0-9])(?!.*(\\.\\s[^0-9]))\"\n",
        "            split_abs = full_abs.split(bert_ans)\n",
        "            matches = re.finditer(regex, split_abs[0], re.MULTILINE)\n",
        "            for matchNum1, match in enumerate(matches, start=1):\n",
        "              y=(split_abs[0][:match.start()+1])\n",
        "            matches = re.finditer(regex, y, re.MULTILINE)\n",
        "            for matchNum2, match in enumerate(matches, start=1):\n",
        "              x=(y[match.start()+1:])\n",
        "            if x=='':\n",
        "              x=split_abs[0]\n",
        "            matches = re.finditer(regex, split_abs[0], re.MULTILINE)\n",
        "            for matchNum3, match in enumerate(matches, start=1):\n",
        "              z=(split_abs[0][match.start()+1:])  \n",
        "            sentance_beginning = x+z\n",
        "            regex2=r\"(.*?(?<!\\b\\w)[.?!])\\s+[a-zA-Z0-9]\"\n",
        "            if len(split_abs) == 1:\n",
        "                sentance_end_pos = len(full_abs)\n",
        "                sentance_end =''\n",
        "            else:\n",
        "                matches = re.finditer(regex2, split_abs[1], re.MULTILINE)\n",
        "                for matchNum4, match in enumerate(matches, start=1):\n",
        "                    if matchNum4==1:\n",
        "                        t=(split_abs[1][:match.end()-2])\n",
        "                sentance_end_pos = split_abs[1].find('. ')+1\n",
        "                if sentance_end_pos == 0:\n",
        "                    sentance_end = split_abs[1]\n",
        "                else:\n",
        "                    sentance_end = t\n",
        "            #if len(split_abs) == 1:\n",
        "            #    sentance_end_pos = len(full_abs)\n",
        "            #    sentance_end =''\n",
        "            #else:\n",
        "            #    sentance_end_pos = split_abs[1].find('. ')+1\n",
        "            #    if sentance_end_pos == 0:\n",
        "            #      sentance_end = split_abs[1]\n",
        "            #    else:\n",
        "            #      sentance_end = split_abs[1][:sentance_end_pos]\n",
        "                \n",
        "            #sentance_full = sentance_beginning + bert_ans+ sentance_end\n",
        "            answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n",
        "            answers[c]['partial_answer'] = bert_ans+sentance_end\n",
        "            answers[c]['sentence_beginning'] = sentance_beginning\n",
        "            answers[c]['sentence_end'] = sentance_end\n",
        "            answers[c]['title'] = title\n",
        "        else:\n",
        "            answers.pop(c)\n",
        "    \n",
        "    \n",
        "    ## now rerank based on semantic similarity of the answers to the question\n",
        "    ## Universal sentence encoder\n",
        "    cList = list(answers.keys())\n",
        "    allAnswers = [answers[c]['full_answer'] for c in cList]\n",
        "    \n",
        "    messages = [question]+allAnswers\n",
        "    \n",
        "    encoding_matrix = embed_fn(messages)\n",
        "    similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n",
        "    rankings = similarity_matrix[1:,0]\n",
        "    \n",
        "    for i,c in enumerate(cList):\n",
        "        answers[rankings[i]] = answers.pop(c)\n",
        "\n",
        "    ## now form pandas dv\n",
        "    confidence = list(answers.keys())\n",
        "    confidence.sort(reverse=True)\n",
        "    pandasData = []\n",
        "    ranked_aswers = []\n",
        "    full_abs_list=[]\n",
        "    for c in confidence:\n",
        "        rowData=[]\n",
        "        title = answers[c]['title']\n",
        "        idx = answers[c]['idx']\n",
        "        rowData += [idx]            \n",
        "        sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='red'>\"+answers[c]['answer']+\"</font> \"+answers[c]['sentence_end']+'</div>'\n",
        "        \n",
        "        rowData += [sentance_html, c,title]\n",
        "        pandasData.append(rowData)\n",
        "        ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n",
        "        full_abs_list.append(' '.join([answers[c]['abstract_bert']]))\n",
        "    \n",
        "    if FIND_PDFS or SEARCH_MEDRXIV:\n",
        "        pdata2 = []\n",
        "        pm_ids = []\n",
        "        for rowData in pandasData:\n",
        "            rd = rowData\n",
        "            idx = rowData[0]\n",
        "    else:\n",
        "        pdata2 = pandasData\n",
        "        \n",
        "    \n",
        "    display(HTML(question_HTML))\n",
        "\n",
        "    if USE_SUMMARY:\n",
        "        ## try generating an exacutive summary with bart abstractive summarizer\n",
        "        allAnswersTxt = ' '.join(ranked_aswers[:10]).replace('\\n','')\n",
        "    #    exec_sum = summarizerModel(allAnswersTxt, min_length=1, max_length=500)    \n",
        "     #   execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>BERT Extractive Summary:</b>: '+exec_sum+'</div>'\n",
        "\n",
        "        answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=2048)['input_ids'].to(torch_device)\n",
        "        #answers_input_ids = SUMMARY_TOKENIZER.prepare_seq2seq_batch([allAnswersTxt], return_tensors='pt', truncation=True, padding='longest')['input_ids'].to(torch_device)\n",
        "        summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n",
        "                                               num_beams=10,\n",
        "                                               length_penalty=1.1,\n",
        "                                               max_length=2048,\n",
        "                                               min_length=64,\n",
        "                                               no_repeat_ngram_size=0,\n",
        "                                                do_sample=False )\n",
        "\n",
        "        exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
        "        execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>BART Abstractive Summary:</b>: '+exec_sum+'</div>'\n",
        "        display(HTML(execSum_HTML))\n",
        "        warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 12px; padding-bottom:12px; color:#CCCC00; margin-top:1pt\"> Warning this is an autogenerated summary based on semantic search of abstracts, always examine the sources before accepting this conclusion.  If the evidence only mentions topic in passing or the evidence is not clear, the summary will likely not clearly answer the question.</div>'\n",
        "        display(HTML(warning_HTML))\n",
        "\n",
        "#    display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>Body of Evidence:</b></div>'))\n",
        "    \n",
        "    if FIND_PDFS or SEARCH_MEDRXIV:\n",
        "        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])\n",
        "    else:\n",
        "        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])\n",
        "        \n",
        "    display(HTML(df.to_html(render_links=True, escape=False)))\n",
        "    return full_abs_list,ranked_aswers,pm_ids\n",
        "\n",
        "    \n"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y12l1rgYgAFF",
        "outputId": "42c9dbda-3376-4099-be1d-fb053ba342be"
      },
      "source": [
        "full_abs_list,ranked_aswers,pm_ids=displayResults(hit_dictionary, answers, query)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Lucene ID</th>\n",
              "      <th>BERT-SQuAD Answer with Highlights</th>\n",
              "      <th>Confidence</th>\n",
              "      <th>Title/Link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lassa fever</td>\n",
              "      <td><div> sentence _ 90 as of 2003,10-16 % of people in sierra leone and liberia admitted to hospital had the virus. sentence _ 91 the case fatality rate for those who are hospitalized for the disease is  <font color='red'>about 15-20 %</font> .</div></td>\n",
              "      <td>0.586256</td>\n",
              "      <td>Lassa fever</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Acute disseminated encephalomyelitis</td>\n",
              "      <td><div> sentence _ 11 adem shows seasonal variation with higher incidence in winter and spring months which may coincide with higher viral infections during these months. sentence _ 12 the mortality rate may be as high as 5 % ; however, full recovery is seen  <font color='red'>in 50 to 75 % of cases with increase in survival rates up to 70 to 90 %</font>  with figures including minor residual disability as well.</div></td>\n",
              "      <td>0.583253</td>\n",
              "      <td>Acute disseminated encephalomyelitis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Rosacea</td>\n",
              "      <td><div> sentence _ 59 other cases, if left untreated, worsen over time. section _ 11 behavior sentence _ 60 avoiding triggers that worsen the condition can help reduce the onset of rosacea,  <font color='red'>but alone will not normally lead to remission except in mild cases</font> .</div></td>\n",
              "      <td>0.578586</td>\n",
              "      <td>Rosacea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Transient global amnesia</td>\n",
              "      <td><div> sentence _ 82 tga is most common in people between age 56 and 75, with the average age of a person experiencing tga being approximately 62. section _ 12 see also list _ 2 amnesia | [ [ dissociative _ amnesia | dissociative amnesia ] ] <font color='red'>only mild symptoms with no need for hospitalization</font> </div></td>\n",
              "      <td>0.573354</td>\n",
              "      <td>Transient global amnesia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lung cancer</td>\n",
              "      <td><div> sentence _ 159 research has not found two other available tests — sputum cytology or [ [ chest _ radiograph | chest radiograph ] ] (cxr) screening tests — to have any benefit. sentence _ 160 the [ [ united _ states _ preventive _ services _ task _ force | united states preventive services task force ] ] (uspstf) recommends yearly screening using low-dose computed tomography in those who have a total smoking history of 30 pack-years and are  <font color='red'>between 55 and 80 years old</font>  until a person has not been smoking for more than 15 years.</div></td>\n",
              "      <td>0.550006</td>\n",
              "      <td>Lung cancer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Major trauma</td>\n",
              "      <td><div> sentence _ 24 penetrating trauma is caused when a [ [ foreign _ body | foreign body ] ] such as a bullet or a knife enters the [ [ tissue _ (biology) | body tissue ] ], creating an open wound. sentence _ 25 in the united states, most deaths caused by penetrating trauma occur in urban areas and 80 % of these deaths are caused  <font color='red'>by firearms</font> .</div></td>\n",
              "      <td>0.474236</td>\n",
              "      <td>Major trauma</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Bucillamine</td>\n",
              "      <td><div> food & drug administration (fda) has approved revive therapeutics ltd. to proceed with a randomized, double-blind, placebo-controlled confirmatory phase 3 clinical trial protocol to evaluate the safety and efficacy of bucillamine in patients  <font color='red'>with mild-moderate</font>  covid-19.</div></td>\n",
              "      <td>0.438401</td>\n",
              "      <td>Bucillamine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Arteriovenous malformation</td>\n",
              "      <td><div> miller ] ] was diagnosed with avm after filming [ [ yogi _ bear _ (film) | yogi bear ] ] in new zealand in 2010 ; miller described his experience with the disease on the pete holmes podcast you made it weird on october 28,2011, shedding his comedian side for a moment and becoming more philosophical, narrating his behaviors and inability to sleep during that time. he suffered a seizure upon return to los angeles and successfully underwent surgery that had a mortality rate  <font color='red'>of ten percent</font> . | jazz guitarist [ [ pat _ martino | pat martino ] ] experienced an avm and subsequently developed amnesia and manic depression.</div></td>\n",
              "      <td>0.400459</td>\n",
              "      <td>Arteriovenous malformation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Alcoholism</td>\n",
              "      <td><div> sentence _ 2 for the song by starsailor, see [ [ alcoholic _ (song) | alcoholic (song) ] ]. table _ 0 alcoholism | other names alcohol dependence syndrome, alcohol use disorder (aud) | [ [ specialty _ (medicine) | specialty ] ] [ [ psychiatry | psychiatry ] ], [ [ clinical _ psychology | clinical psychology ] ], toxicology, addiction medicine | symptoms drinking large amounts of alcohol over a long period, difficulty cutting down, acquiring and drinking alcohol taking up a lot of time, usage resulting in problems, [ [ alcohol _ withdrawal _ syndrome | withdrawal ] ] occurring when stopping | [ [ complication _ (medicine) | complications ] ] [ [ mental _ illness | mental illness ] ], [ [ delirium _ tremens | delirium ] ], [ [ wernicke – korsakoff _ syndrome | wernicke – korsakoff syndrome ] ], [ [ heart _ arrhythmia | irregular heartbeat ] ], [ [ cirrhosis | cirrhosis of the liver ] ], [ [ cancer | cancer ] ], [ [ fetal _ alcohol _ spectrum _ disorder | fetal alcohol spectrum disorder ] ], [ [ suicide | suicide ] ] | duration long term | causes environmental and genetic factors | risk factors [ [ stress _ (biological) | stress ] ], anxiety, inexpensive, easy access | [ [ diagnostic _ method | diagnostic method ] ] questionnaires, [ [ blood _ test | blood tests ] ] | treatment [ [ alcohol _ detoxification | alcohol detoxification ] ] typically with [ [ benzodiazepine | benzodiazepines ] ], counselling, [ [ acamprosate | acamprosate ] ], [ [ disulfiram | disulfiram ] ], [ [ naltrexone | naltrexone ] ] | frequency 380 million / 5. 1 % adults (2016) |  <font color='red'>deaths 3. 3 million / 5. 9 %</font>  sentence _ 3 alcoholism is, broadly, any drinking of [ [ alcohol _ (drug) | alcohol ] ] that results in significant mental or physical [ [ health | health ] ] problems.</div></td>\n",
              "      <td>0.348510</td>\n",
              "      <td>Alcoholism</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Economy of Lebanon</td>\n",
              "      <td><div> sentence _ 142 this economic crisis made lebanon ' s [ [ gross _ domestic _ product | gross domestic product ] ] fall to about $ 44 billion, which was about $ 55 billion the year before. sentence _ 143 the crisis became worse when the [ [ covid-19 _ pandemic | covid- <font color='red'>19 pandemic</font>  ] ] affected the lebanese economy.</div></td>\n",
              "      <td>0.297735</td>\n",
              "      <td>Economy of Lebanon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Sampling bias</td>\n",
              "      <td><div> sentence _ 69 this would adjust any estimates to achieve the same expected value as a sample that included exactly 50 men and 50 women, unless men and women differed in their likelihood of taking part in the survey. section _ 8 see also list _ 2 [ [ censored _ regression _ model | censored regression model ] ] | [ [ cherry _ picking _ (fallacy) | cherry picking (fallacy) ] ] | | [ [ friendship _ paradox | friendship paradox ] ] | [ [ reporting _ bias | reporting bias ] ] | [ [ sampling _ probability | sampling probability ] ] | [ [ selection _ bias | selection bias ] ] | [ [ spectrum _ bias | spectrum bias ] ] | [ [ truncated _ regression _ model | truncated regression model ] ] <font color='red'>only mild symptoms with no need for hospitalization</font> </div></td>\n",
              "      <td>0.218133</td>\n",
              "      <td>Sampling bias</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Llama</td>\n",
              "      <td><div> sentence _ 180 the fiber comes in many different colors ranging from white or grey to reddish-brown, brown, dark brown and black. table _ 2 animal fiber diameter ([ [ micrometre | micrometres ] ]) | [ [ vicuna # vicuna _ wool | vicuna ] ] 6 – 10 | alpaca ([ [ alpaca _ fiber | suri ] ]) 10 – 15 | [ [ muskox | muskox ] ] ([ [ qiviut | qiviut ] ]) 11 – 13 | [ [ merino # wool _ qualities | merino sheep ] ] 12 – 20 | [ [ angora _ rabbit | angora rabbit ] ] ([ [ angora _ wool | angora wool ] ]) 13 | [ [ cashmere _ goat | cashmere goat ] ] ([ [ cashmere _ wool | cashmere wool ] ]) 15 – 19 | [ [ yak | yak ] ] ([ [ yak _ fiber | yak fiber ] ]) 15 – 19 | [ [ camel | camel ] ] ([ [ camel _ hair | camel hair ] ]) 16 – 25 | [ [ guanaco # guanaco _ fiber | guanaco ] ] 16 – 18 | llama (tapada) 20 – 30 | [ [ chinchilla | chinchilla ] ] 21 | [ [ angora _ goat | angora goat ] ] ([ [ mohair | mohair ] ]) 25 – 45 | [ [ huacaya _ alpaca # fibre | huacaya alpaca ] ] 27. 7 | llama (ccara) 30 – 40 | average diameter of some of the finest, natural fibers section _ 17 see also <font color='red'>only mild symptoms with no need for hospitalization</font> </div></td>\n",
              "      <td>0.084864</td>\n",
              "      <td>Llama</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-40c5b353baa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_abs_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mranked_aswers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpm_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplayResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhit_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-133-0f28f52770f0>\u001b[0m in \u001b[0;36mdisplayResults\u001b[0;34m(hit_dictionary, answers, question)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfull_abs_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mranked_aswers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpm_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'pm_ids' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "bTcoserZgN1d",
        "outputId": "6e37309e-59d3-41fa-ef7d-b8ef48851642"
      },
      "source": [
        "hits[0].lucene_document.fields.toString().stored()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-9a31ead30b2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlucene_document\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstored\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'stored'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yis0B0OVH2_q"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbvIllGHH2SW",
        "outputId": "d37c2c80-1a42-485c-8552-ff2c37e29a37"
      },
      "source": [
        "#!pip install pyserini==0.8.1.0\n",
        "from pyserini.search import pysearch\n",
        "\n",
        "import re\n",
        "import sys\n",
        "!pip install anglicize\n",
        "from anglicize import anglicize\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "!python -m pip install -U spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from spacy.lang.en import English\n",
        "import spacy\n",
        "#!python -m spacy download en_core_web_sm\n",
        "import spacy.cli # Colab fix\n",
        "spacy.cli.download(\"en_core_web_sm\") # Colab fix\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "#nlp.add_pipe(\"merge_noun_chunks\")\n",
        "nlp.add_pipe(nlp.create_pipe('merge_noun_chunks')) # Colab fix?\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "from timeit import default_timer as timer\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import statistics\n",
        "import pandas as pd\n",
        "\n",
        "DATA_TRAIN = \"/content/train.jsonl\" # Change this\n",
        "DATA_DEV = \"/content/dev.jsonl\" # Change this\n",
        "\n",
        "dataset_path = DATA_DEV\n",
        "#dataset_path = DATA_TRAIN\n",
        "\n",
        "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "pd.set_option(\"display.max_rows\", 500)\n",
        "pd.set_option(\"display.max_columns\", 500)\n",
        "pd.set_option(\"display.width\", 1000)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: anglicize in /usr/local/lib/python3.7/dist-packages (0.0.3)\n",
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: thinc<8.1.0,>=8.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.7)\n",
            "Requirement already satisfied, skipping upgrade: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-RP7PSiR3C3"
      },
      "source": [
        "luceneDir = 'indexes/fever/lucene-index-fever-paragraph'\n",
        "searcher = pysearch.SimpleSearcher(luceneDir)\n",
        "\n",
        "def get_relevant_pages(claim, FULL_CLAIM, SPACY_ENTITIES, CASE_ENTITIES, ADD_SYNONYMS, ADD_SIMILAR, \n",
        "          LINKED_PAGES, LINKING_PAGES, CATCH_VERBS, TFIDF_TOP, FIND_ALL, N_RESULTS=70, SIMILAR_THRESHOLD=0.85, TFIDF_TOP_N=3):\n",
        "    \n",
        "    if LINKING_PAGES:\n",
        "        import sqlalchemy as sqla\n",
        "\n",
        "        db_fullpath = \"D:/FEVER/feverous_wikiv1.db\" # Change this\n",
        "        db = sqla.create_engine(\"sqlite:///{}\".format(db_fullpath))\n",
        "\n",
        "        from sqlalchemy.orm import sessionmaker\n",
        "\n",
        "        Session = sessionmaker(bind=db)\n",
        "        session = Session()\n",
        "    \n",
        "    if not SPACY_ENTITIES and not CASE_ENTITIES and not LINKED_PAGES:\n",
        "        FULL_CLAIM = True\n",
        "    \n",
        "    start_timer = timer()\n",
        "\n",
        "    claim = nlp(claim.replace(\" \", \" \").replace(\"­\", \" \")) # Replaces the obnoxious space character with normal space\n",
        "\n",
        "    keywords = set()\n",
        "    entities = set()\n",
        "\n",
        "    if SPACY_ENTITIES:\n",
        "        spacy_entities = [entity.text for entity in claim.ents]\n",
        "        entities.update(spacy_entities)\n",
        "\n",
        "    if CASE_ENTITIES:\n",
        "        case_entities = set()\n",
        "        chunks = claim.noun_chunks\n",
        "        for chunk in chunks:\n",
        "            for token in tokenizer(chunk.text):\n",
        "                if token.text[0].isupper():\n",
        "                    case_entities.add(chunk.text)\n",
        "                    break\n",
        "\n",
        "        entities.update(case_entities)\n",
        "        #print(case_entities)\n",
        "        #print(entities)\n",
        "        #sys.exit(0)\n",
        "\n",
        "    keywords.update(entities)\n",
        "    #print(keywords)\n",
        "\n",
        "    if ADD_SYNONYMS:\n",
        "        for token in claim:\n",
        "            if token.is_stop:\n",
        "                continue\n",
        "            synonyms = wn.synsets(token.text)\n",
        "            if synonyms and token:\n",
        "                for synonym in synonyms:\n",
        "                    if synonym.pos() == token.pos_[0].lower() and synonym.pos() == \"n\":\n",
        "                        keywords.update([lemma.replace(\"_\", \" \") for lemma in synonym.lemma_names()])\n",
        "\n",
        "    if ADD_SIMILAR:\n",
        "        similar_check = [keyword for keyword in list(keywords) if keyword in vocab]\n",
        "\n",
        "        if similar_check:\n",
        "            similar_words = model.most_similar(positive=similar_check)\n",
        "            for i in range(0, len(similar_words)):\n",
        "                if i == 0 or similar_words[i][1] >= SIMILAR_THRESHOLD:\n",
        "                    keywords.update([similar_words[i][0]])\n",
        "    \n",
        "    if CATCH_VERBS:\n",
        "        for chunk in claim:\n",
        "            if chunk.pos_ == \"VERB\" and chunk.dep_ != \"case\" and chunk.dep_ != \"prep\":\n",
        "                keywords.update([chunk.lemma_])\n",
        "                \n",
        "    if TFIDF_TOP:\n",
        "        tfidf_sorting = np.argsort(tfidf.transform([claim.text]).toarray()).flatten()[::-1]\n",
        "        tfidf_len = len(tfidf_tokenize(claim.text))\n",
        "        keywords.update(tfidf_features[tfidf_sorting][0:min([TFIDF_TOP_N, tfidf_len])])\n",
        "\n",
        "    if not FULL_CLAIM:\n",
        "        search_query = (\", \".join(keywords) + '\"' + '\", \"'.join(entities) + '\"')\n",
        "#            search_query = (\", \".join(keywords) + \", \".join(entities))\n",
        "    else:\n",
        "        search_query = (claim.text + ' \"' + '\", \"'.join(keywords) + '\" \"' + '\", \"'.join(entities) + '\"')\n",
        "#            search_query = (claim.text + \" \" + \", \".join(keywords) + \", \".join(entities))\n",
        "\n",
        "    if FULL_CLAIM or keywords:\n",
        "        try:\n",
        "            lucene_hits = searcher.search(search_query.encode(\"utf-8\"), k=N_RESULTS)\n",
        "        except:\n",
        "            lucene_hits = None\n",
        "    else:\n",
        "        lucene_hits = None\n",
        "\n",
        "    hit_dictionary = {}\n",
        "\n",
        "    if lucene_hits:\n",
        "\n",
        "        for hit in lucene_hits:\n",
        "            hit_dict = {\"abstract\": None, \"real_abstract\": None, \"title\": None}\n",
        "            hit_dict['abstract']=hit.lucene_document.get(\"raw\")\n",
        "            hit_dict['real_abstract']=hit.lucene_document.get(\"raw\")\n",
        "            hit_dict['title'] = str(hit.docid)\n",
        "            hit_dictionary[str(hit.docid)] = hit_dict\n",
        "    \n",
        "        linked_pages = set()\n",
        "        if LINKED_PAGES:\n",
        "            for hit in lucene_hits:\n",
        "                links = re.findall(r\"(?:\\[\\[)(.*?)(?:\\|)\", hit.raw)\n",
        "                for link in links:\n",
        "                    linked_pages.update([link.replace(\"_\", \" \").encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\")])\n",
        "                    \n",
        "        linking_pages = set()\n",
        "        if LINKING_PAGES:\n",
        "            linking_results = session.execute('select target, sources from inlinks where target IN (\"{}\")'.format('\", \"'.join([hit.docid.replace('\"', '\"\"') for hit in lucene_hits])))\n",
        "            for row in linking_results:\n",
        "                linking_pages.update(row[\"sources\"].split(\";\"))\n",
        "            \"\"\"\n",
        "            for hit in lucene_hits:\n",
        "                linking_results = session.execute('select target, sources from inlinks where sources = \"{}\"'.format(hit.docid))\n",
        "                for row in linking_results:\n",
        "                    linking_pages.update(row[\"sources\"].split(\";\"))\"\"\"\n",
        "\n",
        "                    \n",
        "        #print(lucene_hits[0].docid.encode(\"latin-1\").decode(\"utf-8\"))\n",
        "        found_pages = [hit.docid for hit in lucene_hits]\n",
        "        #found_pages = set([hit.docid.encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\") for page in lucene_hits])\n",
        "        for i in range(0, len(found_pages)):\n",
        "\n",
        "            try:\n",
        "                found_pages[i] = found_pages[i].encode(\"latin-1\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
        "                #print(\"Done:\",found_pages[i])\n",
        "            except:\n",
        "                import regex\n",
        "                print(found_pages[i], anglicize(found_pages[i]), regex.sub(r'[^\\p{Latin}]', '', found_pages[i]).encode(\"latin-1\").decode(\"utf-8\"))\n",
        "                \n",
        "        found_pages = set(found_pages)\n",
        "\n",
        "        hit_scores = [hit.score for hit in lucene_hits]\n",
        "        hit_score_min = min(hit_scores)\n",
        "        hit_score_25 = np.percentile(hit_scores, 25)\n",
        "        hit_score_mean = np.mean(hit_scores)\n",
        "        hit_score_median = statistics.median(hit_scores)\n",
        "        hit_score_75 = np.percentile(hit_scores, 75)\n",
        "        hit_score_max = max(hit_scores)\n",
        "    \n",
        "    else:\n",
        "        \n",
        "        is_found = False\n",
        "        found_pages = set()\n",
        "        entities = set()\n",
        "        keywords = set()\n",
        "        linked_pages = set()\n",
        "        lucene_hits = []\n",
        "        hit_scores = []\n",
        "        hit_score_min = None\n",
        "        hit_score_25 = None\n",
        "        hit_score_mean = None\n",
        "        hit_score_median = None\n",
        "        hit_score_75 = None\n",
        "        hit_score_max = None\n",
        "\n",
        "    end_timer = timer()\n",
        "    elapsed = int(end_timer - start_timer)\n",
        "    elapsed_formatted = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))\n",
        "        \n",
        "    result = [claim.text, elapsed_formatted, \n",
        "                found_pages, \n",
        "                keywords, \n",
        "                linked_pages, \n",
        "                linking_pages,\n",
        "                len(lucene_hits), \n",
        "                hit_score_min, hit_score_25, hit_score_mean, hit_score_median, \n",
        "                hit_score_75, hit_score_max, hit_scores]\n",
        "\n",
        "    #print(result)\n",
        "    \n",
        "    result = pd.DataFrame([result], columns=[\"CLAIM\", \"ELAPSED\", \n",
        "                                             \"PAGES_FOUND\",\n",
        "                                             \"KEYWORDS\", \n",
        "                                             \"LINKED_PAGES\",\n",
        "                                             \"LINKING_PAGES\",\n",
        "                                             \"N_LUCENE_HITS\", \n",
        "                                             \"HIT_SCORE_MIN\", \"HIT_SCORE_25\", \"HIT_SCORE_MEAN\", \"HIT_SCORE_MEDIAN\", \n",
        "                                             \"HIT_SCORE_75\", \"HIT_SCORE_MAX\", \"HIT_SCORES\"])\n",
        "    \n",
        "    return result, hit_dictionary"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "p5eQYYTJT6Rn",
        "outputId": "6a2b319e-3062-4863-fb3b-d004f2881773"
      },
      "source": [
        "hit_stats, hit_dictionary = get_relevant_pages(dataset[0][\"claim\"], FULL_CLAIM=True, SPACY_ENTITIES=True, CASE_ENTITIES=True, ADD_SYNONYMS=False, ADD_SIMILAR=False, \n",
        "            LINKED_PAGES=False, LINKING_PAGES=False, CATCH_VERBS=False, TFIDF_TOP=False, FIND_ALL=True, N_RESULTS=70)\n",
        "\n",
        "print([key for key in hit_dictionary])\n",
        "hit_stats"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ailanthus', 'Myrica', 'Polygonia c-aureum', 'Sasakia charonda', 'Scolitantides orion', 'Gravitcornutia sodalicia', 'Oegoconia caradjai', 'Gnathocolumna', 'Atrocenta', 'Rubroxena', 'Furcinetechma', 'Urania sloanus', 'Metacrias huttoni', 'Spiraea', 'Acacia sensu lato', 'Elysius amapaensis', 'Elysius anomala', 'Elysius barnesi', 'Elysius gladysia', 'Elysius systron', 'Metacrias erichrysa', 'Elysius atrobrunnea', 'Elysius jonesi', 'Elysius ochrota', 'Elysius pretiosa', 'Elysius subterra', 'Elysius carbonarius', 'Elysius chimaera', 'Elysius conjunctus', 'Elysius deceptura', 'Elysius flavoabdominalis', 'Elysius hades', 'Elysius lavinia', 'Elysius ordinaria', 'Elysius phantasma', 'Elysius terraoides', 'Elysius thrailkilli', 'Bonagota arizonae', 'Elysius ruffin', 'Izatha peroneanella', 'Elysius disciplaga', 'Elysius atrata', 'Elysius conspersus', 'Elysius melanoplaga', 'Apolychrosis schwerdtfegeri', 'Anopina guatemalana', 'Anopina phaeopina', 'Apolychrosis ambogonium', 'Atepa colaptes', 'Bicavernaria henicodes', 'Bidorpitia boliviana', 'Bidorpitia cryptica', 'Bidorpitia megasaccula', 'Bidorpitia poolei', 'Chilips claduncus', 'Clarkenia superba', 'Coryssovalva cosmocosta', 'Deltinea costalimai', 'Deltobathra platamodes', 'Dorithia imitatrix', 'Dorithia paraviridana', 'Dorithia wellingana', 'Eriotortrix iresinephora', 'Eriotortrix isipida', 'Ernocornutina gambra', 'Eubetia bigaulae', 'Exoletuncus exoristus', 'Galomecalpa monogramma', 'Gauruncus gelastes', 'Gnatheulia gnathocera']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CLAIM</th>\n",
              "      <th>ELAPSED</th>\n",
              "      <th>PAGES_FOUND</th>\n",
              "      <th>KEYWORDS</th>\n",
              "      <th>LINKED_PAGES</th>\n",
              "      <th>LINKING_PAGES</th>\n",
              "      <th>N_LUCENE_HITS</th>\n",
              "      <th>HIT_SCORE_MIN</th>\n",
              "      <th>HIT_SCORE_25</th>\n",
              "      <th>HIT_SCORE_MEAN</th>\n",
              "      <th>HIT_SCORE_MEDIAN</th>\n",
              "      <th>HIT_SCORE_75</th>\n",
              "      <th>HIT_SCORE_MAX</th>\n",
              "      <th>HIT_SCORES</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tridrepana hainana is a moth found in China th...</td>\n",
              "      <td>00:00:00</td>\n",
              "      <td>{Deltinea costalimai, Elysius conjunctus, Apol...</td>\n",
              "      <td>{Lepidoptera, China, Tridrepana hainana, the D...</td>\n",
              "      <td>{}</td>\n",
              "      <td>{}</td>\n",
              "      <td>70</td>\n",
              "      <td>18.327576</td>\n",
              "      <td>18.327593</td>\n",
              "      <td>18.86494</td>\n",
              "      <td>18.505691</td>\n",
              "      <td>18.526397</td>\n",
              "      <td>25.760599</td>\n",
              "      <td>[25.76059913635254, 22.830799102783203, 21.547...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               CLAIM   ELAPSED                                        PAGES_FOUND                                           KEYWORDS LINKED_PAGES LINKING_PAGES  N_LUCENE_HITS  HIT_SCORE_MIN  HIT_SCORE_25  HIT_SCORE_MEAN  HIT_SCORE_MEDIAN  HIT_SCORE_75  HIT_SCORE_MAX                                         HIT_SCORES\n",
              "0  Tridrepana hainana is a moth found in China th...  00:00:00  {Deltinea costalimai, Elysius conjunctus, Apol...  {Lepidoptera, China, Tridrepana hainana, the D...           {}            {}             70      18.327576     18.327593        18.86494         18.505691     18.526397      25.760599  [25.76059913635254, 22.830799102783203, 21.547..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJFhuZZQeee6"
      },
      "source": [
        "def get_answers(query, hit_dictionary):\n",
        "    for idx,v in hit_dictionary.items():\n",
        "        abs_dirty = v['abstract']\n",
        "        real_abs_dirty = v['real_abstract']\n",
        "        #abs_dirty = v['paragraph']\n",
        "        # looks like the abstract value can be an empty list\n",
        "        v['abstract_paragraphs'] = []\n",
        "        v['abstract_full'] = ''\n",
        "        v['real_abstract_full'] = ''\n",
        "        v['real_abstract_paragraphs']=[]\n",
        "\n",
        "        if abs_dirty:\n",
        "            # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
        "            # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
        "            # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
        "\n",
        "            if isinstance(abs_dirty, list):\n",
        "                for p in abs_dirty:\n",
        "                    v['abstract_paragraphs'].append(p['text'])\n",
        "                    v['abstract_full'] += p['text'] + ' \\n\\n'\n",
        "\n",
        "            # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
        "            if isinstance(abs_dirty, str):\n",
        "                v['abstract_paragraphs'].append(abs_dirty)\n",
        "                v['abstract_full'] += abs_dirty + ' \\n\\n'\n",
        "        if real_abs_dirty:\n",
        "            # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
        "            # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
        "            # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
        "\n",
        "\n",
        "            if isinstance(real_abs_dirty, list):\n",
        "                for p in real_abs_dirty:\n",
        "                    v['real_abstract_paragraphs'].append(p['text'])\n",
        "                    v['real_abstract_full'] += p['text'] + ' \\n\\n'\n",
        "\n",
        "            # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
        "            if isinstance(abs_dirty, str):\n",
        "                v['real_abstract_paragraphs'].append(real_abs_dirty)\n",
        "                v['real_abstract_full'] += real_abs_dirty + ' \\n\\n'\n",
        "        v[\"indexed_para\"]=v[\"abstract_full\"][len(v[\"real_abstract_full\"]):]\n",
        "\n",
        "    def embed_useT(module):\n",
        "        with tf.Graph().as_default():\n",
        "            sentences = tf.compat.v1.placeholder(tf.string)\n",
        "            embed = hub.Module(module)\n",
        "            embeddings = embed(sentences)\n",
        "            session = tf.compat.v1.train.MonitoredSession()\n",
        "        return lambda x: session.run(embeddings, {sentences: x})\n",
        "    embed_fn = embed_useT('/root/kaggle/working/sentence_wise_email/module/module_useT')\n",
        "\n",
        "    import numpy as np\n",
        "    #Reconstruct text for generating the answer text from the BERT result, do the necessary text preprocessing and construct the answer.\n",
        "    def reconstructText(tokens, start=0, stop=-1):\n",
        "        tokens = tokens[start: stop]\n",
        "        if '[SEP]' in tokens:\n",
        "            sepind = tokens.index('[SEP]')\n",
        "            tokens = tokens[sepind+1:]\n",
        "        txt = ' '.join(tokens)\n",
        "        txt = txt.replace(' ##', '')\n",
        "        txt = txt.replace('##', '')\n",
        "        txt = txt.strip()\n",
        "        txt = \" \".join(txt.split())\n",
        "        txt = txt.replace(' .', '.')\n",
        "        txt = txt.replace('( ', '(')\n",
        "        txt = txt.replace(' )', ')')\n",
        "        txt = txt.replace(' - ', '-')\n",
        "        txt_list = txt.split(' , ')\n",
        "        txt = ''\n",
        "        nTxtL = len(txt_list)\n",
        "        if nTxtL == 1:\n",
        "            return txt_list[0]\n",
        "        newList =[]\n",
        "        for i,t in enumerate(txt_list):\n",
        "            if i < nTxtL -1:\n",
        "                if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n",
        "                    newList += [t,',']\n",
        "                else:\n",
        "                    newList += [t, ', ']\n",
        "            else:\n",
        "                newList += [t]\n",
        "        return ''.join(newList)\n",
        "\n",
        "\n",
        "    def makeBERTSQuADPrediction(document, question):\n",
        "        ## we need to rewrite this function so that it chuncks the document into 250-300 word segments with\n",
        "        ## 50 word overlaps on either end so that it can understand and check longer abstracts\n",
        "        ## Get chuck of documents in order to overcome the BERT's max 512 token problem. First count the number of tokens. For example if it is\n",
        "        ##   larger than 2048 token determine the split size and overlapping token count. Split the document into pieces to fed into BERT encoding. Overlapping\n",
        "        ##      is necessary to maintain the coherence between the splits. If the token count is larger than some number 2560 token approximately this will fail.\n",
        "        nWords = len(document.split())\n",
        "        input_ids_all = QA_TOKENIZER.encode(question, document)\n",
        "        tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n",
        "        overlapFac = 1.1\n",
        "        if len(input_ids_all)*overlapFac > 2560:\n",
        "            nSearchWords = int(np.ceil(nWords/6))\n",
        "            fifth = int(np.ceil(nWords/5))\n",
        "            docSplit = document.split()\n",
        "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
        "                        ' '.join(docSplit[fifth-int(nSearchWords*overlapFac/2):fifth+int(fifth*overlapFac/2)]),\n",
        "                        ' '.join(docSplit[fifth*2-int(nSearchWords*overlapFac/2):fifth*2+int(fifth*overlapFac/2)]),\n",
        "                        ' '.join(docSplit[fifth*3-int(nSearchWords*overlapFac/2):fifth*3+int(fifth*overlapFac/2)]),\n",
        "                        ' '.join(docSplit[fifth*4-int(nSearchWords*overlapFac/2):fifth*4+int(fifth*overlapFac/2)]),\n",
        "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
        "\n",
        "        elif len(input_ids_all)*overlapFac > 2048:\n",
        "            nSearchWords = int(np.ceil(nWords/5))\n",
        "            quarter = int(np.ceil(nWords/4))\n",
        "            docSplit = document.split()\n",
        "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
        "                        ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),\n",
        "                        ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),\n",
        "                        ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),\n",
        "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
        "            \n",
        "        elif len(input_ids_all)*overlapFac > 1536:\n",
        "            nSearchWords = int(np.ceil(nWords/4))\n",
        "            third = int(np.ceil(nWords/3))\n",
        "            docSplit = document.split()\n",
        "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
        "                        ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),\n",
        "                        ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),\n",
        "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
        "            \n",
        "        elif len(input_ids_all)*overlapFac > 1024:\n",
        "            nSearchWords = int(np.ceil(nWords/3))\n",
        "            middle = int(np.ceil(nWords/2))\n",
        "            docSplit = document.split()\n",
        "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
        "                        ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),\n",
        "                        ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
        "        elif len(input_ids_all)*overlapFac > 512:\n",
        "            nSearchWords = int(np.ceil(nWords/2))\n",
        "            docSplit = document.split()\n",
        "            docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "            input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
        "        else:\n",
        "            input_ids = [input_ids_all]\n",
        "        absTooLong = False    \n",
        "        \n",
        "        answers = []\n",
        "        cons = []\n",
        "        #print(input_ids)\n",
        "        for iptIds in input_ids:\n",
        "            tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n",
        "            #print(tokens)\n",
        "            sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)\n",
        "            num_seg_a = sep_index + 1\n",
        "            num_seg_b = len(iptIds) - num_seg_a\n",
        "            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "            assert len(segment_ids) == len(iptIds)\n",
        "            n_ids = len(segment_ids)\n",
        "            #print(n_ids)\n",
        "            if n_ids < 512:\n",
        "                outputs = QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
        "                                        token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n",
        "                #print(QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
        "                #                        token_type_ids=torch.tensor([segment_ids]).to(torch_device)))\n",
        "            else:\n",
        "                #this cuts off the text if its more than 512 words so it fits in model space\n",
        "                #need run multiple inferences for longer text. add to the todo. I will try to expand this to longer sequences if it is necessary. \n",
        "                print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n",
        "                absTooLong = True\n",
        "                outputs= QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device), \n",
        "                                        token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))\n",
        "            start_scores=outputs.start_logits\n",
        "            end_scores=outputs.end_logits\n",
        "            start_scores = start_scores[:,1:-1]\n",
        "            end_scores = end_scores[:,1:-1]\n",
        "            answer_start = torch.argmax(start_scores)\n",
        "            answer_end = torch.argmax(end_scores)\n",
        "            #print(answer_start, answer_end)\n",
        "            answer = reconstructText(tokens, answer_start, answer_end+2)\n",
        "        \n",
        "            if answer.startswith('. ') or answer.startswith(', '):\n",
        "                answer = answer[2:]\n",
        "                \n",
        "            c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n",
        "            answers.append(answer)\n",
        "            cons.append(c)\n",
        "        \n",
        "        maxC = max(cons)\n",
        "        iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n",
        "        confidence = cons[iMaxC]\n",
        "        answer = answers[iMaxC]\n",
        "        \n",
        "        sep_index = tokens_all.index('[SEP]')\n",
        "        full_txt_tokens = tokens_all[sep_index+1:]\n",
        "        \n",
        "        abs_returned = reconstructText(full_txt_tokens)\n",
        "\n",
        "        ans={}\n",
        "        ans['answer'] = answer\n",
        "        #print(answer)\n",
        "        if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n",
        "            ans['confidence'] = -1000000\n",
        "        else:\n",
        "            #confidence = torch.max(start_scores) + torch.max(end_scores)\n",
        "            #confidence = np.log(confidence.item())\n",
        "            ans['confidence'] = confidence\n",
        "        #ans['start'] = answer_start.item()\n",
        "        #ans['end'] = answer_end.item()\n",
        "        ans['abstract_bert'] = abs_returned\n",
        "        ans['abs_too_long'] = absTooLong\n",
        "        return ans\n",
        "\n",
        "    from tqdm import tqdm\n",
        "    def searchAbstracts(hit_dictionary, question):\n",
        "        abstractResults = {}\n",
        "        for k,v in tqdm(hit_dictionary.items()):\n",
        "            abstract = v['abstract_full']\n",
        "            indexed_para=v['indexed_para']\n",
        "            if abstract:\n",
        "                ans = makeBERTSQuADPrediction(abstract, question)\n",
        "                if ans['answer']:\n",
        "                    confidence = ans['confidence']\n",
        "                    abstractResults[confidence]={}\n",
        "                    abstractResults[confidence]['answer'] = ans['answer']\n",
        "                    #abstractResults[confidence]['start'] = ans['start']\n",
        "                    #abstractResults[confidence]['end'] = ans['end']\n",
        "                    abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n",
        "                    abstractResults[confidence]['idx'] = k\n",
        "                    abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n",
        "\n",
        "                    \n",
        "        cList = list(abstractResults.keys())\n",
        "\n",
        "        if cList:\n",
        "            maxScore = max(cList)\n",
        "            total = 0.0\n",
        "            exp_scores = []\n",
        "            for c in cList:\n",
        "                s = np.exp(c-maxScore)\n",
        "                exp_scores.append(s)\n",
        "            total = sum(exp_scores)\n",
        "            for i,c in enumerate(cList):\n",
        "                abstractResults[exp_scores[i]/total] = abstractResults.pop(c)\n",
        "        return abstractResults\n",
        "\n",
        "    answers = searchAbstracts(hit_dictionary, query)\n",
        "    \n",
        "    FIND_PDFS=False \n",
        "    SEARCH_MEDRXIV=False\n",
        "\n",
        "    workingPath = '/root/kaggle/working'\n",
        "    import pandas as pd\n",
        "    import re\n",
        "    if FIND_PDFS:\n",
        "        from metapub import UrlReverse\n",
        "        from metapub import FindIt\n",
        "    from IPython.core.display import display, HTML\n",
        "\n",
        "    #from summarizer import Summarizer\n",
        "    #summarizerModel = Summarizer()\n",
        "    def displayResults(hit_dictionary, answers, question):\n",
        "        \n",
        "        question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n",
        "        #all_HTML_txt = question_HTML\n",
        "        confidence = list(answers.keys())\n",
        "        confidence.sort(reverse=True)\n",
        "        \n",
        "        confidence = list(answers.keys())\n",
        "        confidence.sort(reverse=True)\n",
        "        \n",
        "\n",
        "        for c in confidence:\n",
        "            if c>0 and c <= 1 and len(answers[c]['answer']) != 0:\n",
        "                if 'idx' not in  answers[c]:\n",
        "                    continue\n",
        "                rowData = []\n",
        "                idx = answers[c]['idx']\n",
        "                title = hit_dictionary[idx]['title']\n",
        "\n",
        "                \n",
        "                full_abs = answers[c]['abstract_bert']\n",
        "                bert_ans = answers[c]['answer']\n",
        "                \n",
        "                \n",
        "                #split_abs = full_abs.split(bert_ans)\n",
        "                #x=(split_abs[0][:split_abs[0].rfind('. ')+1])\n",
        "                #sentance_beginning = x[x.rfind('. ')+1:] + split_abs[0][split_abs[0].rfind('. ')+1:]\n",
        "                #sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n",
        "                x=''\n",
        "                y=''\n",
        "                z=''\n",
        "                t=''\n",
        "                regex = r\"(\\.\\s[^0-9])(?!.*(\\.\\s[^0-9]))\"\n",
        "                split_abs = full_abs.split(bert_ans)\n",
        "                matches = re.finditer(regex, split_abs[0], re.MULTILINE)\n",
        "                for matchNum1, match in enumerate(matches, start=1):\n",
        "                    y=(split_abs[0][:match.start()+1])\n",
        "                matches = re.finditer(regex, y, re.MULTILINE)\n",
        "                for matchNum2, match in enumerate(matches, start=1):\n",
        "                    x=(y[match.start()+1:])\n",
        "                if x=='':\n",
        "                    x=split_abs[0]\n",
        "                matches = re.finditer(regex, split_abs[0], re.MULTILINE)\n",
        "                for matchNum3, match in enumerate(matches, start=1):\n",
        "                    z=(split_abs[0][match.start()+1:])  \n",
        "                sentance_beginning = x+z\n",
        "                regex2=r\"(.*?(?<!\\b\\w)[.?!])\\s+[a-zA-Z0-9]\"\n",
        "                if len(split_abs) == 1:\n",
        "                    sentance_end_pos = len(full_abs)\n",
        "                    sentance_end =''\n",
        "                else:\n",
        "                    matches = re.finditer(regex2, split_abs[1], re.MULTILINE)\n",
        "                    for matchNum4, match in enumerate(matches, start=1):\n",
        "                        if matchNum4==1:\n",
        "                            t=(split_abs[1][:match.end()-2])\n",
        "                    sentance_end_pos = split_abs[1].find('. ')+1\n",
        "                    if sentance_end_pos == 0:\n",
        "                        sentance_end = split_abs[1]\n",
        "                    else:\n",
        "                        sentance_end = t\n",
        "                #if len(split_abs) == 1:\n",
        "                #    sentance_end_pos = len(full_abs)\n",
        "                #    sentance_end =''\n",
        "                #else:\n",
        "                #    sentance_end_pos = split_abs[1].find('. ')+1\n",
        "                #    if sentance_end_pos == 0:\n",
        "                #      sentance_end = split_abs[1]\n",
        "                #    else:\n",
        "                #      sentance_end = split_abs[1][:sentance_end_pos]\n",
        "                    \n",
        "                #sentance_full = sentance_beginning + bert_ans+ sentance_end\n",
        "                answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n",
        "                answers[c]['partial_answer'] = bert_ans+sentance_end\n",
        "                answers[c]['sentence_beginning'] = sentance_beginning\n",
        "                answers[c]['sentence_end'] = sentance_end\n",
        "                answers[c]['title'] = title\n",
        "            else:\n",
        "                answers.pop(c)\n",
        "        \n",
        "        \n",
        "        ## now rerank based on semantic similarity of the answers to the question\n",
        "        ## Universal sentence encoder\n",
        "        cList = list(answers.keys())\n",
        "        allAnswers = [answers[c]['full_answer'] for c in cList]\n",
        "        \n",
        "        messages = [question]+allAnswers\n",
        "        \n",
        "        encoding_matrix = embed_fn(messages)\n",
        "        similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n",
        "        rankings = similarity_matrix[1:,0]\n",
        "        \n",
        "        for i,c in enumerate(cList):\n",
        "            answers[rankings[i]] = answers.pop(c)\n",
        "\n",
        "        ## now form pandas dv\n",
        "        confidence = list(answers.keys())\n",
        "        confidence.sort(reverse=True)\n",
        "        pandasData = []\n",
        "        ranked_aswers = []\n",
        "        full_abs_list=[]\n",
        "        for c in confidence:\n",
        "            rowData=[]\n",
        "            title = answers[c]['title']\n",
        "            idx = answers[c]['idx']\n",
        "            rowData += [idx]            \n",
        "            sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='red'>\"+answers[c]['answer']+\"</font> \"+answers[c]['sentence_end']+'</div>'\n",
        "            answer_key = answers[c]['answer'].split(\"\\t\")[-1].strip() if \"\\t\" in answers[c]['answer'] else answers[c]['answer'].strip()\n",
        "\n",
        "            rowData += [sentance_html, answer_key, c,title]\n",
        "            pandasData.append(rowData)\n",
        "            ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n",
        "            full_abs_list.append(' '.join([answers[c]['abstract_bert']]))\n",
        "        \n",
        "        if FIND_PDFS or SEARCH_MEDRXIV:\n",
        "            pdata2 = []\n",
        "            pm_ids = []\n",
        "            for rowData in pandasData:\n",
        "                rd = rowData\n",
        "                idx = rowData[0]\n",
        "        else:\n",
        "            pdata2 = pandasData\n",
        "            \n",
        "        \n",
        "        display(HTML(question_HTML))\n",
        "\n",
        "        if USE_SUMMARY:\n",
        "            ## try generating an exacutive summary with bart abstractive summarizer\n",
        "            allAnswersTxt = ' '.join(ranked_aswers[:10]).replace('\\n','')\n",
        "        #    exec_sum = summarizerModel(allAnswersTxt, min_length=1, max_length=500)    \n",
        "        #   execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>BERT Extractive Summary:</b>: '+exec_sum+'</div>'\n",
        "\n",
        "            answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=2048)['input_ids'].to(torch_device)\n",
        "            #answers_input_ids = SUMMARY_TOKENIZER.prepare_seq2seq_batch([allAnswersTxt], return_tensors='pt', truncation=True, padding='longest')['input_ids'].to(torch_device)\n",
        "            summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n",
        "                                                num_beams=10,\n",
        "                                                length_penalty=1.1,\n",
        "                                                max_length=2048,\n",
        "                                                min_length=64,\n",
        "                                                no_repeat_ngram_size=0,\n",
        "                                                    do_sample=False )\n",
        "\n",
        "            exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
        "            execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>BART Abstractive Summary:</b>: '+exec_sum+'</div>'\n",
        "            display(HTML(execSum_HTML))\n",
        "            warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 12px; padding-bottom:12px; color:#CCCC00; margin-top:1pt\"> Warning this is an autogenerated summary based on semantic search of abstracts, always examine the sources before accepting this conclusion.  If the evidence only mentions topic in passing or the evidence is not clear, the summary will likely not clearly answer the question.</div>'\n",
        "            display(HTML(warning_HTML))\n",
        "\n",
        "    #    display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>Body of Evidence:</b></div>'))\n",
        "        \n",
        "        if FIND_PDFS or SEARCH_MEDRXIV:\n",
        "            df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Answer Key', 'Confidence', 'Title/Link'])\n",
        "        else:\n",
        "            df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Answer Key', 'Confidence', 'Title/Link'])\n",
        "            \n",
        "        display(HTML(df.to_html(render_links=True, escape=False)))\n",
        "        return full_abs_list,ranked_aswers # pm_ids? (local variable 'summary_ids' referenced before assignment)\n",
        "\n",
        "    full_abs_list,ranked_aswers=displayResults(hit_dictionary, answers, query)\n",
        "\n"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QEp2LU2fbmgl",
        "outputId": "c4928fa8-a772-422a-da53-24293dd3a912"
      },
      "source": [
        "for data in dataset[:1]:\n",
        "    claim = data[\"claim\"]\n",
        "    hit_stats, hit_dictionary = get_relevant_pages(claim, FULL_CLAIM=True, SPACY_ENTITIES=True, CASE_ENTITIES=True, ADD_SYNONYMS=False, ADD_SIMILAR=False, \n",
        "                            LINKED_PAGES=False, LINKING_PAGES=False, CATCH_VERBS=False, TFIDF_TOP=False, FIND_ALL=True, N_RESULTS=70)\n",
        "    \n",
        "    get_answers(claim, hit_dictionary)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/70 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 351 words long.  There are 571 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 2/70 [00:00<00:06,  9.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 585 words long.  There are 618 tokens\n",
            "****** warning only considering first 512 tokens, document is 477 words long.  There are 792 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 12/70 [00:00<00:03, 16.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 316 words long.  There are 591 tokens\n",
            "****** warning only considering first 512 tokens, document is 523 words long.  There are 625 tokens\n",
            "****** warning only considering first 512 tokens, document is 512 words long.  There are 520 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 14/70 [00:00<00:03, 14.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2887 words long.  There are 1544 tokens\n",
            "****** warning only considering first 512 tokens, document is 2887 words long.  There are 1285 tokens\n",
            "****** warning only considering first 512 tokens, document is 2887 words long.  There are 1421 tokens\n",
            "****** warning only considering first 512 tokens, document is 2887 words long.  There are 1291 tokens\n",
            "****** warning only considering first 512 tokens, document is 2887 words long.  There are 1176 tokens\n",
            "****** warning only considering first 512 tokens, document is 2887 words long.  There are 1141 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 39%|███▊      | 27/70 [00:01<00:02, 17.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 441 words long.  There are 537 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 70/70 [00:02<00:00, 32.32it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: Tridrepana hainana is a moth found in China that belongs to the Drepanidae family of the order Lepidoptera.</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Lucene ID</th>\n",
              "      <th>BERT-SQuAD Answer with Highlights</th>\n",
              "      <th>Answer Key</th>\n",
              "      <th>Confidence</th>\n",
              "      <th>Title/Link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Elysius pretiosa</td>\n",
              "      <td><div> pretiosa | [ [ binomial _ nomenclature | binomial name ] ] sentence _ 0 elysius pretiosa is a [ [ moth | moth ] ] of the family [ [ erebidae | erebidae ] ] [ [ species _ description | first described ] ] by [ [ peter _ jorgensen | peter jorgensen ] ] in 1935. sentence _ 1 it is found  <font color='red'>in paraguay</font> .</div></td>\n",
              "      <td>in paraguay</td>\n",
              "      <td>0.855801</td>\n",
              "      <td>Elysius pretiosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dorithia wellingana</td>\n",
              "      <td><div> wellingana | [ [ binomial _ nomenclature | binomial name ] ] sentence _ 0 dorithia wellingana is a species of [ [ moth | moth ] ] of the family [ [ tortricidae | tortricidae ] ]. sentence _ 1 it is found  <font color='red'>in guatemala</font> .</div></td>\n",
              "      <td>in guatemala</td>\n",
              "      <td>0.844346</td>\n",
              "      <td>Dorithia wellingana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dorithia paraviridana</td>\n",
              "      <td><div> paraviridana | [ [ binomial _ nomenclature | binomial name ] ] sentence _ 0 dorithia paraviridana is a species of [ [ moth | moth ] ] of the family [ [ tortricidae | tortricidae ] ]. sentence _ 1 it is found  <font color='red'>in guatemala</font> .</div></td>\n",
              "      <td>in guatemala</td>\n",
              "      <td>0.824886</td>\n",
              "      <td>Dorithia paraviridana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Scolitantides orion</td>\n",
              "      <td><div> the butterflies in the early spring (emerging in a warm room already in february according to tumma), and in the south again in august, very local, being absent from large districts, but generally not rare, particularly on chalk in stony places. in china, [  <font color='red'>[ korea | korea</font>  ] ] and amurland the species is much more generally distributed than in europe, but always confined to the (rocky) localities of the food plant.</div></td>\n",
              "      <td>[ korea | korea</td>\n",
              "      <td>0.712812</td>\n",
              "      <td>Scolitantides orion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Elysius ochrota</td>\n",
              "      <td><div>table _ 0 elysius ochrota | [ [ taxonomy _ (biology) | scientific classification ] ] [ [ elysius ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [  <font color='red'>[ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [ [ erebidae | erebidae ] ] | subfamily : [ [ arctiinae | arctiinae ] ] | genus : [ [ elysius _ (moth) | elysius ] ] | species : e. ochrota | [ [ binomial _ nomenclature | binomial name ] ] sentence _ 0 elysius ochrota</font>  is a [ [ moth | moth ] ] of the family [ [ erebidae | erebidae ] ] [ [ species _ description | first described ] ] by [ [ george _ hampson | george hampson ] ] in 1901.</div></td>\n",
              "      <td>[ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [ [ erebidae | erebidae ] ] | subfamily : [ [ arctiinae | arctiinae ] ] | genus : [ [ elysius _ (moth) | elysius ] ] | species : e. ochrota | [ [ binomial _ nomenclature | binomial name ] ] sentence _ 0 elysius ochrota</td>\n",
              "      <td>0.710083</td>\n",
              "      <td>Elysius ochrota</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Elysius lavinia</td>\n",
              "      <td><div>table _ 0 elysius lavinia | [ [ taxonomy _ (biology) | scientific classification ] ] [ [ elysius ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [  <font color='red'>[ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [ [ erebidae | erebidae ] ] | subfamily : [ [ arctiinae | arctiinae ] ] | genus : [ [ elysius _ (moth) | elysius ] ] | species : e. lavinia | [ [ binomial _ nomenclature | binomial name ] ] | [ [ synonym _ (taxonomy) | synonyms ] ] sentence _ 0 elysius lavinia</font>  is a [ [ moth | moth ] ] of the family [ [ erebidae | erebidae ] ] [ [ species _ description | first described ] ] by [ [ herbert _ druce | herbert druce ] ] in 1906.</div></td>\n",
              "      <td>[ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [ [ erebidae | erebidae ] ] | subfamily : [ [ arctiinae | arctiinae ] ] | genus : [ [ elysius _ (moth) | elysius ] ] | species : e. lavinia | [ [ binomial _ nomenclature | binomial name ] ] | [ [ synonym _ (taxonomy) | synonyms ] ] sentence _ 0 elysius lavinia</td>\n",
              "      <td>0.705732</td>\n",
              "      <td>Elysius lavinia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Elysius melanoplaga</td>\n",
              "      <td><div>table _ 0 elysius melanoplaga | [ [ taxonomy _ (biology) | scientific classification ] ] [ [ elysius ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [  <font color='red'>[ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [ [ erebidae | erebidae ] ] | subfamily : [ [ arctiinae | arctiinae ] ] | genus : [ [ elysius _ (moth) | elysius ] ] | species : e. melanoplaga | [ [ binomial _ nomenclature | binomial name ] ] | [ [ synonym _ (taxonomy) | synonyms ] ] sentence _ 0 elysius melanoplaga</font>  is a [ [ moth | moth ] ] of the family [ [ erebidae | erebidae ] ] [ [ species _ description | first described ] ] by [ [ george _ hampson | george hampson ] ] in 1901.</div></td>\n",
              "      <td>[ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [ [ erebidae | erebidae ] ] | subfamily : [ [ arctiinae | arctiinae ] ] | genus : [ [ elysius _ (moth) | elysius ] ] | species : e. melanoplaga | [ [ binomial _ nomenclature | binomial name ] ] | [ [ synonym _ (taxonomy) | synonyms ] ] sentence _ 0 elysius melanoplaga</td>\n",
              "      <td>0.695277</td>\n",
              "      <td>Elysius melanoplaga</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Elysius chimaera</td>\n",
              "      <td><div>table _ 0 elysius chimaera | [ [ taxonomy _ (biology) | scientific classification ] ] [ [ elysius ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [ [ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [  <font color='red'>[ erebidae | erebidae ] ]</font> </div></td>\n",
              "      <td>[ erebidae | erebidae ] ]</td>\n",
              "      <td>0.658757</td>\n",
              "      <td>Elysius chimaera</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Elysius hades</td>\n",
              "      <td><div>table _ 0 elysius hades | [ [ taxonomy _ (biology) | scientific classification ] ] [ [ elysius ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [ [ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [  <font color='red'>[ erebidae | erebidae ] ]</font> </div></td>\n",
              "      <td>[ erebidae | erebidae ] ]</td>\n",
              "      <td>0.606301</td>\n",
              "      <td>Elysius hades</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Elysius ordinaria</td>\n",
              "      <td><div>table _ 0 elysius ordinaria | [ [ taxonomy _ (biology) | scientific classification ] ] [ [ elysius ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [ [ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [  <font color='red'>[ erebidae | erebidae ] ]</font> </div></td>\n",
              "      <td>[ erebidae | erebidae ] ]</td>\n",
              "      <td>0.603284</td>\n",
              "      <td>Elysius ordinaria</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Atrocenta</td>\n",
              "      <td><div>table _ 0 atrocenta | [ [ taxonomy _ (biology) | scientific classification ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [ [ lepidoptera | lepidoptera ] ] | family : [ [ tortricidae | tortricidae ] ] | subfamily : [  <font color='red'>[ tortricinae | tortricinae ] ]</font> </div></td>\n",
              "      <td>[ tortricinae | tortricinae ] ]</td>\n",
              "      <td>0.600939</td>\n",
              "      <td>Atrocenta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Rubroxena</td>\n",
              "      <td><div>table _ 0 rubroxena | [ [ taxonomy _ (biology) | scientific classification ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [ [ lepidoptera | lepidoptera ] ] | family : [ [ tortricidae | tortricidae ] ] | subfamily : [  <font color='red'>[ tortricinae | tortricinae</font> </div></td>\n",
              "      <td>[ tortricinae | tortricinae</td>\n",
              "      <td>0.600142</td>\n",
              "      <td>Rubroxena</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Gnathocolumna</td>\n",
              "      <td><div>table _ 0 gnathocolumna | [ [ taxonomy _ (biology) | scientific classification ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [ [ lepidoptera | lepidoptera ] ] | family : [ [ tortricidae | tortricidae ] ] | subfamily : [  <font color='red'>[ tortricinae | tortricinae ] ]</font> </div></td>\n",
              "      <td>[ tortricinae | tortricinae ] ]</td>\n",
              "      <td>0.599937</td>\n",
              "      <td>Gnathocolumna</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Elysius conspersus</td>\n",
              "      <td><div>table _ 0 elysius conspersus | [ [ taxonomy _ (biology) | scientific classification ] ] [ [ elysius ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [ [ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [  <font color='red'>[ erebidae | erebidae ] ]</font> </div></td>\n",
              "      <td>[ erebidae | erebidae ] ]</td>\n",
              "      <td>0.599431</td>\n",
              "      <td>Elysius conspersus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Elysius disciplaga</td>\n",
              "      <td><div>table _ 0 elysius disciplaga | [ [ taxonomy _ (biology) | scientific classification ] ] [ [ elysius ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [ [ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [  <font color='red'>[ erebidae | erebidae ] ]</font> </div></td>\n",
              "      <td>[ erebidae | erebidae ] ]</td>\n",
              "      <td>0.597162</td>\n",
              "      <td>Elysius disciplaga</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Elysius deceptura</td>\n",
              "      <td><div>table _ 0 elysius deceptura | [ [ taxonomy _ (biology) | scientific classification ] ] [ [ elysius ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [ [ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [  <font color='red'>[ erebidae | erebidae ] ]</font> </div></td>\n",
              "      <td>[ erebidae | erebidae ] ]</td>\n",
              "      <td>0.596748</td>\n",
              "      <td>Elysius deceptura</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Elysius atrata</td>\n",
              "      <td><div>table _ 0 elysius atrata | [ [ taxonomy _ (biology) | scientific classification ] ] [ [ elysius ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [ [ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [  <font color='red'>[ erebidae | erebidae ] ]</font> </div></td>\n",
              "      <td>[ erebidae | erebidae ] ]</td>\n",
              "      <td>0.595051</td>\n",
              "      <td>Elysius atrata</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Elysius subterra</td>\n",
              "      <td><div>table _ 0 elysius subterra | [ [ taxonomy _ (biology) | scientific classification ] ] [ [ elysius ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [ [ lepidoptera | lepidoptera ] ] | superfamily : [ [ noctuoidea | noctuoidea ] ] | family : [  <font color='red'>[ erebidae | erebidae</font> </div></td>\n",
              "      <td>[ erebidae | erebidae</td>\n",
              "      <td>0.587395</td>\n",
              "      <td>Elysius subterra</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Furcinetechma</td>\n",
              "      <td><div>table _  <font color='red'>0 furcinetechma</font>  | [ [ taxonomy _ (biology) | scientific classification ] ] | kingdom : [ [ animal | animalia ] ] | phylum : [ [ arthropod | arthropoda ] ] | class : [ [ insect | insecta ] ] | order : [ [ lepidoptera | lepidoptera ] ] | family : [ [ tortricidae | tortricidae ] ] | subfamily : [ [ tortricinae | tortricinae ] ] | genus : furcinetechma razowski & wojtusiak, 2008 sentence _ </div></td>\n",
              "      <td>0 furcinetechma</td>\n",
              "      <td>0.565542</td>\n",
              "      <td>Furcinetechma</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Ernocornutina gambra</td>\n",
              "      <td><div>table _  <font color='red'>0 ernocornutina gambra</font> </div></td>\n",
              "      <td>0 ernocornutina gambra</td>\n",
              "      <td>0.115415</td>\n",
              "      <td>Ernocornutina gambra</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Exoletuncus exoristus</td>\n",
              "      <td><div>table _  <font color='red'>0 exoletuncus exoristus</font> </div></td>\n",
              "      <td>0 exoletuncus exoristus</td>\n",
              "      <td>0.110599</td>\n",
              "      <td>Exoletuncus exoristus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Apolychrosis ambogonium</td>\n",
              "      <td><div>table _  <font color='red'>0 apolychrosis ambogonium</font> </div></td>\n",
              "      <td>0 apolychrosis ambogonium</td>\n",
              "      <td>0.110303</td>\n",
              "      <td>Apolychrosis ambogonium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Eriotortrix isipida</td>\n",
              "      <td><div>table _  <font color='red'>0 eriotortrix isipida</font> </div></td>\n",
              "      <td>0 eriotortrix isipida</td>\n",
              "      <td>0.088349</td>\n",
              "      <td>Eriotortrix isipida</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}