# -*- coding: utf-8 -*-
"""FeverAnserini.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cmkchl6MKnJddDr1jrpWN5RvvrB2qyie
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !apt-get install maven -qq

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
plt.style.use('ggplot')
import os
import json
import glob
import sys

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !git clone --recurse-submodules https://github.com/castorini/anserini.git
# %cd anserini
# !cd tools/eval && tar xvfz trec_eval.9.0.4.tar.gz && cd trec_eval.9.0.4 && make && cd ../../..
# !mvn clean package appassembler:assemble -DskipTests -Dmaven.javadoc.skip=true

#import the necessary data (i am not sure this is the latest data)
!mkdir collections/fever
!mkdir indexes/fever

!wget https://s3-eu-west-1.amazonaws.com/fever.public/feverous/feverous-wiki-pages.zip -P collections/fever
!unzip collections/fever/feverous-wiki-pages.zip -d collections/fever

!wget https://s3-eu-west-1.amazonaws.com/fever.public/feverous/train.jsonl -P collections/fever
!wget https://s3-eu-west-1.amazonaws.com/fever.public/feverous/dev.jsonl -P collections/fever

!ls collections/fever/FeverousWikiv1

# A folder path, a file path, or a list of file paths is needed

# DATA_LOCATION = "wiki_pages"
# DATA_LOCATION = "wiki_pages/wiki_000.jsonl"
DATA_LOCATION = [
    "collections/fever/FeverousWikiv1/wiki_000.jsonl",
    "collections/fever/FeverousWikiv1/wiki_001.jsonl",
    "collections/fever/FeverousWikiv1/wiki_002.jsonl",
    "collections/fever/FeverousWikiv1/wiki_003.jsonl",
    "collections/fever/FeverousWikiv1/wiki_004.jsonl",
]
OUTPUT_FOLDER = "wiki_pages_anserini"

SIMPLIFY_PAGE_LINKS = False
PREFER_URL_FOR_LINKS = False

# If SIMPLIFY_PAGE_LINKS and not PREFER_URL_FOR_LINKS: [[Page_(disambiguation)|Page Title]] -> Page Title
# If SIMPLIFY_PAGE_LINKS and PREFER_URL_FOR_LINKS: [[Page_(disambiguation)|Page Title]] -> Page (disambiguation)
# Note that the URL basename can sometimes contain extra information, but it can also redirect to a page that is different than the visible label.

SUBSECTION_SEP = "|"
SUBSUBSECTION_SEP = " "
import glob
import os

to_process = None

if type(DATA_LOCATION) == list:  # A list of file paths
    to_process = [file for file in DATA_LOCATION]
elif os.path.isfile(DATA_LOCATION):  # A file path
    to_process = [DATA_LOCATION]
elif os.path.isdir(DATA_LOCATION):  # A folder
    to_process = glob.glob("{}/*.jsonl".format(DATA_LOCATION))
else:
    raise ValueError("Data location is not a valid file or folder.")

print("{} file(s) will be processed.".format(len(to_process)))

import json
import re


def clean_value(value):
    """Replaces unnecessary space (including \t and such) with a single space
    character.

    Arg:
        value: A string.

    Returns:
        The simplified version of the provided string.
    """
    return " ".join(value.split())


def final_clean(
    field,
    simplify_page_links=SIMPLIFY_PAGE_LINKS,
    prefer_url_for_links=PREFER_URL_FOR_LINKS,
):
    """Removes multiple space characters with a single space character. Removes
    leading and trailing spaces. Simplifies the page links according to the
    parameters.

    Args:
        field (str): A field of the processed page data.
        simplify_page_links (Boolean): A Boolean indicating whether the page
            links must be simplified. By default, it is set to
            SIMPLIFY_PAGE_LINKS.
        prefer_url_for_links (Boolean): A Boolean indicating whether the page
            links must be simplified to the URL basename instead of the link's
            label. By default, it is set to SIMPLIFY_PAGE_LINKS.

    Returns:
        str: The simplified version of the provided string.
    """
    field = re.sub(" +", " ", field.strip(), flags=re.MULTILINE)
    if SIMPLIFY_PAGE_LINKS and PREFER_URL_FOR_LINKS:
        # Simplifies links by replacing them with the URL basename (replaces underscores)
        # Underscores are replaced with space
        field = re.sub(r"(?:\[\[)(.*)?(?:\|)", field.replace("_", " "), field)
        field = re.sub(r"(\|).*?(\]\])|\[\[", "", field, flags=re.MULTILINE)
    elif SIMPLIFY_PAGE_LINKS:
        # Simplifies links by replacing them with the titles
        field = re.sub(r"(\[\[).*?(\|)|]]", "", field, flags=re.MULTILINE)
    return field


if not os.path.exists(OUTPUT_FOLDER):  # Creates the output folder if it does not exist
    os.mkdir(OUTPUT_FOLDER)

counter = 0
for file_path in to_process:
    file_basename = os.path.basename(file_path)
    output_path = os.path.join(OUTPUT_FOLDER, file_basename)

    open(output_path, mode="w").close()  # Creating an empty output file

    with open(file_path, "r", encoding="utf-8") as f:
        file_data = [json.loads(line) for line in f]

    for page in file_data:

        page_data_processed = {"id": page["title"], "text": "", "lines": ""}

        # The order list of the page will be used to parse the items
        for item_id, item in enumerate(page["order"]):
            if item.startswith("sentence_"):  # Sentence
                clean_element = clean_value(page[item])
                page_data_processed["text"] += " {}".format(clean_element)
                page_data_processed["lines"] += "{}\t{}\n".format(item, clean_element)
            elif item.startswith("table_"):  # Table
                page_data_processed["text"] += " "
                page_data_processed["lines"] += "{}\t".format(item)
                table_rows = []
                for row in page[item]["table"]:
                    row_items = []
                    for cell in row:
                        row_items.append(clean_value(cell["value"]))

                    row_text = "{}".format(SUBSUBSECTION_SEP).join(row_items)
                    table_rows.append(row_text)

                if "caption" in page[item]:
                    table_rows.append(clean_value(page[item]["caption"]))

                table_text = "{}".format(SUBSECTION_SEP).join(table_rows)
                page_data_processed["text"] += table_text
                page_data_processed["lines"] += table_text
                page_data_processed["lines"] += "\n"
            elif item.startswith("section_"):  # Section
                clean_element = clean_value(page[item]["value"])
                page_data_processed["text"] += " {}".format(clean_element)
                page_data_processed["lines"] += "{}\t{}\n".format(item, clean_element)
            elif item.startswith("list_"):  # List
                list_items = []
                for list_item in page[item]["list"]:
                    list_items.append(clean_value(list_item["value"]))

                list_text = "{}".format(SUBSECTION_SEP).join(list_items)
                page_data_processed["text"] += list_text
                page_data_processed["lines"] += "{}\t{}\n".format(item, list_text)
            else:  # All alternatives must be handled and the code must not reach here
                raise ValueError("Unidentified page element found.")

        page_data_processed["text"] = final_clean(page_data_processed["text"])
        page_data_processed["lines"] = final_clean(page_data_processed["lines"])

        # Appending the processed page data to the file
        with open(output_path, "a", encoding="utf-8") as f:
            json.dump(page_data_processed, f, ensure_ascii=False)
            f.write("\n")

!sh target/appassembler/bin/IndexCollection -collection FeverParagraphCollection \
 -input wiki_pages_anserini \
 -index indexes/fever/lucene-index-fever-paragraph \
 -generator DefaultLuceneDocumentGenerator \
 -threads 1 -storePositions -storeDocvectors -storeRaw \
  >& logs/log.fever &

!tar -cvf "/content/anserini/indexes/fever/lucene-index-fever-paragraph.tar" "/content/anserini/indexes/fever/lucene-index-fever-paragraph" #Tar operation for exporting the index and make it reusable
# Only one indexing will be enough we will use the tar file after that use the code below to untar it and equate to the LucuneDir
#!tar xvfz lucene-index-fever-paragraph.tar

luceneDir = 'indexes/fever/lucene-index-fever-paragraph'

USE_SUMMARY = False

import os
#%%capture
!curl -O https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz
!mv openjdk-11.0.2_linux-x64_bin.tar.gz /usr/lib/jvm/; cd /usr/lib/jvm/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz
!update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-11.0.2/bin/java 1
!update-alternatives --set java /usr/lib/jvm/jdk-11.0.2/bin/java
os.environ["JAVA_HOME"] = "/usr/lib/jvm/jdk-11.0.2"

import tensorflow as tf
import tensorflow_hub as hub
!mkdir ~/kaggle
!mkdir ~/kaggle/working/
!mkdir ~/kaggle/working/sentence_wise_email/
!mkdir ~/kaggle/working/sentence_wise_email/module/
!mkdir ~/kaggle/working/sentence_wise_email/module/module_useT
# Download the module, and uncompress it to the destination folder. 
!curl -L "https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed" | tar -zxvC ~/kaggle/working/sentence_wise_email/module/module_useT

import torch
!pip install transformers
!pip install --no-cache-dir transformers sentencepiece
#from transformers import BertForQuestionAnswering
#from transformers import BertTokenizer
from transformers import BartTokenizer, BartForConditionalGeneration
from transformers import AutoModelForQuestionAnswering, AutoTokenizer

torch_device = 'cuda'

#dmis-lab/biobert-base-cased-v1.1-squad
#abhijithneilabraham/longformer_covid_qa
#graviraja/covidbert_squad 

QA_TOKENIZER = AutoTokenizer.from_pretrained("dmis-lab/biobert-base-cased-v1.1-squad")
QA_MODEL = AutoModelForQuestionAnswering.from_pretrained("dmis-lab/biobert-base-cased-v1.1-squad")

#QA_MODEL = AutoModelForQuestionAnswering.from_pretrained("NeuML/bert-small-cord19qa")
#QA_TOKENIZER = AutoTokenizer.from_pretrained("NeuML/bert-small-cord19qa")

#QA_MODEL = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
#QA_TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')
QA_MODEL.to(torch_device)
QA_MODEL.eval()
#Auto Summarization with BART
if USE_SUMMARY:
    SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
    SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')

    SUMMARY_MODEL.to(torch_device)
    SUMMARY_MODEL.eval()

#%%capture
!pip install pyserini==0.8.1.0
from pyserini.search import pysearch

query_processed = 'As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?'
query = 'As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?'

keywords = '2019-nCoV, SARS-CoV-2, COVID-19, symptoms, hospitalization'

import json

searcher = pysearch.SimpleSearcher(luceneDir)
hits = searcher.search(query_processed + '. ' + keywords,k=50) # We can retrieve the documents according to concatanated string of query and keywords we can change this setup.
n_hits = len(hits)
## collect the relevant data in a hit dictionary
hit_dictionary = {}

import json
#'As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?'

searcher = pysearch.SimpleSearcher(luceneDir)
hits = searcher.search(query_processed + '. ' + keywords,k=50)
n_hits = len(hits)
## collect the relevant data in a hit dictionary
hit_dictionary = {}
for i in range(0, n_hits):
  if 1==1:
    doc_json=dict()
    idx = str(hits[i].docid)
    hit_dictionary[idx] = doc_json
    hit_dictionary[idx]['abstract']=hits[i].lucene_document.get("raw")
    hit_dictionary[idx]['real_abstract']=hits[i].lucene_document.get("raw")
    hit_dictionary[idx]['title'] = str(hits[i].docid)

## scrub the abstracts in prep for BERT-SQuAD
for idx,v in hit_dictionary.items():
    abs_dirty = v['abstract']
    real_abs_dirty = v['real_abstract']
    #abs_dirty = v['paragraph']
    # looks like the abstract value can be an empty list
    v['abstract_paragraphs'] = []
    v['abstract_full'] = ''
    v['real_abstract_full'] = ''
    v['real_abstract_paragraphs']=[]

    if abs_dirty:
        # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key
        # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph
        # and a new entry that is full abstract text as both could be valuable for BERT derrived QA


        if isinstance(abs_dirty, list):
            for p in abs_dirty:
                v['abstract_paragraphs'].append(p['text'])
                v['abstract_full'] += p['text'] + ' \n\n'

        # looks like in some cases the abstract can be straight up text so we can actually leave that alone
        if isinstance(abs_dirty, str):
            v['abstract_paragraphs'].append(abs_dirty)
            v['abstract_full'] += abs_dirty + ' \n\n'
    if real_abs_dirty:
        # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key
        # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph
        # and a new entry that is full abstract text as both could be valuable for BERT derrived QA


        if isinstance(real_abs_dirty, list):
            for p in real_abs_dirty:
                v['real_abstract_paragraphs'].append(p['text'])
                v['real_abstract_full'] += p['text'] + ' \n\n'

        # looks like in some cases the abstract can be straight up text so we can actually leave that alone
        if isinstance(abs_dirty, str):
            v['real_abstract_paragraphs'].append(real_abs_dirty)
            v['real_abstract_full'] += real_abs_dirty + ' \n\n'
    v["indexed_para"]=v["abstract_full"][len(v["real_abstract_full"]):]

def embed_useT(module):
    with tf.Graph().as_default():
        sentences = tf.compat.v1.placeholder(tf.string)
        embed = hub.Module(module)
        embeddings = embed(sentences)
        session = tf.compat.v1.train.MonitoredSession()
    return lambda x: session.run(embeddings, {sentences: x})
embed_fn = embed_useT('/root/kaggle/working/sentence_wise_email/module/module_useT')

import numpy as np
#Reconstruct text for generating the answer text from the BERT result, do the necessary text preprocessing and construct the answer.
def reconstructText(tokens, start=0, stop=-1):
    tokens = tokens[start: stop]
    if '[SEP]' in tokens:
        sepind = tokens.index('[SEP]')
        tokens = tokens[sepind+1:]
    txt = ' '.join(tokens)
    txt = txt.replace(' ##', '')
    txt = txt.replace('##', '')
    txt = txt.strip()
    txt = " ".join(txt.split())
    txt = txt.replace(' .', '.')
    txt = txt.replace('( ', '(')
    txt = txt.replace(' )', ')')
    txt = txt.replace(' - ', '-')
    txt_list = txt.split(' , ')
    txt = ''
    nTxtL = len(txt_list)
    if nTxtL == 1:
        return txt_list[0]
    newList =[]
    for i,t in enumerate(txt_list):
        if i < nTxtL -1:
            if t[-1].isdigit() and txt_list[i+1][0].isdigit():
                newList += [t,',']
            else:
                newList += [t, ', ']
        else:
            newList += [t]
    return ''.join(newList)


def makeBERTSQuADPrediction(document, question):
    ## we need to rewrite this function so that it chuncks the document into 250-300 word segments with
    ## 50 word overlaps on either end so that it can understand and check longer abstracts
    ## Get chuck of documents in order to overcome the BERT's max 512 token problem. First count the number of tokens. For example if it is
    ##   larger than 2048 token determine the split size and overlapping token count. Split the document into pieces to fed into BERT encoding. Overlapping
    ##      is necessary to maintain the coherence between the splits. If the token count is larger than some number 2560 token approximately this will fail.
    nWords = len(document.split())
    input_ids_all = QA_TOKENIZER.encode(question, document)
    tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)
    overlapFac = 1.1
    if len(input_ids_all)*overlapFac > 7148:
        nSearchWords = int(np.ceil(nWords/15))
        fourteenth = int(np.ceil(nWords/14))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), 
                     ' '.join(docSplit[fourteenth-int(nSearchWords*overlapFac/2):fourteenth+int(fourteenth*overlapFac/2)]),
                     ' '.join(docSplit[fourteenth*2-int(nSearchWords*overlapFac/2):fourteenth*2+int(fourteenth*overlapFac/2)]),
                     ' '.join(docSplit[fourteenth*3-int(nSearchWords*overlapFac/2):fourteenth*3+int(fourteenth*overlapFac/2)]),
                     ' '.join(docSplit[fourteenth*4-int(nSearchWords*overlapFac/2):fourteenth*4+int(fourteenth*overlapFac/2)]),
                     ' '.join(docSplit[fourteenth*5-int(nSearchWords*overlapFac/2):fourteenth*5+int(fourteenth*overlapFac/2)]),
                     ' '.join(docSplit[fourteenth*6-int(nSearchWords*overlapFac/2):fourteenth*6+int(fourteenth*overlapFac/2)]),
                     ' '.join(docSplit[fourteenth*7-int(nSearchWords*overlapFac/2):fourteenth*7+int(fourteenth*overlapFac/2)]),
                     ' '.join(docSplit[fourteenth*8-int(nSearchWords*overlapFac/2):fourteenth*8+int(fourteenth*overlapFac/2)]),
                     ' '.join(docSplit[fourteenth*9-int(nSearchWords*overlapFac/2):fourteenth*9+int(fourteenth*overlapFac/2)]),
                     ' '.join(docSplit[fourteenth*10-int(nSearchWords*overlapFac/2):fourteenth*10+int(fourteenth*overlapFac/2)]),
                     ' '.join(docSplit[fourteenth*11-int(nSearchWords*overlapFac/2):fourteenth*11+int(fourteenth*overlapFac/2)]),
                     ' '.join(docSplit[fourteenth*12-int(nSearchWords*overlapFac/2):fourteenth*12+int(fourteenth*overlapFac/2)]),
                     ' '.join(docSplit[fourteenth*13-int(nSearchWords*overlapFac/2):fourteenth*13+int(fourteenth*overlapFac/2)]),
                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]
    elif len(input_ids_all)*overlapFac > 6646:
        nSearchWords = int(np.ceil(nWords/14))
        thirteenth = int(np.ceil(nWords/13))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), 
                     ' '.join(docSplit[thirteenth-int(nSearchWords*overlapFac/2):thirteenth+int(thirteenth*overlapFac/2)]),
                     ' '.join(docSplit[thirteenth*2-int(nSearchWords*overlapFac/2):thirteenth*2+int(thirteenth*overlapFac/2)]),
                     ' '.join(docSplit[thirteenth*3-int(nSearchWords*overlapFac/2):thirteenth*3+int(thirteenth*overlapFac/2)]),
                     ' '.join(docSplit[thirteenth*4-int(nSearchWords*overlapFac/2):thirteenth*4+int(thirteenth*overlapFac/2)]),
                     ' '.join(docSplit[thirteenth*5-int(nSearchWords*overlapFac/2):thirteenth*5+int(thirteenth*overlapFac/2)]),
                     ' '.join(docSplit[thirteenth*6-int(nSearchWords*overlapFac/2):thirteenth*6+int(thirteenth*overlapFac/2)]),
                     ' '.join(docSplit[thirteenth*7-int(nSearchWords*overlapFac/2):thirteenth*7+int(thirteenth*overlapFac/2)]),
                     ' '.join(docSplit[thirteenth*8-int(nSearchWords*overlapFac/2):thirteenth*8+int(thirteenth*overlapFac/2)]),
                     ' '.join(docSplit[thirteenth*9-int(nSearchWords*overlapFac/2):thirteenth*9+int(thirteenth*overlapFac/2)]),
                     ' '.join(docSplit[thirteenth*10-int(nSearchWords*overlapFac/2):thirteenth*10+int(thirteenth*overlapFac/2)]),
                     ' '.join(docSplit[thirteenth*11-int(nSearchWords*overlapFac/2):thirteenth*11+int(thirteenth*overlapFac/2)]),
                     ' '.join(docSplit[thirteenth*12-int(nSearchWords*overlapFac/2):thirteenth*12+int(thirteenth*overlapFac/2)]),
                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]
    elif len(input_ids_all)*overlapFac > 6134:
        nSearchWords = int(np.ceil(nWords/13))
        twelveth = int(np.ceil(nWords/12))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), 
                     ' '.join(docSplit[twelveth-int(nSearchWords*overlapFac/2):twelveth+int(twelveth*overlapFac/2)]),
                     ' '.join(docSplit[twelveth*2-int(nSearchWords*overlapFac/2):twelveth*2+int(twelveth*overlapFac/2)]),
                     ' '.join(docSplit[twelveth*3-int(nSearchWords*overlapFac/2):twelveth*3+int(twelveth*overlapFac/2)]),
                     ' '.join(docSplit[twelveth*4-int(nSearchWords*overlapFac/2):twelveth*4+int(twelveth*overlapFac/2)]),
                     ' '.join(docSplit[twelveth*5-int(nSearchWords*overlapFac/2):twelveth*5+int(twelveth*overlapFac/2)]),
                     ' '.join(docSplit[twelveth*6-int(nSearchWords*overlapFac/2):twelveth*6+int(twelveth*overlapFac/2)]),
                     ' '.join(docSplit[twelveth*7-int(nSearchWords*overlapFac/2):twelveth*7+int(twelveth*overlapFac/2)]),
                     ' '.join(docSplit[twelveth*8-int(nSearchWords*overlapFac/2):twelveth*8+int(twelveth*overlapFac/2)]),
                     ' '.join(docSplit[twelveth*9-int(nSearchWords*overlapFac/2):twelveth*9+int(twelveth*overlapFac/2)]),
                     ' '.join(docSplit[twelveth*10-int(nSearchWords*overlapFac/2):twelveth*10+int(twelveth*overlapFac/2)]),
                     ' '.join(docSplit[twelveth*11-int(nSearchWords*overlapFac/2):twelveth*11+int(twelveth*overlapFac/2)]),
                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]
    elif len(input_ids_all)*overlapFac > 5632:
        nSearchWords = int(np.ceil(nWords/12))
        eleventh = int(np.ceil(nWords/11))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), 
                     ' '.join(docSplit[eleventh-int(nSearchWords*overlapFac/2):eleventh+int(eleventh*overlapFac/2)]),
                     ' '.join(docSplit[eleventh*2-int(nSearchWords*overlapFac/2):eleventh*2+int(eleventh*overlapFac/2)]),
                     ' '.join(docSplit[eleventh*3-int(nSearchWords*overlapFac/2):eleventh*3+int(eleventh*overlapFac/2)]),
                     ' '.join(docSplit[eleventh*4-int(nSearchWords*overlapFac/2):eleventh*4+int(eleventh*overlapFac/2)]),
                     ' '.join(docSplit[eleventh*5-int(nSearchWords*overlapFac/2):eleventh*5+int(eleventh*overlapFac/2)]),
                     ' '.join(docSplit[eleventh*6-int(nSearchWords*overlapFac/2):eleventh*6+int(eleventh*overlapFac/2)]),
                     ' '.join(docSplit[eleventh*7-int(nSearchWords*overlapFac/2):eleventh*7+int(eleventh*overlapFac/2)]),
                     ' '.join(docSplit[eleventh*8-int(nSearchWords*overlapFac/2):eleventh*8+int(eleventh*overlapFac/2)]),
                     ' '.join(docSplit[eleventh*9-int(nSearchWords*overlapFac/2):eleventh*9+int(eleventh*overlapFac/2)]),
                     ' '.join(docSplit[eleventh*10-int(nSearchWords*overlapFac/2):eleventh*10+int(eleventh*overlapFac/2)]),
                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]
    elif len(input_ids_all)*overlapFac > 5120:
        nSearchWords = int(np.ceil(nWords/11))
        tenth = int(np.ceil(nWords/10))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), 
                     ' '.join(docSplit[tenth-int(nSearchWords*overlapFac/2):tenth+int(tenth*overlapFac/2)]),
                     ' '.join(docSplit[tenth*2-int(nSearchWords*overlapFac/2):tenth*2+int(tenth*overlapFac/2)]),
                     ' '.join(docSplit[tenth*3-int(nSearchWords*overlapFac/2):tenth*3+int(tenth*overlapFac/2)]),
                     ' '.join(docSplit[tenth*4-int(nSearchWords*overlapFac/2):tenth*4+int(tenth*overlapFac/2)]),
                     ' '.join(docSplit[tenth*5-int(nSearchWords*overlapFac/2):tenth*5+int(tenth*overlapFac/2)]),
                     ' '.join(docSplit[tenth*6-int(nSearchWords*overlapFac/2):tenth*6+int(tenth*overlapFac/2)]),
                     ' '.join(docSplit[tenth*7-int(nSearchWords*overlapFac/2):tenth*7+int(tenth*overlapFac/2)]),
                     ' '.join(docSplit[tenth*8-int(nSearchWords*overlapFac/2):tenth*8+int(tenth*overlapFac/2)]),
                     ' '.join(docSplit[tenth*9-int(nSearchWords*overlapFac/2):tenth*9+int(tenth*overlapFac/2)]),
                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]
    elif len(input_ids_all)*overlapFac > 4608:
        nSearchWords = int(np.ceil(nWords/10))
        nineth = int(np.ceil(nWords/9))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), 
                     ' '.join(docSplit[nineth-int(nSearchWords*overlapFac/2):nineth+int(nineth*overlapFac/2)]),
                     ' '.join(docSplit[nineth*2-int(nSearchWords*overlapFac/2):nineth*2+int(nineth*overlapFac/2)]),
                     ' '.join(docSplit[nineth*3-int(nSearchWords*overlapFac/2):nineth*3+int(nineth*overlapFac/2)]),
                     ' '.join(docSplit[nineth*4-int(nSearchWords*overlapFac/2):nineth*4+int(nineth*overlapFac/2)]),
                     ' '.join(docSplit[nineth*5-int(nSearchWords*overlapFac/2):nineth*5+int(nineth*overlapFac/2)]),
                     ' '.join(docSplit[nineth*6-int(nSearchWords*overlapFac/2):nineth*6+int(nineth*overlapFac/2)]),
                     ' '.join(docSplit[nineth*7-int(nSearchWords*overlapFac/2):nineth*7+int(nineth*overlapFac/2)]),
                     ' '.join(docSplit[nineth*8-int(nSearchWords*overlapFac/2):nineth*8+int(nineth*overlapFac/2)]),
                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]
    elif len(input_ids_all)*overlapFac > 4096:
        nSearchWords = int(np.ceil(nWords/9))
        eighth = int(np.ceil(nWords/8))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), 
                     ' '.join(docSplit[eighth-int(nSearchWords*overlapFac/2):eighth+int(eighth*overlapFac/2)]),
                     ' '.join(docSplit[eighth*2-int(nSearchWords*overlapFac/2):eighth*2+int(eighth*overlapFac/2)]),
                     ' '.join(docSplit[eighth*3-int(nSearchWords*overlapFac/2):eighth*3+int(eighth*overlapFac/2)]),
                     ' '.join(docSplit[eighth*4-int(nSearchWords*overlapFac/2):eighth*4+int(eighth*overlapFac/2)]),
                     ' '.join(docSplit[eighth*5-int(nSearchWords*overlapFac/2):eighth*5+int(eighth*overlapFac/2)]),
                     ' '.join(docSplit[eighth*6-int(nSearchWords*overlapFac/2):eighth*6+int(eighth*overlapFac/2)]),
                     ' '.join(docSplit[eighth*7-int(nSearchWords*overlapFac/2):eighth*7+int(eighth*overlapFac/2)]),
                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]
    elif len(input_ids_all)*overlapFac > 3584:
        nSearchWords = int(np.ceil(nWords/8))
        seventh = int(np.ceil(nWords/7))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), 
                     ' '.join(docSplit[seventh-int(nSearchWords*overlapFac/2):seventh+int(seventh*overlapFac/2)]),
                     ' '.join(docSplit[seventh*2-int(nSearchWords*overlapFac/2):seventh*2+int(seventh*overlapFac/2)]),
                     ' '.join(docSplit[seventh*3-int(nSearchWords*overlapFac/2):seventh*3+int(seventh*overlapFac/2)]),
                     ' '.join(docSplit[seventh*4-int(nSearchWords*overlapFac/2):seventh*4+int(seventh*overlapFac/2)]),
                     ' '.join(docSplit[seventh*5-int(nSearchWords*overlapFac/2):seventh*5+int(seventh*overlapFac/2)]),
                     ' '.join(docSplit[seventh*6-int(nSearchWords*overlapFac/2):seventh*6+int(seventh*overlapFac/2)]),
                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]
    elif len(input_ids_all)*overlapFac > 3072:
        nSearchWords = int(np.ceil(nWords/7))
        sixth = int(np.ceil(nWords/6))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), 
                     ' '.join(docSplit[sixth-int(nSearchWords*overlapFac/2):sixth+int(sixth*overlapFac/2)]),
                     ' '.join(docSplit[sixth*2-int(nSearchWords*overlapFac/2):sixth*2+int(sixth*overlapFac/2)]),
                     ' '.join(docSplit[sixth*3-int(nSearchWords*overlapFac/2):sixth*3+int(sixth*overlapFac/2)]),
                     ' '.join(docSplit[sixth*4-int(nSearchWords*overlapFac/2):sixth*4+int(sixth*overlapFac/2)]),
                     ' '.join(docSplit[sixth*5-int(nSearchWords*overlapFac/2):sixth*5+int(sixth*overlapFac/2)]),
                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]

    elif len(input_ids_all)*overlapFac > 2560:
        nSearchWords = int(np.ceil(nWords/6))
        fifth = int(np.ceil(nWords/5))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), 
                     ' '.join(docSplit[fifth-int(nSearchWords*overlapFac/2):fifth+int(fifth*overlapFac/2)]),
                     ' '.join(docSplit[fifth*2-int(nSearchWords*overlapFac/2):fifth*2+int(fifth*overlapFac/2)]),
                     ' '.join(docSplit[fifth*3-int(nSearchWords*overlapFac/2):fifth*3+int(fifth*overlapFac/2)]),
                     ' '.join(docSplit[fifth*4-int(nSearchWords*overlapFac/2):fifth*4+int(fifth*overlapFac/2)]),
                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]

    elif len(input_ids_all)*overlapFac > 2048:
        nSearchWords = int(np.ceil(nWords/5))
        quarter = int(np.ceil(nWords/4))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), 
                     ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),
                     ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),
                     ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),
                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        
        
    elif len(input_ids_all)*overlapFac > 1536:
        nSearchWords = int(np.ceil(nWords/4))
        third = int(np.ceil(nWords/3))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), 
                     ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),
                     ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),
                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        
        
    elif len(input_ids_all)*overlapFac > 1024:
        nSearchWords = int(np.ceil(nWords/3))
        middle = int(np.ceil(nWords/2))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), 
                     ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),
                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]
    elif len(input_ids_all)*overlapFac > 512:
        nSearchWords = int(np.ceil(nWords/2))
        docSplit = document.split()
        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]
        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]
    else:
        input_ids = [input_ids_all]
    absTooLong = False    
    
    answers = []
    cons = []
    #print(input_ids)
    for iptIds in input_ids:
        tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)
        #print(tokens)
        sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)
        num_seg_a = sep_index + 1
        num_seg_b = len(iptIds) - num_seg_a
        segment_ids = [0]*num_seg_a + [1]*num_seg_b
        assert len(segment_ids) == len(iptIds)
        n_ids = len(segment_ids)
        #print(n_ids)
        if n_ids < 512:
            outputs = QA_MODEL(torch.tensor([iptIds]).to(torch_device), 
                                     token_type_ids=torch.tensor([segment_ids]).to(torch_device))
            #print(QA_MODEL(torch.tensor([iptIds]).to(torch_device), 
             #                        token_type_ids=torch.tensor([segment_ids]).to(torch_device)))
        else:
            #this cuts off the text if its more than 512 words so it fits in model space
            #need run multiple inferences for longer text. add to the todo. I will try to expand this to longer sequences if it is necessary. 
            print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')
            absTooLong = True
            outputs= QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device), 
                                     token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))
        start_scores=outputs.start_logits
        end_scores=outputs.end_logits
        start_scores = start_scores[:,1:-1]
        end_scores = end_scores[:,1:-1]
        answer_start = torch.argmax(start_scores)
        answer_end = torch.argmax(end_scores)
        #print(answer_start, answer_end)
        answer = reconstructText(tokens, answer_start, answer_end+2)
    
        if answer.startswith('. ') or answer.startswith(', '):
            answer = answer[2:]
            
        c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()
        answers.append(answer)
        cons.append(c)
    
    maxC = max(cons)
    iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]
    confidence = cons[iMaxC]
    answer = answers[iMaxC]
    
    sep_index = tokens_all.index('[SEP]')
    full_txt_tokens = tokens_all[sep_index+1:]
    
    abs_returned = reconstructText(full_txt_tokens)

    ans={}
    ans['answer'] = answer
    #print(answer)
    if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):
        ans['confidence'] = -1000000
    else:
        #confidence = torch.max(start_scores) + torch.max(end_scores)
        #confidence = np.log(confidence.item())
        ans['confidence'] = confidence
    #ans['start'] = answer_start.item()
    #ans['end'] = answer_end.item()
    ans['abstract_bert'] = abs_returned
    ans['abs_too_long'] = absTooLong
    return ans

from tqdm import tqdm
def searchAbstracts(hit_dictionary, question):
    abstractResults = {}
    for k,v in tqdm(hit_dictionary.items()):
        abstract = v['abstract_full']
        indexed_para=v['indexed_para']
        if abstract:
            ans = makeBERTSQuADPrediction(abstract, question)
            if ans['answer']:
                confidence = ans['confidence']
                abstractResults[confidence]={}
                abstractResults[confidence]['answer'] = ans['answer']
                #abstractResults[confidence]['start'] = ans['start']
                #abstractResults[confidence]['end'] = ans['end']
                abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']
                abstractResults[confidence]['idx'] = k
                abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']

                
    cList = list(abstractResults.keys())

    if cList:
        maxScore = max(cList)
        total = 0.0
        exp_scores = []
        for c in cList:
            s = np.exp(c-maxScore)
            exp_scores.append(s)
        total = sum(exp_scores)
        for i,c in enumerate(cList):
            abstractResults[exp_scores[i]/total] = abstractResults.pop(c)
    return abstractResults

answers = searchAbstracts(hit_dictionary, query)

FIND_PDFS=False 
SEARCH_MEDRXIV=False

workingPath = '/root/kaggle/working'
import pandas as pd
import re
if FIND_PDFS:
    from metapub import UrlReverse
    from metapub import FindIt
from IPython.core.display import display, HTML

#from summarizer import Summarizer
#summarizerModel = Summarizer()
def displayResults(hit_dictionary, answers, question):
    
    question_HTML = '<div style="font-family: Times New Roman; font-size: 28px; padding-bottom:28px"><b>Query</b>: '+question+'</div>'
    #all_HTML_txt = question_HTML
    confidence = list(answers.keys())
    confidence.sort(reverse=True)
    
    confidence = list(answers.keys())
    confidence.sort(reverse=True)
    

    for c in confidence:
        if c>0 and c <= 1 and len(answers[c]['answer']) != 0:
            if 'idx' not in  answers[c]:
                continue
            rowData = []
            idx = answers[c]['idx']
            title = hit_dictionary[idx]['title']

            
            full_abs = answers[c]['abstract_bert']
            bert_ans = answers[c]['answer']
            
            
            #split_abs = full_abs.split(bert_ans)
            #x=(split_abs[0][:split_abs[0].rfind('. ')+1])
            #sentance_beginning = x[x.rfind('. ')+1:] + split_abs[0][split_abs[0].rfind('. ')+1:]
            #sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]
            x=''
            y=''
            z=''
            t=''
            regex = r"(\.\s[^0-9])(?!.*(\.\s[^0-9]))"
            split_abs = full_abs.split(bert_ans)
            matches = re.finditer(regex, split_abs[0], re.MULTILINE)
            for matchNum1, match in enumerate(matches, start=1):
              y=(split_abs[0][:match.start()+1])
            matches = re.finditer(regex, y, re.MULTILINE)
            for matchNum2, match in enumerate(matches, start=1):
              x=(y[match.start()+1:])
            if x=='':
              x=split_abs[0]
            matches = re.finditer(regex, split_abs[0], re.MULTILINE)
            for matchNum3, match in enumerate(matches, start=1):
              z=(split_abs[0][match.start()+1:])  
            sentance_beginning = x+z
            regex2=r"(.*?(?<!\b\w)[.?!])\s+[a-zA-Z0-9]"
            if len(split_abs) == 1:
                sentance_end_pos = len(full_abs)
                sentance_end =''
            else:
                matches = re.finditer(regex2, split_abs[1], re.MULTILINE)
                for matchNum4, match in enumerate(matches, start=1):
                  if matchNum4==1:
                    t=(split_abs[1][:match.end()-2])
                sentance_end_pos = split_abs[1].find('. ')+1
                if sentance_end_pos == 0:
                    sentance_end = split_abs[1]
                else:
                    sentance_end = t
            #if len(split_abs) == 1:
            #    sentance_end_pos = len(full_abs)
            #    sentance_end =''
            #else:
            #    sentance_end_pos = split_abs[1].find('. ')+1
            #    if sentance_end_pos == 0:
            #      sentance_end = split_abs[1]
            #    else:
            #      sentance_end = split_abs[1][:sentance_end_pos]
                
            #sentance_full = sentance_beginning + bert_ans+ sentance_end
            answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end
            answers[c]['partial_answer'] = bert_ans+sentance_end
            answers[c]['sentence_beginning'] = sentance_beginning
            answers[c]['sentence_end'] = sentance_end
            answers[c]['title'] = title
        else:
            answers.pop(c)
    
    
    ## now rerank based on semantic similarity of the answers to the question
    ## Universal sentence encoder
    cList = list(answers.keys())
    allAnswers = [answers[c]['full_answer'] for c in cList]
    
    messages = [question]+allAnswers
    
    encoding_matrix = embed_fn(messages)
    similarity_matrix = np.inner(encoding_matrix, encoding_matrix)
    rankings = similarity_matrix[1:,0]
    
    for i,c in enumerate(cList):
        answers[rankings[i]] = answers.pop(c)

    ## now form pandas dv
    confidence = list(answers.keys())
    confidence.sort(reverse=True)
    pandasData = []
    ranked_aswers = []
    full_abs_list=[]
    for c in confidence:
        rowData=[]
        title = answers[c]['title']
        idx = answers[c]['idx']
        rowData += [idx]            
        sentance_html = '<div>' +answers[c]['sentence_beginning'] + " <font color='red'>"+answers[c]['answer']+"</font> "+answers[c]['sentence_end']+'</div>'
        
        rowData += [sentance_html, c,title]
        pandasData.append(rowData)
        ranked_aswers.append(' '.join([answers[c]['full_answer']]))
        full_abs_list.append(' '.join([answers[c]['abstract_bert']]))
    
    if FIND_PDFS or SEARCH_MEDRXIV:
        pdata2 = []
        pm_ids = []
        for rowData in pandasData:
            rd = rowData
            idx = rowData[0]
    else:
        pdata2 = pandasData
        
    
    display(HTML(question_HTML))

    if USE_SUMMARY:
        ## try generating an exacutive summary with bart abstractive summarizer
        allAnswersTxt = ' '.join(ranked_aswers[:10]).replace('\n','')
    #    exec_sum = summarizerModel(allAnswersTxt, min_length=1, max_length=500)    
     #   execSum_HTML = '<div style="font-family: Times New Roman; font-size: 18px; padding-bottom:18px"><b>BERT Extractive Summary:</b>: '+exec_sum+'</div>'

        answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=2048)['input_ids'].to(torch_device)
        #answers_input_ids = SUMMARY_TOKENIZER.prepare_seq2seq_batch([allAnswersTxt], return_tensors='pt', truncation=True, padding='longest')['input_ids'].to(torch_device)
        summary_ids = SUMMARY_MODEL.generate(answers_input_ids,
                                               num_beams=10,
                                               length_penalty=1.1,
                                               max_length=2048,
                                               min_length=64,
                                               no_repeat_ngram_size=0,
                                                do_sample=False )

        exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)
        execSum_HTML = '<div style="font-family: Times New Roman; font-size: 18px; margin-bottom:1pt"><b>BART Abstractive Summary:</b>: '+exec_sum+'</div>'
        display(HTML(execSum_HTML))
        warning_HTML = '<div style="font-family: Times New Roman; font-size: 12px; padding-bottom:12px; color:#CCCC00; margin-top:1pt"> Warning this is an autogenerated summary based on semantic search of abstracts, always examine the sources before accepting this conclusion.  If the evidence only mentions topic in passing or the evidence is not clear, the summary will likely not clearly answer the question.</div>'
        display(HTML(warning_HTML))

#    display(HTML('<div style="font-family: Times New Roman; font-size: 18px; padding-bottom:18px"><b>Body of Evidence:</b></div>'))
    
    if FIND_PDFS or SEARCH_MEDRXIV:
        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])
    else:
        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])
        
    display(HTML(df.to_html(render_links=True, escape=False)))
    return full_abs_list,ranked_aswers,pm_ids

full_abs_list,ranked_aswers,pm_ids=displayResults(hit_dictionary, answers, query)

hits[0].lucene_document.fields.toString().stored()