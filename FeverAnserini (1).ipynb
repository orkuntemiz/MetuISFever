{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FeverAnserini.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQtJd7m0H_gC"
      },
      "source": [
        "%%capture\n",
        "!apt-get install maven -qq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKkh78zferE8"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUoccBGhIaiP"
      },
      "source": [
        "%%capture\n",
        "!git clone --recurse-submodules https://github.com/castorini/anserini.git\n",
        "%cd anserini\n",
        "!cd tools/eval && tar xvfz trec_eval.9.0.4.tar.gz && cd trec_eval.9.0.4 && make && cd ../../..\n",
        "!mvn clean package appassembler:assemble -DskipTests -Dmaven.javadoc.skip=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRWvO9ftIcdB",
        "outputId": "eb21c085-40ff-4bf5-acd9-f70f72497e45"
      },
      "source": [
        "#import the necessary data (i am not sure this is the latest data)\n",
        "!mkdir collections/fever\n",
        "!mkdir indexes/fever\n",
        "\n",
        "!wget https://s3-eu-west-1.amazonaws.com/fever.public/feverous/feverous-wiki-pages.zip -P collections/fever\n",
        "!unzip collections/fever/feverous-wiki-pages.zip -d collections/fever\n",
        "\n",
        "!wget https://s3-eu-west-1.amazonaws.com/fever.public/feverous/train.jsonl -P collections/fever\n",
        "!wget https://s3-eu-west-1.amazonaws.com/fever.public/feverous/dev.jsonl -P collections/fever"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-13 18:47:44--  https://s3-eu-west-1.amazonaws.com/fever.public/feverous/feverous-wiki-pages.zip\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.89.131\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.89.131|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9906569155 (9.2G) [application/zip]\n",
            "Saving to: ‘collections/fever/feverous-wiki-pages.zip’\n",
            "\n",
            "feverous-wiki-pages 100%[===================>]   9.23G  63.7MB/s    in 2m 26s  \n",
            "\n",
            "2021-06-13 18:50:10 (64.9 MB/s) - ‘collections/fever/feverous-wiki-pages.zip’ saved [9906569155/9906569155]\n",
            "\n",
            "Archive:  collections/fever/feverous-wiki-pages.zip\n",
            "   creating: collections/fever/FeverousWikiv1/\n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_283.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_402.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_177.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_287.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_483.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_210.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_451.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_220.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_110.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_320.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_425.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_040.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_238.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_336.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_033.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_008.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_298.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_189.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_326.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_198.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_159.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_426.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_131.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_137.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_069.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_311.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_230.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_526.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_038.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_349.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_428.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_174.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_239.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_392.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_254.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_361.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_480.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_401.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_063.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_318.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_452.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_379.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_003.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_205.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_099.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_173.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_236.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_116.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_393.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_015.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_302.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_217.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_502.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_180.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_193.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_141.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_176.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_257.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_411.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_303.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_247.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_046.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_048.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_344.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_309.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_044.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_342.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_051.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_140.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_413.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_242.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_028.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_115.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_530.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_509.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_385.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_072.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_163.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_510.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_167.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_519.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_057.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_132.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_061.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_415.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_443.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_035.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_092.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_360.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_481.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_340.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_136.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_098.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_076.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_486.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_190.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_474.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_100.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_016.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_104.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_409.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_259.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_388.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_029.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_292.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_525.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_507.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_146.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_383.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_312.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_228.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_358.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_289.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_215.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_601.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_246.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_306.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_290.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_001.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_606.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_417.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_339.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_325.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_000.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_041.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_485.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_271.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_160.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_206.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_119.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_148.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_107.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_448.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_521.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_114.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_195.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_017.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_418.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_497.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_070.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_145.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_408.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_023.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_356.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_218.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_399.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_406.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_187.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_071.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_030.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_026.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_012.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_291.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_468.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_245.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_352.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_433.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_088.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_405.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_602.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_208.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_477.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_412.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_224.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_087.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_427.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_094.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_517.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_101.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_364.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_499.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_475.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_310.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_372.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_300.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_382.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_064.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_125.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_332.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_049.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_231.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_316.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_429.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_184.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_282.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_232.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_274.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_234.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_523.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_032.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_095.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_391.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_214.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_075.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_374.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_455.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_262.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_068.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_089.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_295.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_453.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_147.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_084.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_149.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_060.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_256.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_605.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_288.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_450.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_122.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_221.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_459.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_251.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_447.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_462.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_348.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_467.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_407.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_482.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_233.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_249.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_512.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_416.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_351.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_389.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_161.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_400.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_050.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_010.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_508.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_152.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_473.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_059.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_255.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_105.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_018.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_394.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_258.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_494.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_192.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_337.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_322.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_449.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_346.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_500.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_097.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_165.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_155.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_034.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_381.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_111.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_007.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_085.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_377.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_074.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_212.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_511.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_345.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_194.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_281.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_533.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_479.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_438.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_191.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_005.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_285.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_273.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_103.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_127.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_367.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_266.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_534.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_083.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_264.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_207.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_219.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_333.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_047.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_444.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_398.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_144.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_157.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_077.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_216.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_020.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_515.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_373.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_013.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_263.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_404.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_492.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_253.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_209.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_472.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_488.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_117.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_461.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_043.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_432.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_168.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_156.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_252.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_162.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_225.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_204.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_275.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_229.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_053.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_284.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_268.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_011.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_158.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_260.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_607.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_495.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_498.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_503.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_042.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_380.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_307.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_505.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_024.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_314.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_265.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_065.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_128.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_324.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_501.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_424.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_313.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_489.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_054.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_330.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_270.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_021.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_308.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_169.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_478.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_327.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_513.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_395.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_341.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_121.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_241.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_172.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_150.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_384.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_269.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_369.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_002.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_004.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_126.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_430.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_223.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_368.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_133.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_299.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_504.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_419.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_056.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_343.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_532.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_378.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_202.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_240.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_244.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_151.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_082.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_297.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_397.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_138.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_237.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_476.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_055.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_603.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_226.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_331.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_387.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_457.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_437.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_524.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_436.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_293.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_182.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_319.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_484.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_143.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_506.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_520.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_006.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_079.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_052.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_609.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_420.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_353.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_102.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_014.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_250.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_009.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_493.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_197.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_036.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_196.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_154.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_350.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_458.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_045.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_203.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_454.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_109.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_186.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_366.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_359.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_516.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_370.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_371.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_178.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_025.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_067.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_175.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_108.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_261.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_465.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_363.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_200.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_466.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_375.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_134.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_403.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_354.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_090.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_445.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_376.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_086.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_321.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_027.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_460.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_338.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_490.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_078.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_357.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_091.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_410.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_201.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_470.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_130.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_022.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_062.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_355.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_166.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_434.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_080.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_301.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_464.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_305.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_435.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_124.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_423.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_170.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_129.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_487.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_422.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_317.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_179.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_439.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_518.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_081.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_066.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_276.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_113.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_600.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_106.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_414.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_294.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_267.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_315.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_142.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_531.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_037.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_235.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_112.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_496.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_248.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_456.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_440.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_222.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_608.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_604.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_334.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_277.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_446.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_039.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_135.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_093.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_153.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_171.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_469.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_390.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_164.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_280.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_335.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_019.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_491.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_421.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_031.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_471.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_527.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_329.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_211.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_610.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_386.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_188.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_442.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_396.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_120.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_463.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_213.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_441.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_347.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_199.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_365.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_073.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_304.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_328.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_362.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_123.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_522.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_272.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_058.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_183.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_286.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_243.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_096.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_431.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_279.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_139.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_514.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_323.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_181.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_278.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_118.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_185.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_296.jsonl  \n",
            "  inflating: collections/fever/FeverousWikiv1/wiki_227.jsonl  \n",
            "--2021-06-13 19:00:11--  https://s3-eu-west-1.amazonaws.com/fever.public/feverous/train.jsonl\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.30.131\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.30.131|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 175493294 (167M) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘collections/fever/train.jsonl’\n",
            "\n",
            "train.jsonl         100%[===================>] 167.36M  91.7MB/s    in 1.8s    \n",
            "\n",
            "2021-06-13 19:00:13 (91.7 MB/s) - ‘collections/fever/train.jsonl’ saved [175493294/175493294]\n",
            "\n",
            "--2021-06-13 19:00:13--  https://s3-eu-west-1.amazonaws.com/fever.public/feverous/dev.jsonl\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.30.131\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.30.131|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17827949 (17M) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘collections/fever/dev.jsonl’\n",
            "\n",
            "dev.jsonl           100%[===================>]  17.00M  58.8MB/s    in 0.3s    \n",
            "\n",
            "2021-06-13 19:00:13 (58.8 MB/s) - ‘collections/fever/dev.jsonl’ saved [17827949/17827949]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmNJ0RIncWBy",
        "outputId": "b1a7095b-6d4d-4e5b-e914-87e6fe82bf44"
      },
      "source": [
        "!ls collections/fever/FeverousWikiv1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wiki_000.jsonl\twiki_109.jsonl\twiki_218.jsonl\twiki_327.jsonl\twiki_436.jsonl\n",
            "wiki_001.jsonl\twiki_110.jsonl\twiki_219.jsonl\twiki_328.jsonl\twiki_437.jsonl\n",
            "wiki_002.jsonl\twiki_111.jsonl\twiki_220.jsonl\twiki_329.jsonl\twiki_438.jsonl\n",
            "wiki_003.jsonl\twiki_112.jsonl\twiki_221.jsonl\twiki_330.jsonl\twiki_439.jsonl\n",
            "wiki_004.jsonl\twiki_113.jsonl\twiki_222.jsonl\twiki_331.jsonl\twiki_440.jsonl\n",
            "wiki_005.jsonl\twiki_114.jsonl\twiki_223.jsonl\twiki_332.jsonl\twiki_441.jsonl\n",
            "wiki_006.jsonl\twiki_115.jsonl\twiki_224.jsonl\twiki_333.jsonl\twiki_442.jsonl\n",
            "wiki_007.jsonl\twiki_116.jsonl\twiki_225.jsonl\twiki_334.jsonl\twiki_443.jsonl\n",
            "wiki_008.jsonl\twiki_117.jsonl\twiki_226.jsonl\twiki_335.jsonl\twiki_444.jsonl\n",
            "wiki_009.jsonl\twiki_118.jsonl\twiki_227.jsonl\twiki_336.jsonl\twiki_445.jsonl\n",
            "wiki_010.jsonl\twiki_119.jsonl\twiki_228.jsonl\twiki_337.jsonl\twiki_446.jsonl\n",
            "wiki_011.jsonl\twiki_120.jsonl\twiki_229.jsonl\twiki_338.jsonl\twiki_447.jsonl\n",
            "wiki_012.jsonl\twiki_121.jsonl\twiki_230.jsonl\twiki_339.jsonl\twiki_448.jsonl\n",
            "wiki_013.jsonl\twiki_122.jsonl\twiki_231.jsonl\twiki_340.jsonl\twiki_449.jsonl\n",
            "wiki_014.jsonl\twiki_123.jsonl\twiki_232.jsonl\twiki_341.jsonl\twiki_450.jsonl\n",
            "wiki_015.jsonl\twiki_124.jsonl\twiki_233.jsonl\twiki_342.jsonl\twiki_451.jsonl\n",
            "wiki_016.jsonl\twiki_125.jsonl\twiki_234.jsonl\twiki_343.jsonl\twiki_452.jsonl\n",
            "wiki_017.jsonl\twiki_126.jsonl\twiki_235.jsonl\twiki_344.jsonl\twiki_453.jsonl\n",
            "wiki_018.jsonl\twiki_127.jsonl\twiki_236.jsonl\twiki_345.jsonl\twiki_454.jsonl\n",
            "wiki_019.jsonl\twiki_128.jsonl\twiki_237.jsonl\twiki_346.jsonl\twiki_455.jsonl\n",
            "wiki_020.jsonl\twiki_129.jsonl\twiki_238.jsonl\twiki_347.jsonl\twiki_456.jsonl\n",
            "wiki_021.jsonl\twiki_130.jsonl\twiki_239.jsonl\twiki_348.jsonl\twiki_457.jsonl\n",
            "wiki_022.jsonl\twiki_131.jsonl\twiki_240.jsonl\twiki_349.jsonl\twiki_458.jsonl\n",
            "wiki_023.jsonl\twiki_132.jsonl\twiki_241.jsonl\twiki_350.jsonl\twiki_459.jsonl\n",
            "wiki_024.jsonl\twiki_133.jsonl\twiki_242.jsonl\twiki_351.jsonl\twiki_460.jsonl\n",
            "wiki_025.jsonl\twiki_134.jsonl\twiki_243.jsonl\twiki_352.jsonl\twiki_461.jsonl\n",
            "wiki_026.jsonl\twiki_135.jsonl\twiki_244.jsonl\twiki_353.jsonl\twiki_462.jsonl\n",
            "wiki_027.jsonl\twiki_136.jsonl\twiki_245.jsonl\twiki_354.jsonl\twiki_463.jsonl\n",
            "wiki_028.jsonl\twiki_137.jsonl\twiki_246.jsonl\twiki_355.jsonl\twiki_464.jsonl\n",
            "wiki_029.jsonl\twiki_138.jsonl\twiki_247.jsonl\twiki_356.jsonl\twiki_465.jsonl\n",
            "wiki_030.jsonl\twiki_139.jsonl\twiki_248.jsonl\twiki_357.jsonl\twiki_466.jsonl\n",
            "wiki_031.jsonl\twiki_140.jsonl\twiki_249.jsonl\twiki_358.jsonl\twiki_467.jsonl\n",
            "wiki_032.jsonl\twiki_141.jsonl\twiki_250.jsonl\twiki_359.jsonl\twiki_468.jsonl\n",
            "wiki_033.jsonl\twiki_142.jsonl\twiki_251.jsonl\twiki_360.jsonl\twiki_469.jsonl\n",
            "wiki_034.jsonl\twiki_143.jsonl\twiki_252.jsonl\twiki_361.jsonl\twiki_470.jsonl\n",
            "wiki_035.jsonl\twiki_144.jsonl\twiki_253.jsonl\twiki_362.jsonl\twiki_471.jsonl\n",
            "wiki_036.jsonl\twiki_145.jsonl\twiki_254.jsonl\twiki_363.jsonl\twiki_472.jsonl\n",
            "wiki_037.jsonl\twiki_146.jsonl\twiki_255.jsonl\twiki_364.jsonl\twiki_473.jsonl\n",
            "wiki_038.jsonl\twiki_147.jsonl\twiki_256.jsonl\twiki_365.jsonl\twiki_474.jsonl\n",
            "wiki_039.jsonl\twiki_148.jsonl\twiki_257.jsonl\twiki_366.jsonl\twiki_475.jsonl\n",
            "wiki_040.jsonl\twiki_149.jsonl\twiki_258.jsonl\twiki_367.jsonl\twiki_476.jsonl\n",
            "wiki_041.jsonl\twiki_150.jsonl\twiki_259.jsonl\twiki_368.jsonl\twiki_477.jsonl\n",
            "wiki_042.jsonl\twiki_151.jsonl\twiki_260.jsonl\twiki_369.jsonl\twiki_478.jsonl\n",
            "wiki_043.jsonl\twiki_152.jsonl\twiki_261.jsonl\twiki_370.jsonl\twiki_479.jsonl\n",
            "wiki_044.jsonl\twiki_153.jsonl\twiki_262.jsonl\twiki_371.jsonl\twiki_480.jsonl\n",
            "wiki_045.jsonl\twiki_154.jsonl\twiki_263.jsonl\twiki_372.jsonl\twiki_481.jsonl\n",
            "wiki_046.jsonl\twiki_155.jsonl\twiki_264.jsonl\twiki_373.jsonl\twiki_482.jsonl\n",
            "wiki_047.jsonl\twiki_156.jsonl\twiki_265.jsonl\twiki_374.jsonl\twiki_483.jsonl\n",
            "wiki_048.jsonl\twiki_157.jsonl\twiki_266.jsonl\twiki_375.jsonl\twiki_484.jsonl\n",
            "wiki_049.jsonl\twiki_158.jsonl\twiki_267.jsonl\twiki_376.jsonl\twiki_485.jsonl\n",
            "wiki_050.jsonl\twiki_159.jsonl\twiki_268.jsonl\twiki_377.jsonl\twiki_486.jsonl\n",
            "wiki_051.jsonl\twiki_160.jsonl\twiki_269.jsonl\twiki_378.jsonl\twiki_487.jsonl\n",
            "wiki_052.jsonl\twiki_161.jsonl\twiki_270.jsonl\twiki_379.jsonl\twiki_488.jsonl\n",
            "wiki_053.jsonl\twiki_162.jsonl\twiki_271.jsonl\twiki_380.jsonl\twiki_489.jsonl\n",
            "wiki_054.jsonl\twiki_163.jsonl\twiki_272.jsonl\twiki_381.jsonl\twiki_490.jsonl\n",
            "wiki_055.jsonl\twiki_164.jsonl\twiki_273.jsonl\twiki_382.jsonl\twiki_491.jsonl\n",
            "wiki_056.jsonl\twiki_165.jsonl\twiki_274.jsonl\twiki_383.jsonl\twiki_492.jsonl\n",
            "wiki_057.jsonl\twiki_166.jsonl\twiki_275.jsonl\twiki_384.jsonl\twiki_493.jsonl\n",
            "wiki_058.jsonl\twiki_167.jsonl\twiki_276.jsonl\twiki_385.jsonl\twiki_494.jsonl\n",
            "wiki_059.jsonl\twiki_168.jsonl\twiki_277.jsonl\twiki_386.jsonl\twiki_495.jsonl\n",
            "wiki_060.jsonl\twiki_169.jsonl\twiki_278.jsonl\twiki_387.jsonl\twiki_496.jsonl\n",
            "wiki_061.jsonl\twiki_170.jsonl\twiki_279.jsonl\twiki_388.jsonl\twiki_497.jsonl\n",
            "wiki_062.jsonl\twiki_171.jsonl\twiki_280.jsonl\twiki_389.jsonl\twiki_498.jsonl\n",
            "wiki_063.jsonl\twiki_172.jsonl\twiki_281.jsonl\twiki_390.jsonl\twiki_499.jsonl\n",
            "wiki_064.jsonl\twiki_173.jsonl\twiki_282.jsonl\twiki_391.jsonl\twiki_500.jsonl\n",
            "wiki_065.jsonl\twiki_174.jsonl\twiki_283.jsonl\twiki_392.jsonl\twiki_501.jsonl\n",
            "wiki_066.jsonl\twiki_175.jsonl\twiki_284.jsonl\twiki_393.jsonl\twiki_502.jsonl\n",
            "wiki_067.jsonl\twiki_176.jsonl\twiki_285.jsonl\twiki_394.jsonl\twiki_503.jsonl\n",
            "wiki_068.jsonl\twiki_177.jsonl\twiki_286.jsonl\twiki_395.jsonl\twiki_504.jsonl\n",
            "wiki_069.jsonl\twiki_178.jsonl\twiki_287.jsonl\twiki_396.jsonl\twiki_505.jsonl\n",
            "wiki_070.jsonl\twiki_179.jsonl\twiki_288.jsonl\twiki_397.jsonl\twiki_506.jsonl\n",
            "wiki_071.jsonl\twiki_180.jsonl\twiki_289.jsonl\twiki_398.jsonl\twiki_507.jsonl\n",
            "wiki_072.jsonl\twiki_181.jsonl\twiki_290.jsonl\twiki_399.jsonl\twiki_508.jsonl\n",
            "wiki_073.jsonl\twiki_182.jsonl\twiki_291.jsonl\twiki_400.jsonl\twiki_509.jsonl\n",
            "wiki_074.jsonl\twiki_183.jsonl\twiki_292.jsonl\twiki_401.jsonl\twiki_510.jsonl\n",
            "wiki_075.jsonl\twiki_184.jsonl\twiki_293.jsonl\twiki_402.jsonl\twiki_511.jsonl\n",
            "wiki_076.jsonl\twiki_185.jsonl\twiki_294.jsonl\twiki_403.jsonl\twiki_512.jsonl\n",
            "wiki_077.jsonl\twiki_186.jsonl\twiki_295.jsonl\twiki_404.jsonl\twiki_513.jsonl\n",
            "wiki_078.jsonl\twiki_187.jsonl\twiki_296.jsonl\twiki_405.jsonl\twiki_514.jsonl\n",
            "wiki_079.jsonl\twiki_188.jsonl\twiki_297.jsonl\twiki_406.jsonl\twiki_515.jsonl\n",
            "wiki_080.jsonl\twiki_189.jsonl\twiki_298.jsonl\twiki_407.jsonl\twiki_516.jsonl\n",
            "wiki_081.jsonl\twiki_190.jsonl\twiki_299.jsonl\twiki_408.jsonl\twiki_517.jsonl\n",
            "wiki_082.jsonl\twiki_191.jsonl\twiki_300.jsonl\twiki_409.jsonl\twiki_518.jsonl\n",
            "wiki_083.jsonl\twiki_192.jsonl\twiki_301.jsonl\twiki_410.jsonl\twiki_519.jsonl\n",
            "wiki_084.jsonl\twiki_193.jsonl\twiki_302.jsonl\twiki_411.jsonl\twiki_520.jsonl\n",
            "wiki_085.jsonl\twiki_194.jsonl\twiki_303.jsonl\twiki_412.jsonl\twiki_521.jsonl\n",
            "wiki_086.jsonl\twiki_195.jsonl\twiki_304.jsonl\twiki_413.jsonl\twiki_522.jsonl\n",
            "wiki_087.jsonl\twiki_196.jsonl\twiki_305.jsonl\twiki_414.jsonl\twiki_523.jsonl\n",
            "wiki_088.jsonl\twiki_197.jsonl\twiki_306.jsonl\twiki_415.jsonl\twiki_524.jsonl\n",
            "wiki_089.jsonl\twiki_198.jsonl\twiki_307.jsonl\twiki_416.jsonl\twiki_525.jsonl\n",
            "wiki_090.jsonl\twiki_199.jsonl\twiki_308.jsonl\twiki_417.jsonl\twiki_526.jsonl\n",
            "wiki_091.jsonl\twiki_200.jsonl\twiki_309.jsonl\twiki_418.jsonl\twiki_527.jsonl\n",
            "wiki_092.jsonl\twiki_201.jsonl\twiki_310.jsonl\twiki_419.jsonl\twiki_530.jsonl\n",
            "wiki_093.jsonl\twiki_202.jsonl\twiki_311.jsonl\twiki_420.jsonl\twiki_531.jsonl\n",
            "wiki_094.jsonl\twiki_203.jsonl\twiki_312.jsonl\twiki_421.jsonl\twiki_532.jsonl\n",
            "wiki_095.jsonl\twiki_204.jsonl\twiki_313.jsonl\twiki_422.jsonl\twiki_533.jsonl\n",
            "wiki_096.jsonl\twiki_205.jsonl\twiki_314.jsonl\twiki_423.jsonl\twiki_534.jsonl\n",
            "wiki_097.jsonl\twiki_206.jsonl\twiki_315.jsonl\twiki_424.jsonl\twiki_600.jsonl\n",
            "wiki_098.jsonl\twiki_207.jsonl\twiki_316.jsonl\twiki_425.jsonl\twiki_601.jsonl\n",
            "wiki_099.jsonl\twiki_208.jsonl\twiki_317.jsonl\twiki_426.jsonl\twiki_602.jsonl\n",
            "wiki_100.jsonl\twiki_209.jsonl\twiki_318.jsonl\twiki_427.jsonl\twiki_603.jsonl\n",
            "wiki_101.jsonl\twiki_210.jsonl\twiki_319.jsonl\twiki_428.jsonl\twiki_604.jsonl\n",
            "wiki_102.jsonl\twiki_211.jsonl\twiki_320.jsonl\twiki_429.jsonl\twiki_605.jsonl\n",
            "wiki_103.jsonl\twiki_212.jsonl\twiki_321.jsonl\twiki_430.jsonl\twiki_606.jsonl\n",
            "wiki_104.jsonl\twiki_213.jsonl\twiki_322.jsonl\twiki_431.jsonl\twiki_607.jsonl\n",
            "wiki_105.jsonl\twiki_214.jsonl\twiki_323.jsonl\twiki_432.jsonl\twiki_608.jsonl\n",
            "wiki_106.jsonl\twiki_215.jsonl\twiki_324.jsonl\twiki_433.jsonl\twiki_609.jsonl\n",
            "wiki_107.jsonl\twiki_216.jsonl\twiki_325.jsonl\twiki_434.jsonl\twiki_610.jsonl\n",
            "wiki_108.jsonl\twiki_217.jsonl\twiki_326.jsonl\twiki_435.jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCY3dd7lYnuQ",
        "outputId": "5feb444d-4288-463c-ab58-e54516b228dd"
      },
      "source": [
        "# A folder path, a file path, or a list of file paths is needed\n",
        "\n",
        "# DATA_LOCATION = \"wiki_pages\"\n",
        "# DATA_LOCATION = \"wiki_pages/wiki_000.jsonl\"\n",
        "DATA_LOCATION = [\n",
        "    \"collections/fever/FeverousWikiv1/wiki_000.jsonl\",\n",
        "    \"collections/fever/FeverousWikiv1/wiki_001.jsonl\",\n",
        "    \"collections/fever/FeverousWikiv1/wiki_002.jsonl\",\n",
        "    \"collections/fever/FeverousWikiv1/wiki_003.jsonl\",\n",
        "    \"collections/fever/FeverousWikiv1/wiki_004.jsonl\",\n",
        "]\n",
        "OUTPUT_FOLDER = \"wiki_pages_anserini\"\n",
        "\n",
        "SIMPLIFY_PAGE_LINKS = False\n",
        "PREFER_URL_FOR_LINKS = False\n",
        "\n",
        "# If SIMPLIFY_PAGE_LINKS and not PREFER_URL_FOR_LINKS: [[Page_(disambiguation)|Page Title]] -> Page Title\n",
        "# If SIMPLIFY_PAGE_LINKS and PREFER_URL_FOR_LINKS: [[Page_(disambiguation)|Page Title]] -> Page (disambiguation)\n",
        "# Note that the URL basename can sometimes contain extra information, but it can also redirect to a page that is different than the visible label.\n",
        "\n",
        "SUBSECTION_SEP = \"|\"\n",
        "SUBSUBSECTION_SEP = \" \"\n",
        "import glob\n",
        "import os\n",
        "\n",
        "to_process = None\n",
        "\n",
        "if type(DATA_LOCATION) == list:  # A list of file paths\n",
        "    to_process = [file for file in DATA_LOCATION]\n",
        "elif os.path.isfile(DATA_LOCATION):  # A file path\n",
        "    to_process = [DATA_LOCATION]\n",
        "elif os.path.isdir(DATA_LOCATION):  # A folder\n",
        "    to_process = glob.glob(\"{}/*.jsonl\".format(DATA_LOCATION))\n",
        "else:\n",
        "    raise ValueError(\"Data location is not a valid file or folder.\")\n",
        "\n",
        "print(\"{} file(s) will be processed.\".format(len(to_process)))\n",
        "\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "def clean_value(value):\n",
        "    \"\"\"Replaces unnecessary space (including \\t and such) with a single space\n",
        "    character.\n",
        "\n",
        "    Arg:\n",
        "        value: A string.\n",
        "\n",
        "    Returns:\n",
        "        The simplified version of the provided string.\n",
        "    \"\"\"\n",
        "    return \" \".join(value.split())\n",
        "\n",
        "\n",
        "def final_clean(\n",
        "    field,\n",
        "    simplify_page_links=SIMPLIFY_PAGE_LINKS,\n",
        "    prefer_url_for_links=PREFER_URL_FOR_LINKS,\n",
        "):\n",
        "    \"\"\"Removes multiple space characters with a single space character. Removes\n",
        "    leading and trailing spaces. Simplifies the page links according to the\n",
        "    parameters.\n",
        "\n",
        "    Args:\n",
        "        field (str): A field of the processed page data.\n",
        "        simplify_page_links (Boolean): A Boolean indicating whether the page\n",
        "            links must be simplified. By default, it is set to\n",
        "            SIMPLIFY_PAGE_LINKS.\n",
        "        prefer_url_for_links (Boolean): A Boolean indicating whether the page\n",
        "            links must be simplified to the URL basename instead of the link's\n",
        "            label. By default, it is set to SIMPLIFY_PAGE_LINKS.\n",
        "\n",
        "    Returns:\n",
        "        str: The simplified version of the provided string.\n",
        "    \"\"\"\n",
        "    field = re.sub(\" +\", \" \", field.strip(), flags=re.MULTILINE)\n",
        "    if SIMPLIFY_PAGE_LINKS and PREFER_URL_FOR_LINKS:\n",
        "        # Simplifies links by replacing them with the URL basename (replaces underscores)\n",
        "        # Underscores are replaced with space\n",
        "        field = re.sub(r\"(?:\\[\\[)(.*)?(?:\\|)\", field.replace(\"_\", \" \"), field)\n",
        "        field = re.sub(r\"(\\|).*?(\\]\\])|\\[\\[\", \"\", field, flags=re.MULTILINE)\n",
        "    elif SIMPLIFY_PAGE_LINKS:\n",
        "        # Simplifies links by replacing them with the titles\n",
        "        field = re.sub(r\"(\\[\\[).*?(\\|)|]]\", \"\", field, flags=re.MULTILINE)\n",
        "    return field\n",
        "\n",
        "\n",
        "if not os.path.exists(OUTPUT_FOLDER):  # Creates the output folder if it does not exist\n",
        "    os.mkdir(OUTPUT_FOLDER)\n",
        "\n",
        "counter = 0\n",
        "for file_path in to_process:\n",
        "    file_basename = os.path.basename(file_path)\n",
        "    output_path = os.path.join(OUTPUT_FOLDER, file_basename)\n",
        "\n",
        "    open(output_path, mode=\"w\").close()  # Creating an empty output file\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        file_data = [json.loads(line) for line in f]\n",
        "\n",
        "    for page in file_data:\n",
        "\n",
        "        page_data_processed = {\"id\": page[\"title\"], \"text\": \"\", \"lines\": \"\"}\n",
        "\n",
        "        # The order list of the page will be used to parse the items\n",
        "        for item_id, item in enumerate(page[\"order\"]):\n",
        "            if item.startswith(\"sentence_\"):  # Sentence\n",
        "                clean_element = clean_value(page[item])\n",
        "                page_data_processed[\"text\"] += \" {}\".format(clean_element)\n",
        "                page_data_processed[\"lines\"] += \"{}\\t{}\\n\".format(item, clean_element)\n",
        "            elif item.startswith(\"table_\"):  # Table\n",
        "                page_data_processed[\"text\"] += \" \"\n",
        "                page_data_processed[\"lines\"] += \"{}\\t\".format(item)\n",
        "                table_rows = []\n",
        "                for row in page[item][\"table\"]:\n",
        "                    row_items = []\n",
        "                    for cell in row:\n",
        "                        row_items.append(clean_value(cell[\"value\"]))\n",
        "\n",
        "                    row_text = \"{}\".format(SUBSUBSECTION_SEP).join(row_items)\n",
        "                    table_rows.append(row_text)\n",
        "\n",
        "                if \"caption\" in page[item]:\n",
        "                    table_rows.append(clean_value(page[item][\"caption\"]))\n",
        "\n",
        "                table_text = \"{}\".format(SUBSECTION_SEP).join(table_rows)\n",
        "                page_data_processed[\"text\"] += table_text\n",
        "                page_data_processed[\"lines\"] += table_text\n",
        "                page_data_processed[\"lines\"] += \"\\n\"\n",
        "            elif item.startswith(\"section_\"):  # Section\n",
        "                clean_element = clean_value(page[item][\"value\"])\n",
        "                page_data_processed[\"text\"] += \" {}\".format(clean_element)\n",
        "                page_data_processed[\"lines\"] += \"{}\\t{}\\n\".format(item, clean_element)\n",
        "            elif item.startswith(\"list_\"):  # List\n",
        "                list_items = []\n",
        "                for list_item in page[item][\"list\"]:\n",
        "                    list_items.append(clean_value(list_item[\"value\"]))\n",
        "\n",
        "                list_text = \"{}\".format(SUBSECTION_SEP).join(list_items)\n",
        "                page_data_processed[\"text\"] += list_text\n",
        "                page_data_processed[\"lines\"] += \"{}\\t{}\\n\".format(item, list_text)\n",
        "            else:  # All alternatives must be handled and the code must not reach here\n",
        "                raise ValueError(\"Unidentified page element found.\")\n",
        "\n",
        "        page_data_processed[\"text\"] = final_clean(page_data_processed[\"text\"])\n",
        "        page_data_processed[\"lines\"] = final_clean(page_data_processed[\"lines\"])\n",
        "\n",
        "        # Appending the processed page data to the file\n",
        "        with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
        "            json.dump(page_data_processed, f, ensure_ascii=False)\n",
        "            f.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 file(s) will be processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkZAGTDasS3m"
      },
      "source": [
        "!sh target/appassembler/bin/IndexCollection -collection FeverParagraphCollection \\\n",
        " -input wiki_pages_anserini \\\n",
        " -index indexes/fever/lucene-index-fever-paragraph \\\n",
        " -generator DefaultLuceneDocumentGenerator \\\n",
        " -threads 1 -storePositions -storeDocvectors -storeRaw \\\n",
        "  >& logs/log.fever &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3jcK9thJgKo"
      },
      "source": [
        "!tar -cvf \"/content/anserini/indexes/fever/lucene-index-fever-paragraph.tar\" \"/content/anserini/indexes/fever/lucene-index-fever-paragraph\" #Tar operation for exporting the index and make it reusable\n",
        "# Only one indexing will be enough we will use the tar file after that use the code below to untar it and equate to the LucuneDir\n",
        "#!tar xvfz lucene-index-fever-paragraph.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzHeiKMIWjLk"
      },
      "source": [
        "luceneDir = 'indexes/fever/lucene-index-fever-paragraph'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8zhAZVwfAmm"
      },
      "source": [
        "USE_SUMMARY = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM5KN40Peyit",
        "outputId": "2b4e6273-0455-4dd3-dfce-033ea39ac9c2"
      },
      "source": [
        "import os\n",
        "#%%capture\n",
        "!curl -O https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz\n",
        "!mv openjdk-11.0.2_linux-x64_bin.tar.gz /usr/lib/jvm/; cd /usr/lib/jvm/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n",
        "!update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-11.0.2/bin/java 1\n",
        "!update-alternatives --set java /usr/lib/jvm/jdk-11.0.2/bin/java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/jdk-11.0.2\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  178M  100  178M    0     0   111M      0  0:00:01  0:00:01 --:--:--  111M\n",
            "jdk-11.0.2/bin/jaotc\n",
            "jdk-11.0.2/bin/jar\n",
            "jdk-11.0.2/bin/jarsigner\n",
            "jdk-11.0.2/bin/java\n",
            "jdk-11.0.2/bin/javac\n",
            "jdk-11.0.2/bin/javadoc\n",
            "jdk-11.0.2/bin/javap\n",
            "jdk-11.0.2/bin/jcmd\n",
            "jdk-11.0.2/bin/jconsole\n",
            "jdk-11.0.2/bin/jdb\n",
            "jdk-11.0.2/bin/jdeprscan\n",
            "jdk-11.0.2/bin/jdeps\n",
            "jdk-11.0.2/bin/jhsdb\n",
            "jdk-11.0.2/bin/jimage\n",
            "jdk-11.0.2/bin/jinfo\n",
            "jdk-11.0.2/bin/jjs\n",
            "jdk-11.0.2/bin/jlink\n",
            "jdk-11.0.2/bin/jmap\n",
            "jdk-11.0.2/bin/jmod\n",
            "jdk-11.0.2/bin/jps\n",
            "jdk-11.0.2/bin/jrunscript\n",
            "jdk-11.0.2/bin/jshell\n",
            "jdk-11.0.2/bin/jstack\n",
            "jdk-11.0.2/bin/jstat\n",
            "jdk-11.0.2/bin/jstatd\n",
            "jdk-11.0.2/bin/keytool\n",
            "jdk-11.0.2/bin/pack200\n",
            "jdk-11.0.2/bin/rmic\n",
            "jdk-11.0.2/bin/rmid\n",
            "jdk-11.0.2/bin/rmiregistry\n",
            "jdk-11.0.2/bin/serialver\n",
            "jdk-11.0.2/bin/unpack200\n",
            "jdk-11.0.2/conf/logging.properties\n",
            "jdk-11.0.2/conf/management/jmxremote.access\n",
            "jdk-11.0.2/conf/management/jmxremote.password.template\n",
            "jdk-11.0.2/conf/management/management.properties\n",
            "jdk-11.0.2/conf/net.properties\n",
            "jdk-11.0.2/conf/security/java.policy\n",
            "jdk-11.0.2/conf/security/java.security\n",
            "jdk-11.0.2/conf/security/policy/README.txt\n",
            "jdk-11.0.2/conf/security/policy/limited/default_US_export.policy\n",
            "jdk-11.0.2/conf/security/policy/limited/default_local.policy\n",
            "jdk-11.0.2/conf/security/policy/limited/exempt_local.policy\n",
            "jdk-11.0.2/conf/security/policy/unlimited/default_US_export.policy\n",
            "jdk-11.0.2/conf/security/policy/unlimited/default_local.policy\n",
            "jdk-11.0.2/conf/sound.properties\n",
            "jdk-11.0.2/include/classfile_constants.h\n",
            "jdk-11.0.2/include/jawt.h\n",
            "jdk-11.0.2/include/jdwpTransport.h\n",
            "jdk-11.0.2/include/jni.h\n",
            "jdk-11.0.2/include/jvmti.h\n",
            "jdk-11.0.2/include/jvmticmlr.h\n",
            "jdk-11.0.2/include/linux/jawt_md.h\n",
            "jdk-11.0.2/include/linux/jni_md.h\n",
            "jdk-11.0.2/jmods/java.base.jmod\n",
            "jdk-11.0.2/jmods/java.compiler.jmod\n",
            "jdk-11.0.2/jmods/java.datatransfer.jmod\n",
            "jdk-11.0.2/jmods/java.desktop.jmod\n",
            "jdk-11.0.2/jmods/java.instrument.jmod\n",
            "jdk-11.0.2/jmods/java.logging.jmod\n",
            "jdk-11.0.2/jmods/java.management.jmod\n",
            "jdk-11.0.2/jmods/java.management.rmi.jmod\n",
            "jdk-11.0.2/jmods/java.naming.jmod\n",
            "jdk-11.0.2/jmods/java.net.http.jmod\n",
            "jdk-11.0.2/jmods/java.prefs.jmod\n",
            "jdk-11.0.2/jmods/java.rmi.jmod\n",
            "jdk-11.0.2/jmods/java.scripting.jmod\n",
            "jdk-11.0.2/jmods/java.se.jmod\n",
            "jdk-11.0.2/jmods/java.security.jgss.jmod\n",
            "jdk-11.0.2/jmods/java.security.sasl.jmod\n",
            "jdk-11.0.2/jmods/java.smartcardio.jmod\n",
            "jdk-11.0.2/jmods/java.sql.jmod\n",
            "jdk-11.0.2/jmods/java.sql.rowset.jmod\n",
            "jdk-11.0.2/jmods/java.transaction.xa.jmod\n",
            "jdk-11.0.2/jmods/java.xml.crypto.jmod\n",
            "jdk-11.0.2/jmods/java.xml.jmod\n",
            "jdk-11.0.2/jmods/jdk.accessibility.jmod\n",
            "jdk-11.0.2/jmods/jdk.aot.jmod\n",
            "jdk-11.0.2/jmods/jdk.attach.jmod\n",
            "jdk-11.0.2/jmods/jdk.charsets.jmod\n",
            "jdk-11.0.2/jmods/jdk.compiler.jmod\n",
            "jdk-11.0.2/jmods/jdk.crypto.cryptoki.jmod\n",
            "jdk-11.0.2/jmods/jdk.crypto.ec.jmod\n",
            "jdk-11.0.2/jmods/jdk.dynalink.jmod\n",
            "jdk-11.0.2/jmods/jdk.editpad.jmod\n",
            "jdk-11.0.2/jmods/jdk.hotspot.agent.jmod\n",
            "jdk-11.0.2/jmods/jdk.httpserver.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.ed.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.jvmstat.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.le.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.opt.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.vm.ci.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.vm.compiler.jmod\n",
            "jdk-11.0.2/jmods/jdk.internal.vm.compiler.management.jmod\n",
            "jdk-11.0.2/jmods/jdk.jartool.jmod\n",
            "jdk-11.0.2/jmods/jdk.javadoc.jmod\n",
            "jdk-11.0.2/jmods/jdk.jcmd.jmod\n",
            "jdk-11.0.2/jmods/jdk.jconsole.jmod\n",
            "jdk-11.0.2/jmods/jdk.jdeps.jmod\n",
            "jdk-11.0.2/jmods/jdk.jdi.jmod\n",
            "jdk-11.0.2/jmods/jdk.jdwp.agent.jmod\n",
            "jdk-11.0.2/jmods/jdk.jfr.jmod\n",
            "jdk-11.0.2/jmods/jdk.jlink.jmod\n",
            "jdk-11.0.2/jmods/jdk.jshell.jmod\n",
            "jdk-11.0.2/jmods/jdk.jsobject.jmod\n",
            "jdk-11.0.2/jmods/jdk.jstatd.jmod\n",
            "jdk-11.0.2/jmods/jdk.localedata.jmod\n",
            "jdk-11.0.2/jmods/jdk.management.agent.jmod\n",
            "jdk-11.0.2/jmods/jdk.management.jfr.jmod\n",
            "jdk-11.0.2/jmods/jdk.management.jmod\n",
            "jdk-11.0.2/jmods/jdk.naming.dns.jmod\n",
            "jdk-11.0.2/jmods/jdk.naming.rmi.jmod\n",
            "jdk-11.0.2/jmods/jdk.net.jmod\n",
            "jdk-11.0.2/jmods/jdk.pack.jmod\n",
            "jdk-11.0.2/jmods/jdk.rmic.jmod\n",
            "jdk-11.0.2/jmods/jdk.scripting.nashorn.jmod\n",
            "jdk-11.0.2/jmods/jdk.scripting.nashorn.shell.jmod\n",
            "jdk-11.0.2/jmods/jdk.sctp.jmod\n",
            "jdk-11.0.2/jmods/jdk.security.auth.jmod\n",
            "jdk-11.0.2/jmods/jdk.security.jgss.jmod\n",
            "jdk-11.0.2/jmods/jdk.unsupported.desktop.jmod\n",
            "jdk-11.0.2/jmods/jdk.unsupported.jmod\n",
            "jdk-11.0.2/jmods/jdk.xml.dom.jmod\n",
            "jdk-11.0.2/jmods/jdk.zipfs.jmod\n",
            "jdk-11.0.2/legal/java.base/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.base/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.base/LICENSE\n",
            "jdk-11.0.2/legal/java.base/aes.md\n",
            "jdk-11.0.2/legal/java.base/asm.md\n",
            "jdk-11.0.2/legal/java.base/c-libutl.md\n",
            "jdk-11.0.2/legal/java.base/cldr.md\n",
            "jdk-11.0.2/legal/java.base/icu.md\n",
            "jdk-11.0.2/legal/java.base/public_suffix.md\n",
            "jdk-11.0.2/legal/java.base/unicode.md\n",
            "jdk-11.0.2/legal/java.compiler/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.compiler/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.compiler/LICENSE\n",
            "jdk-11.0.2/legal/java.datatransfer/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.datatransfer/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.datatransfer/LICENSE\n",
            "jdk-11.0.2/legal/java.desktop/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.desktop/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.desktop/LICENSE\n",
            "jdk-11.0.2/legal/java.desktop/colorimaging.md\n",
            "jdk-11.0.2/legal/java.desktop/giflib.md\n",
            "jdk-11.0.2/legal/java.desktop/harfbuzz.md\n",
            "jdk-11.0.2/legal/java.desktop/jpeg.md\n",
            "jdk-11.0.2/legal/java.desktop/lcms.md\n",
            "jdk-11.0.2/legal/java.desktop/libpng.md\n",
            "jdk-11.0.2/legal/java.desktop/mesa3d.md\n",
            "jdk-11.0.2/legal/java.desktop/opengl.md\n",
            "jdk-11.0.2/legal/java.desktop/xwindows.md\n",
            "jdk-11.0.2/legal/java.instrument/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.instrument/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.instrument/LICENSE\n",
            "jdk-11.0.2/legal/java.logging/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.logging/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.logging/LICENSE\n",
            "jdk-11.0.2/legal/java.management.rmi/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.management.rmi/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.management.rmi/LICENSE\n",
            "jdk-11.0.2/legal/java.management/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.management/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.management/LICENSE\n",
            "jdk-11.0.2/legal/java.naming/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.naming/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.naming/LICENSE\n",
            "jdk-11.0.2/legal/java.net.http/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.net.http/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.net.http/LICENSE\n",
            "jdk-11.0.2/legal/java.prefs/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.prefs/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.prefs/LICENSE\n",
            "jdk-11.0.2/legal/java.rmi/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.rmi/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.rmi/LICENSE\n",
            "jdk-11.0.2/legal/java.scripting/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.scripting/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.scripting/LICENSE\n",
            "jdk-11.0.2/legal/java.se/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.se/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.se/LICENSE\n",
            "jdk-11.0.2/legal/java.security.jgss/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.security.jgss/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.security.jgss/LICENSE\n",
            "jdk-11.0.2/legal/java.security.sasl/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.security.sasl/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.security.sasl/LICENSE\n",
            "jdk-11.0.2/legal/java.smartcardio/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.smartcardio/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.smartcardio/LICENSE\n",
            "jdk-11.0.2/legal/java.smartcardio/pcsclite.md\n",
            "jdk-11.0.2/legal/java.sql.rowset/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.sql.rowset/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.sql.rowset/LICENSE\n",
            "jdk-11.0.2/legal/java.sql/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.sql/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.sql/LICENSE\n",
            "jdk-11.0.2/legal/java.transaction.xa/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.transaction.xa/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.transaction.xa/LICENSE\n",
            "jdk-11.0.2/legal/java.xml.crypto/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.xml.crypto/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.xml.crypto/LICENSE\n",
            "jdk-11.0.2/legal/java.xml.crypto/santuario.md\n",
            "jdk-11.0.2/legal/java.xml/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/java.xml/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/java.xml/LICENSE\n",
            "jdk-11.0.2/legal/java.xml/bcel.md\n",
            "jdk-11.0.2/legal/java.xml/dom.md\n",
            "jdk-11.0.2/legal/java.xml/jcup.md\n",
            "jdk-11.0.2/legal/java.xml/xalan.md\n",
            "jdk-11.0.2/legal/java.xml/xerces.md\n",
            "jdk-11.0.2/legal/jdk.accessibility/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.accessibility/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.accessibility/LICENSE\n",
            "jdk-11.0.2/legal/jdk.aot/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.aot/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.aot/LICENSE\n",
            "jdk-11.0.2/legal/jdk.attach/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.attach/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.attach/LICENSE\n",
            "jdk-11.0.2/legal/jdk.charsets/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.charsets/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.charsets/LICENSE\n",
            "jdk-11.0.2/legal/jdk.compiler/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.compiler/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.compiler/LICENSE\n",
            "jdk-11.0.2/legal/jdk.crypto.cryptoki/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.crypto.cryptoki/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.crypto.cryptoki/LICENSE\n",
            "jdk-11.0.2/legal/jdk.crypto.cryptoki/pkcs11cryptotoken.md\n",
            "jdk-11.0.2/legal/jdk.crypto.cryptoki/pkcs11wrapper.md\n",
            "jdk-11.0.2/legal/jdk.crypto.ec/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.crypto.ec/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.crypto.ec/LICENSE\n",
            "jdk-11.0.2/legal/jdk.crypto.ec/ecc.md\n",
            "jdk-11.0.2/legal/jdk.dynalink/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.dynalink/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.dynalink/LICENSE\n",
            "jdk-11.0.2/legal/jdk.dynalink/dynalink.md\n",
            "jdk-11.0.2/legal/jdk.editpad/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.editpad/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.editpad/LICENSE\n",
            "jdk-11.0.2/legal/jdk.hotspot.agent/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.hotspot.agent/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.hotspot.agent/LICENSE\n",
            "jdk-11.0.2/legal/jdk.httpserver/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.httpserver/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.httpserver/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.ed/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.ed/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.ed/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.jvmstat/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.jvmstat/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.jvmstat/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.le/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.le/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.le/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.le/jline.md\n",
            "jdk-11.0.2/legal/jdk.internal.opt/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.opt/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.opt/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.opt/jopt-simple.md\n",
            "jdk-11.0.2/legal/jdk.internal.vm.ci/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.vm.ci/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.vm.ci/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.vm.compiler.management/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.vm.compiler.management/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.vm.compiler.management/LICENSE\n",
            "jdk-11.0.2/legal/jdk.internal.vm.compiler/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.internal.vm.compiler/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.internal.vm.compiler/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jartool/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jartool/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jartool/LICENSE\n",
            "jdk-11.0.2/legal/jdk.javadoc/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.javadoc/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.javadoc/LICENSE\n",
            "jdk-11.0.2/legal/jdk.javadoc/jquery-migrate.md\n",
            "jdk-11.0.2/legal/jdk.javadoc/jquery.md\n",
            "jdk-11.0.2/legal/jdk.javadoc/jqueryUI.md\n",
            "jdk-11.0.2/legal/jdk.javadoc/jszip.md\n",
            "jdk-11.0.2/legal/jdk.javadoc/pako.md\n",
            "jdk-11.0.2/legal/jdk.jcmd/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jcmd/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jcmd/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jconsole/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jconsole/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jconsole/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jdeps/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jdeps/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jdeps/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jdi/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jdi/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jdi/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jdwp.agent/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jdwp.agent/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jdwp.agent/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jfr/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jfr/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jfr/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jlink/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jlink/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jlink/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jshell/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jshell/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jshell/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jsobject/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jsobject/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jsobject/LICENSE\n",
            "jdk-11.0.2/legal/jdk.jstatd/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.jstatd/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.jstatd/LICENSE\n",
            "jdk-11.0.2/legal/jdk.localedata/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.localedata/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.localedata/LICENSE\n",
            "jdk-11.0.2/legal/jdk.localedata/cldr.md\n",
            "jdk-11.0.2/legal/jdk.localedata/thaidict.md\n",
            "jdk-11.0.2/legal/jdk.management.agent/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.management.agent/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.management.agent/LICENSE\n",
            "jdk-11.0.2/legal/jdk.management.jfr/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.management.jfr/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.management.jfr/LICENSE\n",
            "jdk-11.0.2/legal/jdk.management/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.management/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.management/LICENSE\n",
            "jdk-11.0.2/legal/jdk.naming.dns/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.naming.dns/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.naming.dns/LICENSE\n",
            "jdk-11.0.2/legal/jdk.naming.rmi/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.naming.rmi/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.naming.rmi/LICENSE\n",
            "jdk-11.0.2/legal/jdk.net/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.net/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.net/LICENSE\n",
            "jdk-11.0.2/legal/jdk.pack/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.pack/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.pack/LICENSE\n",
            "jdk-11.0.2/legal/jdk.rmic/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.rmic/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.rmic/LICENSE\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn.shell/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn.shell/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn.shell/LICENSE\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn/LICENSE\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn/double-conversion.md\n",
            "jdk-11.0.2/legal/jdk.scripting.nashorn/joni.md\n",
            "jdk-11.0.2/legal/jdk.sctp/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.sctp/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.sctp/LICENSE\n",
            "jdk-11.0.2/legal/jdk.security.auth/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.security.auth/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.security.auth/LICENSE\n",
            "jdk-11.0.2/legal/jdk.security.jgss/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.security.jgss/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.security.jgss/LICENSE\n",
            "jdk-11.0.2/legal/jdk.unsupported.desktop/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.unsupported.desktop/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.unsupported.desktop/LICENSE\n",
            "jdk-11.0.2/legal/jdk.unsupported/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.unsupported/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.unsupported/LICENSE\n",
            "jdk-11.0.2/legal/jdk.xml.dom/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.xml.dom/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.xml.dom/LICENSE\n",
            "jdk-11.0.2/legal/jdk.zipfs/ADDITIONAL_LICENSE_INFO\n",
            "jdk-11.0.2/legal/jdk.zipfs/ASSEMBLY_EXCEPTION\n",
            "jdk-11.0.2/legal/jdk.zipfs/LICENSE\n",
            "jdk-11.0.2/lib/classlist\n",
            "jdk-11.0.2/lib/ct.sym\n",
            "jdk-11.0.2/lib/jexec\n",
            "jdk-11.0.2/lib/jfr/default.jfc\n",
            "jdk-11.0.2/lib/jfr/profile.jfc\n",
            "jdk-11.0.2/lib/jli/libjli.so\n",
            "jdk-11.0.2/lib/jrt-fs.jar\n",
            "jdk-11.0.2/lib/jvm.cfg\n",
            "jdk-11.0.2/lib/libattach.so\n",
            "jdk-11.0.2/lib/libawt.so\n",
            "jdk-11.0.2/lib/libawt_headless.so\n",
            "jdk-11.0.2/lib/libawt_xawt.so\n",
            "jdk-11.0.2/lib/libdt_socket.so\n",
            "jdk-11.0.2/lib/libextnet.so\n",
            "jdk-11.0.2/lib/libfontmanager.so\n",
            "jdk-11.0.2/lib/libinstrument.so\n",
            "jdk-11.0.2/lib/libj2gss.so\n",
            "jdk-11.0.2/lib/libj2pcsc.so\n",
            "jdk-11.0.2/lib/libj2pkcs11.so\n",
            "jdk-11.0.2/lib/libjaas.so\n",
            "jdk-11.0.2/lib/libjava.so\n",
            "jdk-11.0.2/lib/libjavajpeg.so\n",
            "jdk-11.0.2/lib/libjawt.so\n",
            "jdk-11.0.2/lib/libjdwp.so\n",
            "jdk-11.0.2/lib/libjimage.so\n",
            "jdk-11.0.2/lib/libjsig.so\n",
            "jdk-11.0.2/lib/libjsound.so\n",
            "jdk-11.0.2/lib/liblcms.so\n",
            "jdk-11.0.2/lib/libmanagement.so\n",
            "jdk-11.0.2/lib/libmanagement_agent.so\n",
            "jdk-11.0.2/lib/libmanagement_ext.so\n",
            "jdk-11.0.2/lib/libmlib_image.so\n",
            "jdk-11.0.2/lib/libnet.so\n",
            "jdk-11.0.2/lib/libnio.so\n",
            "jdk-11.0.2/lib/libprefs.so\n",
            "jdk-11.0.2/lib/librmi.so\n",
            "jdk-11.0.2/lib/libsaproc.so\n",
            "jdk-11.0.2/lib/libsctp.so\n",
            "jdk-11.0.2/lib/libsplashscreen.so\n",
            "jdk-11.0.2/lib/libsunec.so\n",
            "jdk-11.0.2/lib/libunpack.so\n",
            "jdk-11.0.2/lib/libverify.so\n",
            "jdk-11.0.2/lib/libzip.so\n",
            "jdk-11.0.2/lib/modules\n",
            "jdk-11.0.2/lib/psfont.properties.ja\n",
            "jdk-11.0.2/lib/psfontj2d.properties\n",
            "jdk-11.0.2/lib/security/blacklisted.certs\n",
            "jdk-11.0.2/lib/security/cacerts\n",
            "jdk-11.0.2/lib/security/default.policy\n",
            "jdk-11.0.2/lib/security/public_suffix_list.dat\n",
            "jdk-11.0.2/lib/server/Xusage.txt\n",
            "jdk-11.0.2/lib/server/libjsig.so\n",
            "jdk-11.0.2/lib/server/libjvm.so\n",
            "jdk-11.0.2/lib/src.zip\n",
            "jdk-11.0.2/lib/tzdb.dat\n",
            "jdk-11.0.2/release\n",
            "update-alternatives: using /usr/lib/jvm/jdk-11.0.2/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-U3oN2Ge3Tk",
        "outputId": "1f49fcde-7e38-4892-df4b-d09e520dad45"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "!mkdir ~/kaggle\n",
        "!mkdir ~/kaggle/working/\n",
        "!mkdir ~/kaggle/working/sentence_wise_email/\n",
        "!mkdir ~/kaggle/working/sentence_wise_email/module/\n",
        "!mkdir ~/kaggle/working/sentence_wise_email/module/module_useT\n",
        "# Download the module, and uncompress it to the destination folder. \n",
        "!curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed\" | tar -zxvC ~/kaggle/working/sentence_wise_email/module/module_useT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "./\n",
            "./tfhub_module.pb\n",
            "./variables/\n",
            "./variables/variables.data-00000-of-00001\n",
            " 99  745M   99  742M    0     0  43.2M      0  0:00:17  0:00:17 --:--:-- 46.2M./variables/variables.index\n",
            "./assets/\n",
            "./saved_model.pb\n",
            "100  745M  100  745M    0     0  43.2M      0  0:00:17  0:00:17 --:--:-- 46.9M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSVu4N7Pe6OK",
        "outputId": "9220b86d-5053-4c31-f4ea-fff4f899bdc2"
      },
      "source": [
        "import torch\n",
        "!pip install transformers\n",
        "!pip install --no-cache-dir transformers sentencepiece\n",
        "#from transformers import BertForQuestionAnswering\n",
        "#from transformers import BertTokenizer\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "torch_device = 'cuda'\n",
        "\n",
        "#dmis-lab/biobert-base-cased-v1.1-squad\n",
        "#abhijithneilabraham/longformer_covid_qa\n",
        "#graviraja/covidbert_squad \n",
        "\n",
        "QA_TOKENIZER = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1-squad\")\n",
        "QA_MODEL = AutoModelForQuestionAnswering.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1-squad\")\n",
        "\n",
        "#QA_MODEL = AutoModelForQuestionAnswering.from_pretrained(\"NeuML/bert-small-cord19qa\")\n",
        "#QA_TOKENIZER = AutoTokenizer.from_pretrained(\"NeuML/bert-small-cord19qa\")\n",
        "\n",
        "#QA_MODEL = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "#QA_TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "QA_MODEL.to(torch_device)\n",
        "QA_MODEL.eval()\n",
        "#Auto Summarization with BART\n",
        "if USE_SUMMARY:\n",
        "    SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "    SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "    SUMMARY_MODEL.to(torch_device)\n",
        "    SUMMARY_MODEL.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BAh4RL0RecR"
      },
      "source": [
        "#%%capture\n",
        "!pip install pyserini==0.8.1.0\n",
        "from pyserini.search import pysearch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjZfCGUbXYDI"
      },
      "source": [
        "query_processed = 'As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?'\n",
        "query = 'As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?'\n",
        "\n",
        "keywords = '2019-nCoV, SARS-CoV-2, COVID-19, symptoms, hospitalization'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TElPXtbMXKaI"
      },
      "source": [
        "import json\n",
        "\n",
        "searcher = pysearch.SimpleSearcher(luceneDir)\n",
        "hits = searcher.search(query_processed + '. ' + keywords,k=50) # We can retrieve the documents according to concatanated string of query and keywords we can change this setup.\n",
        "n_hits = len(hits)\n",
        "## collect the relevant data in a hit dictionary\n",
        "hit_dictionary = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0lPvHURepbj"
      },
      "source": [
        "import json\n",
        "#'As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?'\n",
        "\n",
        "searcher = pysearch.SimpleSearcher(luceneDir)\n",
        "hits = searcher.search(query_processed + '. ' + keywords,k=50)\n",
        "n_hits = len(hits)\n",
        "## collect the relevant data in a hit dictionary\n",
        "hit_dictionary = {}\n",
        "for i in range(0, n_hits):\n",
        "  if 1==1:\n",
        "    doc_json=dict()\n",
        "    idx = str(hits[i].docid)\n",
        "    hit_dictionary[idx] = doc_json\n",
        "    hit_dictionary[idx]['abstract']=hits[i].lucene_document.get(\"raw\")\n",
        "    hit_dictionary[idx]['real_abstract']=hits[i].lucene_document.get(\"raw\")\n",
        "    hit_dictionary[idx]['title'] = str(hits[i].docid)\n",
        "\n",
        "## scrub the abstracts in prep for BERT-SQuAD\n",
        "for idx,v in hit_dictionary.items():\n",
        "    abs_dirty = v['abstract']\n",
        "    real_abs_dirty = v['real_abstract']\n",
        "    #abs_dirty = v['paragraph']\n",
        "    # looks like the abstract value can be an empty list\n",
        "    v['abstract_paragraphs'] = []\n",
        "    v['abstract_full'] = ''\n",
        "    v['real_abstract_full'] = ''\n",
        "    v['real_abstract_paragraphs']=[]\n",
        "\n",
        "    if abs_dirty:\n",
        "        # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
        "        # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
        "        # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
        "\n",
        "\n",
        "        if isinstance(abs_dirty, list):\n",
        "            for p in abs_dirty:\n",
        "                v['abstract_paragraphs'].append(p['text'])\n",
        "                v['abstract_full'] += p['text'] + ' \\n\\n'\n",
        "\n",
        "        # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
        "        if isinstance(abs_dirty, str):\n",
        "            v['abstract_paragraphs'].append(abs_dirty)\n",
        "            v['abstract_full'] += abs_dirty + ' \\n\\n'\n",
        "    if real_abs_dirty:\n",
        "        # looks like if it is a list, then the only entry is a dictionary wher text is in 'text' key\n",
        "        # looks like it is broken up by paragraph if it is in that form.  lets make lists for every paragraph\n",
        "        # and a new entry that is full abstract text as both could be valuable for BERT derrived QA\n",
        "\n",
        "\n",
        "        if isinstance(real_abs_dirty, list):\n",
        "            for p in real_abs_dirty:\n",
        "                v['real_abstract_paragraphs'].append(p['text'])\n",
        "                v['real_abstract_full'] += p['text'] + ' \\n\\n'\n",
        "\n",
        "        # looks like in some cases the abstract can be straight up text so we can actually leave that alone\n",
        "        if isinstance(abs_dirty, str):\n",
        "            v['real_abstract_paragraphs'].append(real_abs_dirty)\n",
        "            v['real_abstract_full'] += real_abs_dirty + ' \\n\\n'\n",
        "    v[\"indexed_para\"]=v[\"abstract_full\"][len(v[\"real_abstract_full\"]):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TZVSOX4XiFw",
        "outputId": "bc2aeaa0-1543-4862-a3a1-ffc2537b9926"
      },
      "source": [
        "def embed_useT(module):\n",
        "    with tf.Graph().as_default():\n",
        "        sentences = tf.compat.v1.placeholder(tf.string)\n",
        "        embed = hub.Module(module)\n",
        "        embeddings = embed(sentences)\n",
        "        session = tf.compat.v1.train.MonitoredSession()\n",
        "    return lambda x: session.run(embeddings, {sentences: x})\n",
        "embed_fn = embed_useT('/root/kaggle/working/sentence_wise_email/module/module_useT')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ22nEB1rYdw"
      },
      "source": [
        "import numpy as np\n",
        "#Reconstruct text for generating the answer text from the BERT result, do the necessary text preprocessing and construct the answer.\n",
        "def reconstructText(tokens, start=0, stop=-1):\n",
        "    tokens = tokens[start: stop]\n",
        "    if '[SEP]' in tokens:\n",
        "        sepind = tokens.index('[SEP]')\n",
        "        tokens = tokens[sepind+1:]\n",
        "    txt = ' '.join(tokens)\n",
        "    txt = txt.replace(' ##', '')\n",
        "    txt = txt.replace('##', '')\n",
        "    txt = txt.strip()\n",
        "    txt = \" \".join(txt.split())\n",
        "    txt = txt.replace(' .', '.')\n",
        "    txt = txt.replace('( ', '(')\n",
        "    txt = txt.replace(' )', ')')\n",
        "    txt = txt.replace(' - ', '-')\n",
        "    txt_list = txt.split(' , ')\n",
        "    txt = ''\n",
        "    nTxtL = len(txt_list)\n",
        "    if nTxtL == 1:\n",
        "        return txt_list[0]\n",
        "    newList =[]\n",
        "    for i,t in enumerate(txt_list):\n",
        "        if i < nTxtL -1:\n",
        "            if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n",
        "                newList += [t,',']\n",
        "            else:\n",
        "                newList += [t, ', ']\n",
        "        else:\n",
        "            newList += [t]\n",
        "    return ''.join(newList)\n",
        "\n",
        "\n",
        "def makeBERTSQuADPrediction(document, question):\n",
        "    ## we need to rewrite this function so that it chuncks the document into 250-300 word segments with\n",
        "    ## 50 word overlaps on either end so that it can understand and check longer abstracts\n",
        "    ## Get chuck of documents in order to overcome the BERT's max 512 token problem. First count the number of tokens. For example if it is\n",
        "    ##   larger than 2048 token determine the split size and overlapping token count. Split the document into pieces to fed into BERT encoding. Overlapping\n",
        "    ##      is necessary to maintain the coherence between the splits. If the token count is larger than some number 2560 token approximately this will fail.\n",
        "    nWords = len(document.split())\n",
        "    input_ids_all = QA_TOKENIZER.encode(question, document)\n",
        "    tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n",
        "    overlapFac = 1.1\n",
        "    if len(input_ids_all)*overlapFac > 2560:\n",
        "        nSearchWords = int(np.ceil(nWords/6))\n",
        "        fifth = int(np.ceil(nWords/5))\n",
        "        docSplit = document.split()\n",
        "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
        "                     ' '.join(docSplit[fifth-int(nSearchWords*overlapFac/2):fifth+int(fifth*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[fifth*2-int(nSearchWords*overlapFac/2):fifth*2+int(fifth*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[fifth*3-int(nSearchWords*overlapFac/2):fifth*3+int(fifth*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[fifth*4-int(nSearchWords*overlapFac/2):fifth*4+int(fifth*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
        "\n",
        "    elif len(input_ids_all)*overlapFac > 2048:\n",
        "        nSearchWords = int(np.ceil(nWords/5))\n",
        "        quarter = int(np.ceil(nWords/4))\n",
        "        docSplit = document.split()\n",
        "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
        "                     ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
        "        \n",
        "    elif len(input_ids_all)*overlapFac > 1536:\n",
        "        nSearchWords = int(np.ceil(nWords/4))\n",
        "        third = int(np.ceil(nWords/3))\n",
        "        docSplit = document.split()\n",
        "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
        "                     ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]        \n",
        "        \n",
        "    elif len(input_ids_all)*overlapFac > 1024:\n",
        "        nSearchWords = int(np.ceil(nWords/3))\n",
        "        middle = int(np.ceil(nWords/2))\n",
        "        docSplit = document.split()\n",
        "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
        "                     ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),\n",
        "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
        "    elif len(input_ids_all)*overlapFac > 512:\n",
        "        nSearchWords = int(np.ceil(nWords/2))\n",
        "        docSplit = document.split()\n",
        "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
        "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
        "    else:\n",
        "        input_ids = [input_ids_all]\n",
        "    absTooLong = False    \n",
        "    \n",
        "    answers = []\n",
        "    cons = []\n",
        "    #print(input_ids)\n",
        "    for iptIds in input_ids:\n",
        "        tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n",
        "        #print(tokens)\n",
        "        sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)\n",
        "        num_seg_a = sep_index + 1\n",
        "        num_seg_b = len(iptIds) - num_seg_a\n",
        "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "        assert len(segment_ids) == len(iptIds)\n",
        "        n_ids = len(segment_ids)\n",
        "        #print(n_ids)\n",
        "        if n_ids < 512:\n",
        "            outputs = QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
        "                                     token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n",
        "            #print(QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
        "             #                        token_type_ids=torch.tensor([segment_ids]).to(torch_device)))\n",
        "        else:\n",
        "            #this cuts off the text if its more than 512 words so it fits in model space\n",
        "            #need run multiple inferences for longer text. add to the todo. I will try to expand this to longer sequences if it is necessary. \n",
        "            print('****** warning only considering first 512 tokens, document is '+str(nWords)+' words long.  There are '+str(n_ids)+ ' tokens')\n",
        "            absTooLong = True\n",
        "            outputs= QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device), \n",
        "                                     token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))\n",
        "        start_scores=outputs.start_logits\n",
        "        end_scores=outputs.end_logits\n",
        "        start_scores = start_scores[:,1:-1]\n",
        "        end_scores = end_scores[:,1:-1]\n",
        "        answer_start = torch.argmax(start_scores)\n",
        "        answer_end = torch.argmax(end_scores)\n",
        "        #print(answer_start, answer_end)\n",
        "        answer = reconstructText(tokens, answer_start, answer_end+2)\n",
        "    \n",
        "        if answer.startswith('. ') or answer.startswith(', '):\n",
        "            answer = answer[2:]\n",
        "            \n",
        "        c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n",
        "        answers.append(answer)\n",
        "        cons.append(c)\n",
        "    \n",
        "    maxC = max(cons)\n",
        "    iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n",
        "    confidence = cons[iMaxC]\n",
        "    answer = answers[iMaxC]\n",
        "    \n",
        "    sep_index = tokens_all.index('[SEP]')\n",
        "    full_txt_tokens = tokens_all[sep_index+1:]\n",
        "    \n",
        "    abs_returned = reconstructText(full_txt_tokens)\n",
        "\n",
        "    ans={}\n",
        "    ans['answer'] = answer\n",
        "    #print(answer)\n",
        "    if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n",
        "        ans['confidence'] = -1000000\n",
        "    else:\n",
        "        #confidence = torch.max(start_scores) + torch.max(end_scores)\n",
        "        #confidence = np.log(confidence.item())\n",
        "        ans['confidence'] = confidence\n",
        "    #ans['start'] = answer_start.item()\n",
        "    #ans['end'] = answer_end.item()\n",
        "    ans['abstract_bert'] = abs_returned\n",
        "    ans['abs_too_long'] = absTooLong\n",
        "    return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKNQBMURrbv9"
      },
      "source": [
        "from tqdm import tqdm\n",
        "def searchAbstracts(hit_dictionary, question):\n",
        "    abstractResults = {}\n",
        "    for k,v in tqdm(hit_dictionary.items()):\n",
        "        abstract = v['abstract_full']\n",
        "        indexed_para=v['indexed_para']\n",
        "        if abstract:\n",
        "            ans = makeBERTSQuADPrediction(abstract, question)\n",
        "            if ans['answer']:\n",
        "                confidence = ans['confidence']\n",
        "                abstractResults[confidence]={}\n",
        "                abstractResults[confidence]['answer'] = ans['answer']\n",
        "                #abstractResults[confidence]['start'] = ans['start']\n",
        "                #abstractResults[confidence]['end'] = ans['end']\n",
        "                abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n",
        "                abstractResults[confidence]['idx'] = k\n",
        "                abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n",
        "\n",
        "                \n",
        "    cList = list(abstractResults.keys())\n",
        "\n",
        "    if cList:\n",
        "        maxScore = max(cList)\n",
        "        total = 0.0\n",
        "        exp_scores = []\n",
        "        for c in cList:\n",
        "            s = np.exp(c-maxScore)\n",
        "            exp_scores.append(s)\n",
        "        total = sum(exp_scores)\n",
        "        for i,c in enumerate(cList):\n",
        "            abstractResults[exp_scores[i]/total] = abstractResults.pop(c)\n",
        "    return abstractResults"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZsPI-2PreQC",
        "outputId": "4eac8dde-0cf0-4f2b-e454-d7a2d73330fa"
      },
      "source": [
        "answers = searchAbstracts(hit_dictionary, query)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 3699 words long.  There are 1526 tokens\n",
            "****** warning only considering first 512 tokens, document is 3699 words long.  There are 1706 tokens\n",
            "****** warning only considering first 512 tokens, document is 3699 words long.  There are 1361 tokens\n",
            "****** warning only considering first 512 tokens, document is 3699 words long.  There are 1641 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 1/50 [00:00<00:18,  2.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 3699 words long.  There are 1750 tokens\n",
            "****** warning only considering first 512 tokens, document is 3699 words long.  There are 1904 tokens\n",
            "****** warning only considering first 512 tokens, document is 2309 words long.  There are 685 tokens\n",
            "****** warning only considering first 512 tokens, document is 2309 words long.  There are 743 tokens\n",
            "****** warning only considering first 512 tokens, document is 2309 words long.  There are 783 tokens\n",
            "****** warning only considering first 512 tokens, document is 2309 words long.  There are 691 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 2/50 [00:00<00:15,  3.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2309 words long.  There are 858 tokens\n",
            "****** warning only considering first 512 tokens, document is 2309 words long.  There are 856 tokens\n",
            "****** warning only considering first 512 tokens, document is 2317 words long.  There are 884 tokens\n",
            "****** warning only considering first 512 tokens, document is 2317 words long.  There are 1263 tokens\n",
            "****** warning only considering first 512 tokens, document is 2317 words long.  There are 905 tokens\n",
            "****** warning only considering first 512 tokens, document is 2317 words long.  There are 1077 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 3/50 [00:00<00:13,  3.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2317 words long.  There are 822 tokens\n",
            "****** warning only considering first 512 tokens, document is 2317 words long.  There are 818 tokens\n",
            "****** warning only considering first 512 tokens, document is 2282 words long.  There are 1073 tokens\n",
            "****** warning only considering first 512 tokens, document is 2282 words long.  There are 945 tokens\n",
            "****** warning only considering first 512 tokens, document is 2282 words long.  There are 904 tokens\n",
            "****** warning only considering first 512 tokens, document is 2282 words long.  There are 795 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 4/50 [00:01<00:12,  3.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2282 words long.  There are 712 tokens\n",
            "****** warning only considering first 512 tokens, document is 2282 words long.  There are 678 tokens\n",
            "****** warning only considering first 512 tokens, document is 7277 words long.  There are 2637 tokens\n",
            "****** warning only considering first 512 tokens, document is 7277 words long.  There are 2754 tokens\n",
            "****** warning only considering first 512 tokens, document is 7277 words long.  There are 2920 tokens\n",
            "****** warning only considering first 512 tokens, document is 7277 words long.  There are 4321 tokens\n",
            "****** warning only considering first 512 tokens, document is 7277 words long.  There are 3482 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 5/50 [00:01<00:14,  3.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 7277 words long.  There are 3506 tokens\n",
            "****** warning only considering first 512 tokens, document is 4016 words long.  There are 1140 tokens\n",
            "****** warning only considering first 512 tokens, document is 4016 words long.  There are 1063 tokens\n",
            "****** warning only considering first 512 tokens, document is 4016 words long.  There are 1143 tokens\n",
            "****** warning only considering first 512 tokens, document is 4016 words long.  There are 1259 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 6/50 [00:01<00:13,  3.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4016 words long.  There are 1253 tokens\n",
            "****** warning only considering first 512 tokens, document is 4016 words long.  There are 1433 tokens\n",
            "****** warning only considering first 512 tokens, document is 2229 words long.  There are 790 tokens\n",
            "****** warning only considering first 512 tokens, document is 2229 words long.  There are 809 tokens\n",
            "****** warning only considering first 512 tokens, document is 2229 words long.  There are 847 tokens\n",
            "****** warning only considering first 512 tokens, document is 2229 words long.  There are 750 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 7/50 [00:01<00:11,  3.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2229 words long.  There are 805 tokens\n",
            "****** warning only considering first 512 tokens, document is 2229 words long.  There are 898 tokens\n",
            "****** warning only considering first 512 tokens, document is 1688 words long.  There are 648 tokens\n",
            "****** warning only considering first 512 tokens, document is 1688 words long.  There are 684 tokens\n",
            "****** warning only considering first 512 tokens, document is 1688 words long.  There are 556 tokens\n",
            "****** warning only considering first 512 tokens, document is 1688 words long.  There are 600 tokens\n",
            "****** warning only considering first 512 tokens, document is 1688 words long.  There are 572 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 8/50 [00:02<00:10,  3.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1688 words long.  There are 559 tokens\n",
            "****** warning only considering first 512 tokens, document is 7092 words long.  There are 2540 tokens\n",
            "****** warning only considering first 512 tokens, document is 7092 words long.  There are 2569 tokens\n",
            "****** warning only considering first 512 tokens, document is 7092 words long.  There are 3152 tokens\n",
            "****** warning only considering first 512 tokens, document is 7092 words long.  There are 4053 tokens\n",
            "****** warning only considering first 512 tokens, document is 7092 words long.  There are 3828 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 9/50 [00:02<00:12,  3.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 7092 words long.  There are 2590 tokens\n",
            "****** warning only considering first 512 tokens, document is 2119 words long.  There are 915 tokens\n",
            "****** warning only considering first 512 tokens, document is 2119 words long.  There are 769 tokens\n",
            "****** warning only considering first 512 tokens, document is 2119 words long.  There are 867 tokens\n",
            "****** warning only considering first 512 tokens, document is 2119 words long.  There are 861 tokens\n",
            "****** warning only considering first 512 tokens, document is 2119 words long.  There are 785 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 10/50 [00:02<00:11,  3.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2119 words long.  There are 689 tokens\n",
            "****** warning only considering first 512 tokens, document is 3254 words long.  There are 2080 tokens\n",
            "****** warning only considering first 512 tokens, document is 3254 words long.  There are 1921 tokens\n",
            "****** warning only considering first 512 tokens, document is 3254 words long.  There are 1393 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 11/50 [00:03<00:10,  3.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 3254 words long.  There are 1370 tokens\n",
            "****** warning only considering first 512 tokens, document is 3254 words long.  There are 1440 tokens\n",
            "****** warning only considering first 512 tokens, document is 3254 words long.  There are 1425 tokens\n",
            "****** warning only considering first 512 tokens, document is 1718 words long.  There are 555 tokens\n",
            "****** warning only considering first 512 tokens, document is 1718 words long.  There are 681 tokens\n",
            "****** warning only considering first 512 tokens, document is 1718 words long.  There are 554 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 24%|██▍       | 12/50 [00:03<00:09,  3.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1718 words long.  There are 639 tokens\n",
            "****** warning only considering first 512 tokens, document is 1718 words long.  There are 574 tokens\n",
            "****** warning only considering first 512 tokens, document is 1718 words long.  There are 515 tokens\n",
            "****** warning only considering first 512 tokens, document is 2044 words long.  There are 894 tokens\n",
            "****** warning only considering first 512 tokens, document is 2044 words long.  There are 932 tokens\n",
            "****** warning only considering first 512 tokens, document is 2044 words long.  There are 832 tokens\n",
            "****** warning only considering first 512 tokens, document is 2044 words long.  There are 697 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 26%|██▌       | 13/50 [00:03<00:08,  4.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2044 words long.  There are 733 tokens\n",
            "****** warning only considering first 512 tokens, document is 2044 words long.  There are 823 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1694 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1554 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 28%|██▊       | 14/50 [00:03<00:09,  3.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1572 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1561 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1982 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 2028 tokens\n",
            "****** warning only considering first 512 tokens, document is 7632 words long.  There are 2715 tokens\n",
            "****** warning only considering first 512 tokens, document is 7632 words long.  There are 2771 tokens\n",
            "****** warning only considering first 512 tokens, document is 7632 words long.  There are 2781 tokens\n",
            "****** warning only considering first 512 tokens, document is 7632 words long.  There are 2825 tokens\n",
            "****** warning only considering first 512 tokens, document is 7632 words long.  There are 2818 tokens\n",
            "****** warning only considering first 512 tokens, document is 7632 words long.  There are 2772 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 15/50 [00:04<00:10,  3.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 271 words long.  There are 585 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1813 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 2020 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 34%|███▍      | 17/50 [00:04<00:08,  3.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1644 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1672 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1943 tokens\n",
            "****** warning only considering first 512 tokens, document is 4768 words long.  There are 1610 tokens\n",
            "****** warning only considering first 512 tokens, document is 6892 words long.  There are 2160 tokens\n",
            "****** warning only considering first 512 tokens, document is 6892 words long.  There are 2487 tokens\n",
            "****** warning only considering first 512 tokens, document is 6892 words long.  There are 2526 tokens\n",
            "****** warning only considering first 512 tokens, document is 6892 words long.  There are 1984 tokens\n",
            "****** warning only considering first 512 tokens, document is 6892 words long.  There are 2018 tokens\n",
            "****** warning only considering first 512 tokens, document is 6892 words long.  There are 1950 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 19/50 [00:05<00:08,  3.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2233 words long.  There are 1164 tokens\n",
            "****** warning only considering first 512 tokens, document is 2233 words long.  There are 1138 tokens\n",
            "****** warning only considering first 512 tokens, document is 2233 words long.  There are 1197 tokens\n",
            "****** warning only considering first 512 tokens, document is 2233 words long.  There are 1082 tokens\n",
            "****** warning only considering first 512 tokens, document is 2233 words long.  There are 959 tokens\n",
            "****** warning only considering first 512 tokens, document is 2233 words long.  There are 906 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 20/50 [00:05<00:07,  3.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1746 words long.  There are 1043 tokens\n",
            "****** warning only considering first 512 tokens, document is 1746 words long.  There are 900 tokens\n",
            "****** warning only considering first 512 tokens, document is 1746 words long.  There are 760 tokens\n",
            "****** warning only considering first 512 tokens, document is 1746 words long.  There are 791 tokens\n",
            "****** warning only considering first 512 tokens, document is 1746 words long.  There are 645 tokens\n",
            "****** warning only considering first 512 tokens, document is 1746 words long.  There are 746 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 42%|████▏     | 21/50 [00:05<00:07,  3.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 3817 words long.  There are 1402 tokens\n",
            "****** warning only considering first 512 tokens, document is 3817 words long.  There are 1471 tokens\n",
            "****** warning only considering first 512 tokens, document is 3817 words long.  There are 1300 tokens\n",
            "****** warning only considering first 512 tokens, document is 3817 words long.  There are 1532 tokens\n",
            "****** warning only considering first 512 tokens, document is 3817 words long.  There are 1207 tokens\n",
            "****** warning only considering first 512 tokens, document is 3817 words long.  There are 1223 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 22/50 [00:05<00:07,  3.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4484 words long.  There are 1621 tokens\n",
            "****** warning only considering first 512 tokens, document is 4484 words long.  There are 1286 tokens\n",
            "****** warning only considering first 512 tokens, document is 4484 words long.  There are 1447 tokens\n",
            "****** warning only considering first 512 tokens, document is 4484 words long.  There are 1356 tokens\n",
            "****** warning only considering first 512 tokens, document is 4484 words long.  There are 1463 tokens\n",
            "****** warning only considering first 512 tokens, document is 4484 words long.  There are 1868 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 46%|████▌     | 23/50 [00:06<00:06,  3.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1900 words long.  There are 1288 tokens\n",
            "****** warning only considering first 512 tokens, document is 1900 words long.  There are 849 tokens\n",
            "****** warning only considering first 512 tokens, document is 1900 words long.  There are 1061 tokens\n",
            "****** warning only considering first 512 tokens, document is 1900 words long.  There are 725 tokens\n",
            "****** warning only considering first 512 tokens, document is 1900 words long.  There are 1038 tokens\n",
            "****** warning only considering first 512 tokens, document is 1900 words long.  There are 884 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 48%|████▊     | 24/50 [00:06<00:07,  3.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 6072 words long.  There are 1881 tokens\n",
            "****** warning only considering first 512 tokens, document is 6072 words long.  There are 1881 tokens\n",
            "****** warning only considering first 512 tokens, document is 6072 words long.  There are 1837 tokens\n",
            "****** warning only considering first 512 tokens, document is 6072 words long.  There are 1997 tokens\n",
            "****** warning only considering first 512 tokens, document is 6072 words long.  There are 2044 tokens\n",
            "****** warning only considering first 512 tokens, document is 6072 words long.  There are 1874 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 25/50 [00:06<00:06,  4.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1596 words long.  There are 837 tokens\n",
            "****** warning only considering first 512 tokens, document is 1596 words long.  There are 698 tokens\n",
            "****** warning only considering first 512 tokens, document is 1596 words long.  There are 679 tokens\n",
            "****** warning only considering first 512 tokens, document is 1596 words long.  There are 660 tokens\n",
            "****** warning only considering first 512 tokens, document is 1596 words long.  There are 587 tokens\n",
            "****** warning only considering first 512 tokens, document is 8559 words long.  There are 3007 tokens\n",
            "****** warning only considering first 512 tokens, document is 8559 words long.  There are 3785 tokens\n",
            "****** warning only considering first 512 tokens, document is 8559 words long.  There are 3673 tokens\n",
            "****** warning only considering first 512 tokens, document is 8559 words long.  There are 3385 tokens\n",
            "****** warning only considering first 512 tokens, document is 8559 words long.  There are 3429 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 52%|█████▏    | 26/50 [00:07<00:07,  3.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 8559 words long.  There are 2992 tokens\n",
            "****** warning only considering first 512 tokens, document is 7443 words long.  There are 2979 tokens\n",
            "****** warning only considering first 512 tokens, document is 7443 words long.  There are 3149 tokens\n",
            "****** warning only considering first 512 tokens, document is 7443 words long.  There are 2470 tokens\n",
            "****** warning only considering first 512 tokens, document is 7443 words long.  There are 2749 tokens\n",
            "****** warning only considering first 512 tokens, document is 7443 words long.  There are 2816 tokens\n",
            "****** warning only considering first 512 tokens, document is 7443 words long.  There are 2303 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 28/50 [00:07<00:06,  3.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2341 words long.  There are 683 tokens\n",
            "****** warning only considering first 512 tokens, document is 2341 words long.  There are 700 tokens\n",
            "****** warning only considering first 512 tokens, document is 2341 words long.  There are 766 tokens\n",
            "****** warning only considering first 512 tokens, document is 2341 words long.  There are 782 tokens\n",
            "****** warning only considering first 512 tokens, document is 2341 words long.  There are 904 tokens\n",
            "****** warning only considering first 512 tokens, document is 2341 words long.  There are 679 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 58%|█████▊    | 29/50 [00:07<00:06,  3.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4715 words long.  There are 2100 tokens\n",
            "****** warning only considering first 512 tokens, document is 4715 words long.  There are 1723 tokens\n",
            "****** warning only considering first 512 tokens, document is 4715 words long.  There are 1667 tokens\n",
            "****** warning only considering first 512 tokens, document is 4715 words long.  There are 1649 tokens\n",
            "****** warning only considering first 512 tokens, document is 4715 words long.  There are 1837 tokens\n",
            "****** warning only considering first 512 tokens, document is 4715 words long.  There are 1444 tokens\n",
            "****** warning only considering first 512 tokens, document is 6808 words long.  There are 3422 tokens\n",
            "****** warning only considering first 512 tokens, document is 6808 words long.  There are 2952 tokens\n",
            "****** warning only considering first 512 tokens, document is 6808 words long.  There are 2981 tokens\n",
            "****** warning only considering first 512 tokens, document is 6808 words long.  There are 2969 tokens\n",
            "****** warning only considering first 512 tokens, document is 6808 words long.  There are 3147 tokens\n",
            "****** warning only considering first 512 tokens, document is 6808 words long.  There are 2876 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 30/50 [00:08<00:06,  3.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 16712 words long.  There are 5322 tokens\n",
            "****** warning only considering first 512 tokens, document is 16712 words long.  There are 5306 tokens\n",
            "****** warning only considering first 512 tokens, document is 16712 words long.  There are 5607 tokens\n",
            "****** warning only considering first 512 tokens, document is 16712 words long.  There are 5895 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 62%|██████▏   | 31/50 [00:09<00:08,  2.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 16712 words long.  There are 5958 tokens\n",
            "****** warning only considering first 512 tokens, document is 16712 words long.  There are 5352 tokens\n",
            "****** warning only considering first 512 tokens, document is 3975 words long.  There are 1980 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 64%|██████▍   | 32/50 [00:09<00:07,  2.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 3975 words long.  There are 1702 tokens\n",
            "****** warning only considering first 512 tokens, document is 3975 words long.  There are 1528 tokens\n",
            "****** warning only considering first 512 tokens, document is 3975 words long.  There are 1296 tokens\n",
            "****** warning only considering first 512 tokens, document is 3975 words long.  There are 1488 tokens\n",
            "****** warning only considering first 512 tokens, document is 3975 words long.  There are 1639 tokens\n",
            "****** warning only considering first 512 tokens, document is 8075 words long.  There are 3062 tokens\n",
            "****** warning only considering first 512 tokens, document is 8075 words long.  There are 2963 tokens\n",
            "****** warning only considering first 512 tokens, document is 8075 words long.  There are 2945 tokens\n",
            "****** warning only considering first 512 tokens, document is 8075 words long.  There are 2661 tokens\n",
            "****** warning only considering first 512 tokens, document is 8075 words long.  There are 2794 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 66%|██████▌   | 33/50 [00:09<00:06,  2.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 8075 words long.  There are 2963 tokens\n",
            "****** warning only considering first 512 tokens, document is 8597 words long.  There are 2918 tokens\n",
            "****** warning only considering first 512 tokens, document is 8597 words long.  There are 3140 tokens\n",
            "****** warning only considering first 512 tokens, document is 8597 words long.  There are 3045 tokens\n",
            "****** warning only considering first 512 tokens, document is 8597 words long.  There are 3163 tokens\n",
            "****** warning only considering first 512 tokens, document is 8597 words long.  There are 2827 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 68%|██████▊   | 34/50 [00:10<00:06,  2.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 8597 words long.  There are 2790 tokens\n",
            "****** warning only considering first 512 tokens, document is 6804 words long.  There are 3530 tokens\n",
            "****** warning only considering first 512 tokens, document is 6804 words long.  There are 3086 tokens\n",
            "****** warning only considering first 512 tokens, document is 6804 words long.  There are 2883 tokens\n",
            "****** warning only considering first 512 tokens, document is 6804 words long.  There are 2447 tokens\n",
            "****** warning only considering first 512 tokens, document is 6804 words long.  There are 2751 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 36/50 [00:10<00:05,  2.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 6804 words long.  There are 2702 tokens\n",
            "****** warning only considering first 512 tokens, document is 1844 words long.  There are 1059 tokens\n",
            "****** warning only considering first 512 tokens, document is 1844 words long.  There are 845 tokens\n",
            "****** warning only considering first 512 tokens, document is 1844 words long.  There are 623 tokens\n",
            "****** warning only considering first 512 tokens, document is 1844 words long.  There are 668 tokens\n",
            "****** warning only considering first 512 tokens, document is 1844 words long.  There are 632 tokens\n",
            "****** warning only considering first 512 tokens, document is 1844 words long.  There are 644 tokens\n",
            "****** warning only considering first 512 tokens, document is 7465 words long.  There are 2934 tokens\n",
            "****** warning only considering first 512 tokens, document is 7465 words long.  There are 2978 tokens\n",
            "****** warning only considering first 512 tokens, document is 7465 words long.  There are 3758 tokens\n",
            "****** warning only considering first 512 tokens, document is 7465 words long.  There are 3275 tokens\n",
            "****** warning only considering first 512 tokens, document is 7465 words long.  There are 3355 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 74%|███████▍  | 37/50 [00:11<00:05,  2.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 7465 words long.  There are 3792 tokens\n",
            "****** warning only considering first 512 tokens, document is 7884 words long.  There are 3512 tokens\n",
            "****** warning only considering first 512 tokens, document is 7884 words long.  There are 4040 tokens\n",
            "****** warning only considering first 512 tokens, document is 7884 words long.  There are 3444 tokens\n",
            "****** warning only considering first 512 tokens, document is 7884 words long.  There are 3236 tokens\n",
            "****** warning only considering first 512 tokens, document is 7884 words long.  There are 2893 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 76%|███████▌  | 38/50 [00:11<00:05,  2.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 7884 words long.  There are 3084 tokens\n",
            "****** warning only considering first 512 tokens, document is 9431 words long.  There are 3711 tokens\n",
            "****** warning only considering first 512 tokens, document is 9431 words long.  There are 3757 tokens\n",
            "****** warning only considering first 512 tokens, document is 9431 words long.  There are 3824 tokens\n",
            "****** warning only considering first 512 tokens, document is 9431 words long.  There are 3349 tokens\n",
            "****** warning only considering first 512 tokens, document is 9431 words long.  There are 4245 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 78%|███████▊  | 39/50 [00:12<00:04,  2.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 9431 words long.  There are 3716 tokens\n",
            "****** warning only considering first 512 tokens, document is 4539 words long.  There are 1851 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 40/50 [00:12<00:04,  2.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4539 words long.  There are 1926 tokens\n",
            "****** warning only considering first 512 tokens, document is 4539 words long.  There are 2031 tokens\n",
            "****** warning only considering first 512 tokens, document is 4539 words long.  There are 2676 tokens\n",
            "****** warning only considering first 512 tokens, document is 4539 words long.  There are 2824 tokens\n",
            "****** warning only considering first 512 tokens, document is 4539 words long.  There are 1797 tokens\n",
            "****** warning only considering first 512 tokens, document is 13673 words long.  There are 7708 tokens\n",
            "****** warning only considering first 512 tokens, document is 13673 words long.  There are 7043 tokens\n",
            "****** warning only considering first 512 tokens, document is 13673 words long.  There are 6717 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 41/50 [00:13<00:04,  1.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 13673 words long.  There are 8590 tokens\n",
            "****** warning only considering first 512 tokens, document is 13673 words long.  There are 5197 tokens\n",
            "****** warning only considering first 512 tokens, document is 13673 words long.  There are 5579 tokens\n",
            "****** warning only considering first 512 tokens, document is 3536 words long.  There are 1344 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 84%|████████▍ | 42/50 [00:13<00:03,  2.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 3536 words long.  There are 1346 tokens\n",
            "****** warning only considering first 512 tokens, document is 3536 words long.  There are 1357 tokens\n",
            "****** warning only considering first 512 tokens, document is 3536 words long.  There are 1367 tokens\n",
            "****** warning only considering first 512 tokens, document is 3536 words long.  There are 1304 tokens\n",
            "****** warning only considering first 512 tokens, document is 3536 words long.  There are 1208 tokens\n",
            "****** warning only considering first 512 tokens, document is 1809 words long.  There are 797 tokens\n",
            "****** warning only considering first 512 tokens, document is 1809 words long.  There are 808 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 86%|████████▌ | 43/50 [00:13<00:02,  2.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1809 words long.  There are 625 tokens\n",
            "****** warning only considering first 512 tokens, document is 1809 words long.  There are 824 tokens\n",
            "****** warning only considering first 512 tokens, document is 1809 words long.  There are 907 tokens\n",
            "****** warning only considering first 512 tokens, document is 1809 words long.  There are 760 tokens\n",
            "****** warning only considering first 512 tokens, document is 1711 words long.  There are 534 tokens\n",
            "****** warning only considering first 512 tokens, document is 1711 words long.  There are 521 tokens\n",
            "****** warning only considering first 512 tokens, document is 1711 words long.  There are 548 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 44/50 [00:14<00:01,  3.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1711 words long.  There are 538 tokens\n",
            "****** warning only considering first 512 tokens, document is 1711 words long.  There are 556 tokens\n",
            "****** warning only considering first 512 tokens, document is 4291 words long.  There are 1247 tokens\n",
            "****** warning only considering first 512 tokens, document is 4291 words long.  There are 1288 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|█████████ | 45/50 [00:14<00:01,  3.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 4291 words long.  There are 1312 tokens\n",
            "****** warning only considering first 512 tokens, document is 4291 words long.  There are 1359 tokens\n",
            "****** warning only considering first 512 tokens, document is 4291 words long.  There are 1419 tokens\n",
            "****** warning only considering first 512 tokens, document is 4291 words long.  There are 1423 tokens\n",
            "****** warning only considering first 512 tokens, document is 2046 words long.  There are 938 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 92%|█████████▏| 46/50 [00:14<00:01,  3.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2046 words long.  There are 1234 tokens\n",
            "****** warning only considering first 512 tokens, document is 2046 words long.  There are 1052 tokens\n",
            "****** warning only considering first 512 tokens, document is 2046 words long.  There are 1051 tokens\n",
            "****** warning only considering first 512 tokens, document is 2046 words long.  There are 1044 tokens\n",
            "****** warning only considering first 512 tokens, document is 2046 words long.  There are 959 tokens\n",
            "****** warning only considering first 512 tokens, document is 2122 words long.  There are 956 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 94%|█████████▍| 47/50 [00:14<00:00,  3.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2122 words long.  There are 944 tokens\n",
            "****** warning only considering first 512 tokens, document is 2122 words long.  There are 891 tokens\n",
            "****** warning only considering first 512 tokens, document is 2122 words long.  There are 1072 tokens\n",
            "****** warning only considering first 512 tokens, document is 2122 words long.  There are 762 tokens\n",
            "****** warning only considering first 512 tokens, document is 2122 words long.  There are 645 tokens\n",
            "****** warning only considering first 512 tokens, document is 1209 words long.  There are 516 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 96%|█████████▌| 48/50 [00:15<00:00,  4.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 1209 words long.  There are 644 tokens\n",
            "****** warning only considering first 512 tokens, document is 1209 words long.  There are 569 tokens\n",
            "****** warning only considering first 512 tokens, document is 2474 words long.  There are 941 tokens\n",
            "****** warning only considering first 512 tokens, document is 2474 words long.  There are 1169 tokens\n",
            "****** warning only considering first 512 tokens, document is 2474 words long.  There are 1183 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 98%|█████████▊| 49/50 [00:15<00:00,  4.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "****** warning only considering first 512 tokens, document is 2474 words long.  There are 901 tokens\n",
            "****** warning only considering first 512 tokens, document is 2474 words long.  There are 888 tokens\n",
            "****** warning only considering first 512 tokens, document is 2474 words long.  There are 1235 tokens\n",
            "****** warning only considering first 512 tokens, document is 6000 words long.  There are 2560 tokens\n",
            "****** warning only considering first 512 tokens, document is 6000 words long.  There are 2210 tokens\n",
            "****** warning only considering first 512 tokens, document is 6000 words long.  There are 2449 tokens\n",
            "****** warning only considering first 512 tokens, document is 6000 words long.  There are 2243 tokens\n",
            "****** warning only considering first 512 tokens, document is 6000 words long.  There are 2718 tokens\n",
            "****** warning only considering first 512 tokens, document is 6000 words long.  There are 2359 tokens\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:15<00:00,  3.20it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m86xqzSOO8A"
      },
      "source": [
        "FIND_PDFS=False \n",
        "SEARCH_MEDRXIV=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV9gJtmqY9ic"
      },
      "source": [
        "workingPath = '/root/kaggle/working'\n",
        "import pandas as pd\n",
        "import re\n",
        "if FIND_PDFS:\n",
        "    from metapub import UrlReverse\n",
        "    from metapub import FindIt\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "#from summarizer import Summarizer\n",
        "#summarizerModel = Summarizer()\n",
        "def displayResults(hit_dictionary, answers, question):\n",
        "    \n",
        "    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: '+question+'</div>'\n",
        "    #all_HTML_txt = question_HTML\n",
        "    confidence = list(answers.keys())\n",
        "    confidence.sort(reverse=True)\n",
        "    \n",
        "    confidence = list(answers.keys())\n",
        "    confidence.sort(reverse=True)\n",
        "    \n",
        "\n",
        "    for c in confidence:\n",
        "        if c>0 and c <= 1 and len(answers[c]['answer']) != 0:\n",
        "            if 'idx' not in  answers[c]:\n",
        "                continue\n",
        "            rowData = []\n",
        "            idx = answers[c]['idx']\n",
        "            title = hit_dictionary[idx]['title']\n",
        "\n",
        "            \n",
        "            full_abs = answers[c]['abstract_bert']\n",
        "            bert_ans = answers[c]['answer']\n",
        "            \n",
        "            \n",
        "            #split_abs = full_abs.split(bert_ans)\n",
        "            #x=(split_abs[0][:split_abs[0].rfind('. ')+1])\n",
        "            #sentance_beginning = x[x.rfind('. ')+1:] + split_abs[0][split_abs[0].rfind('. ')+1:]\n",
        "            #sentance_beginning = split_abs[0][split_abs[0].rfind('.')+1:]\n",
        "            x=''\n",
        "            y=''\n",
        "            z=''\n",
        "            t=''\n",
        "            regex = r\"(\\.\\s[^0-9])(?!.*(\\.\\s[^0-9]))\"\n",
        "            split_abs = full_abs.split(bert_ans)\n",
        "            matches = re.finditer(regex, split_abs[0], re.MULTILINE)\n",
        "            for matchNum1, match in enumerate(matches, start=1):\n",
        "              y=(split_abs[0][:match.start()+1])\n",
        "            matches = re.finditer(regex, y, re.MULTILINE)\n",
        "            for matchNum2, match in enumerate(matches, start=1):\n",
        "              x=(y[match.start()+1:])\n",
        "            if x=='':\n",
        "              x=split_abs[0]\n",
        "            matches = re.finditer(regex, split_abs[0], re.MULTILINE)\n",
        "            for matchNum3, match in enumerate(matches, start=1):\n",
        "              z=(split_abs[0][match.start()+1:])  \n",
        "            sentance_beginning = x+z\n",
        "            regex2=r\"(.*?(?<!\\b\\w)[.?!])\\s+[a-zA-Z0-9]\"\n",
        "            if len(split_abs) == 1:\n",
        "                sentance_end_pos = len(full_abs)\n",
        "                sentance_end =''\n",
        "            else:\n",
        "                matches = re.finditer(regex2, split_abs[1], re.MULTILINE)\n",
        "                for matchNum4, match in enumerate(matches, start=1):\n",
        "                  if matchNum4==1:\n",
        "                    t=(split_abs[1][:match.end()-2])\n",
        "                sentance_end_pos = split_abs[1].find('. ')+1\n",
        "                if sentance_end_pos == 0:\n",
        "                    sentance_end = split_abs[1]\n",
        "                else:\n",
        "                    sentance_end = t\n",
        "            #if len(split_abs) == 1:\n",
        "            #    sentance_end_pos = len(full_abs)\n",
        "            #    sentance_end =''\n",
        "            #else:\n",
        "            #    sentance_end_pos = split_abs[1].find('. ')+1\n",
        "            #    if sentance_end_pos == 0:\n",
        "            #      sentance_end = split_abs[1]\n",
        "            #    else:\n",
        "            #      sentance_end = split_abs[1][:sentance_end_pos]\n",
        "                \n",
        "            #sentance_full = sentance_beginning + bert_ans+ sentance_end\n",
        "            answers[c]['full_answer'] = sentance_beginning+bert_ans+sentance_end\n",
        "            answers[c]['partial_answer'] = bert_ans+sentance_end\n",
        "            answers[c]['sentence_beginning'] = sentance_beginning\n",
        "            answers[c]['sentence_end'] = sentance_end\n",
        "            answers[c]['title'] = title\n",
        "        else:\n",
        "            answers.pop(c)\n",
        "    \n",
        "    \n",
        "    ## now rerank based on semantic similarity of the answers to the question\n",
        "    ## Universal sentence encoder\n",
        "    cList = list(answers.keys())\n",
        "    allAnswers = [answers[c]['full_answer'] for c in cList]\n",
        "    \n",
        "    messages = [question]+allAnswers\n",
        "    \n",
        "    encoding_matrix = embed_fn(messages)\n",
        "    similarity_matrix = np.inner(encoding_matrix, encoding_matrix)\n",
        "    rankings = similarity_matrix[1:,0]\n",
        "    \n",
        "    for i,c in enumerate(cList):\n",
        "        answers[rankings[i]] = answers.pop(c)\n",
        "\n",
        "    ## now form pandas dv\n",
        "    confidence = list(answers.keys())\n",
        "    confidence.sort(reverse=True)\n",
        "    pandasData = []\n",
        "    ranked_aswers = []\n",
        "    full_abs_list=[]\n",
        "    for c in confidence:\n",
        "        rowData=[]\n",
        "        title = answers[c]['title']\n",
        "        idx = answers[c]['idx']\n",
        "        rowData += [idx]            \n",
        "        sentance_html = '<div>' +answers[c]['sentence_beginning'] + \" <font color='red'>\"+answers[c]['answer']+\"</font> \"+answers[c]['sentence_end']+'</div>'\n",
        "        \n",
        "        rowData += [sentance_html, c,title]\n",
        "        pandasData.append(rowData)\n",
        "        ranked_aswers.append(' '.join([answers[c]['full_answer']]))\n",
        "        full_abs_list.append(' '.join([answers[c]['abstract_bert']]))\n",
        "    \n",
        "    if FIND_PDFS or SEARCH_MEDRXIV:\n",
        "        pdata2 = []\n",
        "        pm_ids = []\n",
        "        for rowData in pandasData:\n",
        "            rd = rowData\n",
        "            idx = rowData[0]\n",
        "    else:\n",
        "        pdata2 = pandasData\n",
        "        \n",
        "    \n",
        "    display(HTML(question_HTML))\n",
        "\n",
        "    if USE_SUMMARY:\n",
        "        ## try generating an exacutive summary with bart abstractive summarizer\n",
        "        allAnswersTxt = ' '.join(ranked_aswers[:10]).replace('\\n','')\n",
        "    #    exec_sum = summarizerModel(allAnswersTxt, min_length=1, max_length=500)    \n",
        "     #   execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>BERT Extractive Summary:</b>: '+exec_sum+'</div>'\n",
        "\n",
        "        answers_input_ids = SUMMARY_TOKENIZER.batch_encode_plus([allAnswersTxt], return_tensors='pt', max_length=2048)['input_ids'].to(torch_device)\n",
        "        #answers_input_ids = SUMMARY_TOKENIZER.prepare_seq2seq_batch([allAnswersTxt], return_tensors='pt', truncation=True, padding='longest')['input_ids'].to(torch_device)\n",
        "        summary_ids = SUMMARY_MODEL.generate(answers_input_ids,\n",
        "                                               num_beams=10,\n",
        "                                               length_penalty=1.1,\n",
        "                                               max_length=2048,\n",
        "                                               min_length=64,\n",
        "                                               no_repeat_ngram_size=0,\n",
        "                                                do_sample=False )\n",
        "\n",
        "        exec_sum = SUMMARY_TOKENIZER.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
        "        execSum_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>BART Abstractive Summary:</b>: '+exec_sum+'</div>'\n",
        "        display(HTML(execSum_HTML))\n",
        "        warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 12px; padding-bottom:12px; color:#CCCC00; margin-top:1pt\"> Warning this is an autogenerated summary based on semantic search of abstracts, always examine the sources before accepting this conclusion.  If the evidence only mentions topic in passing or the evidence is not clear, the summary will likely not clearly answer the question.</div>'\n",
        "        display(HTML(warning_HTML))\n",
        "\n",
        "#    display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:18px\"><b>Body of Evidence:</b></div>'))\n",
        "    \n",
        "    if FIND_PDFS or SEARCH_MEDRXIV:\n",
        "        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])\n",
        "    else:\n",
        "        df = pd.DataFrame(pdata2, columns = ['Lucene ID', 'BERT-SQuAD Answer with Highlights', 'Confidence', 'Title/Link'])\n",
        "        \n",
        "    display(HTML(df.to_html(render_links=True, escape=False)))\n",
        "    return full_abs_list,ranked_aswers,pm_ids\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y12l1rgYgAFF",
        "outputId": "eedfd679-dfff-4660-a9dc-4857d30a6c69"
      },
      "source": [
        "full_abs_list,ranked_aswers,pm_ids=displayResults(hit_dictionary, answers, query)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"font-family: Times New Roman; font-size: 28px; padding-bottom:28px\"><b>Query</b>: As we have mentioned above, COVID-19 is only fatal for a small percentage of people. In fact 80% face only mild symptoms with no need for hospitalization?</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Lucene ID</th>\n",
              "      <th>BERT-SQuAD Answer with Highlights</th>\n",
              "      <th>Confidence</th>\n",
              "      <th>Title/Link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lassa fever</td>\n",
              "      <td><div> sentence _ 90 as of 2003,10-16 % of people in sierra leone and liberia admitted to hospital had the virus. sentence _ 91 the case fatality rate for those who are hospitalized for the disease is  <font color='red'>about 15-20 %</font> .</div></td>\n",
              "      <td>0.586256</td>\n",
              "      <td>Lassa fever</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Acute disseminated encephalomyelitis</td>\n",
              "      <td><div> sentence _ 11 adem shows seasonal variation with higher incidence in winter and spring months which may coincide with higher viral infections during these months. sentence _ 12 the mortality rate may be as high as 5 % ; however, full recovery is seen  <font color='red'>in 50 to 75 % of cases with increase in survival rates up to 70 to 90 %</font>  with figures including minor residual disability as well.</div></td>\n",
              "      <td>0.583253</td>\n",
              "      <td>Acute disseminated encephalomyelitis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Rosacea</td>\n",
              "      <td><div> sentence _ 59 other cases, if left untreated, worsen over time. section _ 11 behavior sentence _ 60 avoiding triggers that worsen the condition can help reduce the onset of rosacea,  <font color='red'>but alone will not normally lead to remission except in mild cases</font> .</div></td>\n",
              "      <td>0.578586</td>\n",
              "      <td>Rosacea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Transient global amnesia</td>\n",
              "      <td><div> sentence _ 82 tga is most common in people between age 56 and 75, with the average age of a person experiencing tga being approximately 62. section _ 12 see also list _ 2 amnesia | [ [ dissociative _ amnesia | dissociative amnesia ] ] <font color='red'>only mild symptoms with no need for hospitalization</font> </div></td>\n",
              "      <td>0.573354</td>\n",
              "      <td>Transient global amnesia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lung cancer</td>\n",
              "      <td><div> sentence _ 159 research has not found two other available tests — sputum cytology or [ [ chest _ radiograph | chest radiograph ] ] (cxr) screening tests — to have any benefit. sentence _ 160 the [ [ united _ states _ preventive _ services _ task _ force | united states preventive services task force ] ] (uspstf) recommends yearly screening using low-dose computed tomography in those who have a total smoking history of 30 pack-years and are  <font color='red'>between 55 and 80 years old</font>  until a person has not been smoking for more than 15 years.</div></td>\n",
              "      <td>0.550006</td>\n",
              "      <td>Lung cancer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Major trauma</td>\n",
              "      <td><div> sentence _ 24 penetrating trauma is caused when a [ [ foreign _ body | foreign body ] ] such as a bullet or a knife enters the [ [ tissue _ (biology) | body tissue ] ], creating an open wound. sentence _ 25 in the united states, most deaths caused by penetrating trauma occur in urban areas and 80 % of these deaths are caused  <font color='red'>by firearms</font> .</div></td>\n",
              "      <td>0.474236</td>\n",
              "      <td>Major trauma</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Bucillamine</td>\n",
              "      <td><div> food & drug administration (fda) has approved revive therapeutics ltd. to proceed with a randomized, double-blind, placebo-controlled confirmatory phase 3 clinical trial protocol to evaluate the safety and efficacy of bucillamine in patients  <font color='red'>with mild-moderate</font>  covid-19.</div></td>\n",
              "      <td>0.438401</td>\n",
              "      <td>Bucillamine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Arteriovenous malformation</td>\n",
              "      <td><div> miller ] ] was diagnosed with avm after filming [ [ yogi _ bear _ (film) | yogi bear ] ] in new zealand in 2010 ; miller described his experience with the disease on the pete holmes podcast you made it weird on october 28,2011, shedding his comedian side for a moment and becoming more philosophical, narrating his behaviors and inability to sleep during that time. he suffered a seizure upon return to los angeles and successfully underwent surgery that had a mortality rate  <font color='red'>of ten percent</font> . | jazz guitarist [ [ pat _ martino | pat martino ] ] experienced an avm and subsequently developed amnesia and manic depression.</div></td>\n",
              "      <td>0.400459</td>\n",
              "      <td>Arteriovenous malformation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Alcoholism</td>\n",
              "      <td><div> sentence _ 2 for the song by starsailor, see [ [ alcoholic _ (song) | alcoholic (song) ] ]. table _ 0 alcoholism | other names alcohol dependence syndrome, alcohol use disorder (aud) | [ [ specialty _ (medicine) | specialty ] ] [ [ psychiatry | psychiatry ] ], [ [ clinical _ psychology | clinical psychology ] ], toxicology, addiction medicine | symptoms drinking large amounts of alcohol over a long period, difficulty cutting down, acquiring and drinking alcohol taking up a lot of time, usage resulting in problems, [ [ alcohol _ withdrawal _ syndrome | withdrawal ] ] occurring when stopping | [ [ complication _ (medicine) | complications ] ] [ [ mental _ illness | mental illness ] ], [ [ delirium _ tremens | delirium ] ], [ [ wernicke – korsakoff _ syndrome | wernicke – korsakoff syndrome ] ], [ [ heart _ arrhythmia | irregular heartbeat ] ], [ [ cirrhosis | cirrhosis of the liver ] ], [ [ cancer | cancer ] ], [ [ fetal _ alcohol _ spectrum _ disorder | fetal alcohol spectrum disorder ] ], [ [ suicide | suicide ] ] | duration long term | causes environmental and genetic factors | risk factors [ [ stress _ (biological) | stress ] ], anxiety, inexpensive, easy access | [ [ diagnostic _ method | diagnostic method ] ] questionnaires, [ [ blood _ test | blood tests ] ] | treatment [ [ alcohol _ detoxification | alcohol detoxification ] ] typically with [ [ benzodiazepine | benzodiazepines ] ], counselling, [ [ acamprosate | acamprosate ] ], [ [ disulfiram | disulfiram ] ], [ [ naltrexone | naltrexone ] ] | frequency 380 million / 5. 1 % adults (2016) |  <font color='red'>deaths 3. 3 million / 5. 9 %</font>  sentence _ 3 alcoholism is, broadly, any drinking of [ [ alcohol _ (drug) | alcohol ] ] that results in significant mental or physical [ [ health | health ] ] problems.</div></td>\n",
              "      <td>0.348510</td>\n",
              "      <td>Alcoholism</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Economy of Lebanon</td>\n",
              "      <td><div> sentence _ 142 this economic crisis made lebanon ' s [ [ gross _ domestic _ product | gross domestic product ] ] fall to about $ 44 billion, which was about $ 55 billion the year before. sentence _ 143 the crisis became worse when the [ [ covid-19 _ pandemic | covid- <font color='red'>19 pandemic</font>  ] ] affected the lebanese economy.</div></td>\n",
              "      <td>0.297735</td>\n",
              "      <td>Economy of Lebanon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Sampling bias</td>\n",
              "      <td><div> sentence _ 69 this would adjust any estimates to achieve the same expected value as a sample that included exactly 50 men and 50 women, unless men and women differed in their likelihood of taking part in the survey. section _ 8 see also list _ 2 [ [ censored _ regression _ model | censored regression model ] ] | [ [ cherry _ picking _ (fallacy) | cherry picking (fallacy) ] ] | | [ [ friendship _ paradox | friendship paradox ] ] | [ [ reporting _ bias | reporting bias ] ] | [ [ sampling _ probability | sampling probability ] ] | [ [ selection _ bias | selection bias ] ] | [ [ spectrum _ bias | spectrum bias ] ] | [ [ truncated _ regression _ model | truncated regression model ] ] <font color='red'>only mild symptoms with no need for hospitalization</font> </div></td>\n",
              "      <td>0.218133</td>\n",
              "      <td>Sampling bias</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Llama</td>\n",
              "      <td><div> sentence _ 180 the fiber comes in many different colors ranging from white or grey to reddish-brown, brown, dark brown and black. table _ 2 animal fiber diameter ([ [ micrometre | micrometres ] ]) | [ [ vicuna # vicuna _ wool | vicuna ] ] 6 – 10 | alpaca ([ [ alpaca _ fiber | suri ] ]) 10 – 15 | [ [ muskox | muskox ] ] ([ [ qiviut | qiviut ] ]) 11 – 13 | [ [ merino # wool _ qualities | merino sheep ] ] 12 – 20 | [ [ angora _ rabbit | angora rabbit ] ] ([ [ angora _ wool | angora wool ] ]) 13 | [ [ cashmere _ goat | cashmere goat ] ] ([ [ cashmere _ wool | cashmere wool ] ]) 15 – 19 | [ [ yak | yak ] ] ([ [ yak _ fiber | yak fiber ] ]) 15 – 19 | [ [ camel | camel ] ] ([ [ camel _ hair | camel hair ] ]) 16 – 25 | [ [ guanaco # guanaco _ fiber | guanaco ] ] 16 – 18 | llama (tapada) 20 – 30 | [ [ chinchilla | chinchilla ] ] 21 | [ [ angora _ goat | angora goat ] ] ([ [ mohair | mohair ] ]) 25 – 45 | [ [ huacaya _ alpaca # fibre | huacaya alpaca ] ] 27. 7 | llama (ccara) 30 – 40 | average diameter of some of the finest, natural fibers section _ 17 see also <font color='red'>only mild symptoms with no need for hospitalization</font> </div></td>\n",
              "      <td>0.084864</td>\n",
              "      <td>Llama</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-40c5b353baa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_abs_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mranked_aswers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpm_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplayResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhit_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-4593f44d44d3>\u001b[0m in \u001b[0;36mdisplayResults\u001b[0;34m(hit_dictionary, answers, question)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfull_abs_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mranked_aswers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpm_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'pm_ids' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "bTcoserZgN1d",
        "outputId": "d09e17ca-ffbd-410b-af18-a48e5e2220d8"
      },
      "source": [
        "hits[0].lucene_document.fields.toString().stored()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-9a31ead30b2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlucene_document\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstored\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'stored'"
          ]
        }
      ]
    }
  ]
}